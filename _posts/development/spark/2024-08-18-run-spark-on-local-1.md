---
title: "ğŸ’» ë¡œì»¬ ë§¥ë¶ì—ì„œ Spark ì‹¤í–‰í•˜ê¸° - 1í¸: Local Mode"
toc: true
toc_sticky: true
categories: ["Spark"]
excerpt: "Spark ê¼¼ê¼¼íˆ ì‚´í´ë³´ê¸°ì˜ ì²«ê±¸ìŒ"
---

2024ë…„ ëª©í‘œë¡œ Databricks Certificationì„ ì·¨ë“í•´ë³´ë ¤ê³  Apache Sparkë¥¼ "ì œëŒ€ë¡œ" ê³µë¶€í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. ğŸ‡
{: .notice--info}

ì•…ê¸°ë¥¼ ë°°ìš¸ ë•Œì˜ ì²« ê±¸ìŒì„ ì•…ê¸°ë¥¼ ì‚¬ëŠ” ê²ƒ(ìŒ.!?), ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ ë°°ìš¸ ë•Œë„ ì–¸ì–´ë¥¼ ì„¸íŒ… í•˜ëŠ” ê²ƒë¶€í„° í•˜ë“¯ì´. Sparkë„ ë¡œì»¬ì— ì„¸íŒ…ì„ í•´ë³´ë©´ì„œ ë°°ìš°ëŠ”ê²Œ ë§ì„ ê±°ë¼ê³  ìƒê°í•œë‹¤. ë³¸ì¸ì€ íšŒì‚¬ì—ì„œ Databricksë¥¼ ì‚¬ìš©í•´ì„œ ê·¸ë™ì•ˆ Sparkë¥¼ í•œë²ˆë„ ì§ì ‘ ì„¸íŒ… í•´ë³¸ ì ì´ ì—†ì—ˆë‹¤...! Databricksê°€ ì°¸ í¸ë¦¬í•˜ê¸´ í•œë°, ê·¼ë³¸ ì›ë¦¬ë¥¼ ëª¨ë¥´ê³  ì“°ë‹¤ë³´ë‹ˆ ëª¨ë˜ì„± ìœ„ì—ì„œ ë­”ê°€ë¥¼ ìŒ“ì•„ì˜¬ë¦¬ëŠ” ëŠë‚Œì„ ë–¨ì¹  ìˆ˜ ì—†ì—ˆë‹¤. ê·¸ë˜ì„œ ì‹œì‘í•˜ê²Œ ëœ Spark ê³µë¶€...!

ì§€ê¸ˆê¹Œì§€ ë§ì€ ê²ƒë“¤ì„ ìƒˆë¡œ ìµíˆê³  ë°°ì› ì§€ë§Œ...!? ìƒˆë¡œìš´ ê±¸ ë°°ìš°ëŠ” ê±¸ ëŠ˜ í˜ë“¤ê³  ê´´ë¡œì› ë‹¤. ì²˜ìŒì—” ì–´ë µê² ì§€ë§Œ ê³§ ìµìˆ™í•´ì§€ê² ì§€ ë­~~ ì•”íŠ¼ ì¼ë‹¨ ë¡œì»¬ì— `pyspark`ë¥¼ ì„¤ì¹˜í•´ì„œ Sparkë¥¼ ëŒë ¤ë³´ì!!

# Install Spark

Sparkë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ë°©ë²•ì´ ìˆê² ìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” `pip3 install pyspark`ë¥¼ ì„¤ì¹˜í•´ ì‚¬ìš©í–ˆë‹¤. `pyspark` ì•ˆì— pip package ë¿ë§Œ ì•„ë‹ˆë¼ ê°ì¢… Spark CLIë“¤ì´ ëª¨ë‘ ë“¤ì–´ìˆë‹¤. ë§Œì•½, `pyspark` ë°©ì‹ì„ í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ Spark CLIë§Œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš©í•´ë³¼ ìˆ˜ë„ ìˆë‹¤.

```bash
$ brew install apache-spark
```

## Just Install PySpark

```sh
# venv ì„¸íŒ…
$ python3 -m venv venv
$ source venv/bin/activate

# pyspark ì„¤ì¹˜
$ pip3 install pyspark==3.5.2
```

## `pyspark` CLI

```sh
$ pyspark
Python 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
24/08/18 17:06:43 WARN Utils: Your hostname, Seokyunui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.21 instead (on interface en0)
24/08/18 17:06:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/08/18 17:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.2
      /_/

Using Python version 3.12.4 (main, Jun  6 2024 18:26:44)
Spark context Web UI available at http://192.168.0.21:4040
Spark context available as 'sc' (master = local[*], app id = local-1723968404162).
SparkSession available as 'spark'.
>>> 
```

ìœ„ì˜ ì¶œë ¥ì—ì„œ ì£¼ëª©í•  ë¶€ë¶„ì´ ìˆëŠ”ë°,

> Spark context Web UI available at `http://192.168.0.21:4040`<br/>
> Spark context available as `sc` (master = local[*], app id = local-1723968404162).<br/>
> SparkSession available as `spark`.

`http://localhoost:4040/jobs/`ì— ë“¤ì–´ê°€ë³´ë©´, ìš”ë ‡ê²Œ Spark UIê°€ ëœ¨ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤.

![](/images/development/spark/pyspark-spark-ui.png){: .align-center style="max-height: 400px" }

í•´ë‹¹ ì½˜ì†”ì—ì„œ Spark Contextì™€ Spark Sessionì— ì ‘ê·¼í•˜ë ¤ë©´ ê°ê° `spark`ì™€ `sc`ë¡œ ì ‘ê·¼í•˜ë©´ ëœë‹¤. (ì°¸ê³ ë¡œ Databricksë„ ë§ˆì°¬ê°€ì§€ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶™ì´ë©´ `spark`, `sc` ë³€ìˆ˜ê°€ í•­ìƒ ì¡´ì¬í•œë‹¤.)

```py
>>> spark
<pyspark.sql.session.SparkSession object at 0x10cc3b6b0>

>>> sc
<SparkContext master=local[*] appName=PySparkShell>
```

Spark Contextì—ì„œ `master=local[*]`ë¡œ ë˜ì–´ ìˆëŠ” ë¶€ë¶„ì€ 2ê°€ì§€ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆëŠ”ë°,

- `local`: Spark ì–´í”Œë¦¬ì¼€ì´ì…˜ì´ ë¡œì»¬ ëª¨ë“œì—ì„œ ì‹¤í–‰ë¨. ë¡œì»¬ ëª¨ë“œì—ì„  í´ëŸ¬ìŠ¤í„°ê°€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ë‹¨ì¼ ë¨¸ì‹ ì—ì„œ ëª¨ë“  ì‘ì—…ì´ ìˆ˜í–‰ë¨.
- `[*]`: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì½”ì–´ë¥¼ ì˜ë¯¸. ì»´í“¨í„°ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì½”ì–´ë¥¼ ì“°ê² ë‹¤ëŠ” ì˜ë¯¸ì„.

ë§Œì•½ ì½”ì–´ ê°¯ìˆ˜ë¥¼ ì§€ì •í•˜ê³  ì‹¶ë‹¤ë©´, `local[4]`ì™€ ê°™ì´ ì ìœ¼ë©´ ëœë‹¤ê³  í•œë‹¤.


## `import pyspark`

`import pyspark`í•´ì„œ pyspark ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. ì•„ì£¼ ê°„ë‹¨í•œ pyspark ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ì. SparkSessionê³¼ SparkContextëŠ” íŠ¹ë³„í•œ ì´ìœ ê°€ ì—†ë‹¤ë©´, ëª¨ë‘ `spark`ì™€ `sc`ë¡œ ì´ë¦„ì„ ì§€ì •í•˜ê² ë‹¤.

```py
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
print(spark)

sc = spark.sparkContext
print(sc)

rdd = sc.parallelize(range(10000))
cnt = rdd.count()
print(cnt)
---
<pyspark.sql.session.SparkSession object at 0x105bbe3c0>
<SparkContext master=local[*] appName=pyspark-shell>
10000 
```

ì´ë•Œ, ë§ˆì§€ë§‰ì— `time.sleep(60 * 60 * 2)`ë¥¼ ì¶”ê°€ í•´ë‘ë©´, Spark UIë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì£¼ì†ŒëŠ” ì´ì „ê³¼ ë˜‘ê°™ì´ `http://localhoost:4040/jobs/`ì´ë‹¤.

![](/images/development/spark/pyspark-spark-ui-2.png){: .align-center style="max-height: 400px" }

ì‹¤í–‰ í–ˆë˜ ê²ƒë“¤ì´ Spark Jobsë“¤ë¡œ ë§Œë“¤ì–´ì ¸ì„œ ì˜ ë‚˜ì˜¨ë‹¤!!

# Spark Local Modeë€?

ì§€ê¸ˆê¹Œì§€ ë¡œì»¬ì—ì„œ `pyspark`ë¥¼ ì„¤ì¹˜í•´ ì‹¤í–‰í•œ ë°©ë²•ì€ Sparkë¥¼ Local Modeë¡œ ì‚¬ìš©í•œ ê²ƒì´ë‹¤. Sparkë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ "Local Mode"ì™€ "Cluster Mode"ë¡œ ë‚˜ë‰œë‹¤. ì‰½ê²Œ ìƒê°í•˜ë©´ ë‹¨ì¼ ë¨¸ì‹ ì´ëƒ ë“œë¼ì´ë²„-ì›Œì»¤ë¡œ êµ¬ì„±ëœ í´ëŸ¬ìŠ¤í„°ë¡œ ëŒë¦¬ëƒì˜ ì°¨ì´.

![](/images/development/spark/spark-local-mode.png){: .align-center style="max-height: 400px" }
Spark Local Mode
{: .align-caption .text-center .small .gray }

- **Local Mode** (ì§€ê¸ˆê¹Œì§€ í•œ ê²ƒ)
- Cluster Mode (ì–¸ì  ê°€ ë‹¤ë£° ì˜ˆì •)
  - Client Deploy Mode
  - Cluster Deploy Mode

ì•”íŠ¼ ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ê°€ ì‹¤í–‰í•œ ë°©ì‹ì€ Local Modeë¼ëŠ” ê²ƒ! ì´ ëª¨ë“œëŠ” ë‹¨ì¼ ë¨¸ì‹ ì—ì„œ ëª¨ë“  ê²ƒì„ ì‹¤í–‰í•œë‹¤. ê·¸ë˜ë„ Diverì™€ ExecutorëŠ” ê°ê° 1ê°œì”© ëœ¬ë‹¤.

# `spark-submit`

`spark-submit`ì„ í†µí•´ Spark ë¨¸ì‹ ì— python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. Local Modeì—ì„œ `spark-submit`ì„ í•´ë³´ì.

```sh
$ spark-submit \
  --master "local[2]" \
  ./hello-spark.py
---
...
24/08/22 01:15:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
24/08/22 01:15:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.30.1.16, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes) 
24/08/22 01:15:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.30.1.16, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes) 
24/08/22 01:15:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/08/22 01:15:54 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/08/22 01:15:55 INFO PythonRunner: Times: total = 329, boot = 285, init = 44, finish = 0
24/08/22 01:15:55 INFO PythonRunner: Times: total = 329, boot = 283, init = 46, finish = 0
24/08/22 01:15:55 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1324 bytes result sent to driver
24/08/22 01:15:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1324 bytes result sent to driver
24/08/22 01:15:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 390 ms on 172.30.1.16 (executor driver) (1/2)
24/08/22 01:15:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 389 ms on 172.30.1.16 (executor driver) (2/2)
24/08/22 01:15:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/08/22 01:15:55 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51645
24/08/22 01:15:55 INFO DAGScheduler: ResultStage 0 (count at /Users/seokyunha/xxxx/hello-spark.py:10) finished in 0.832 s
24/08/22 01:15:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/08/22 01:15:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/08/22 01:15:55 INFO DAGScheduler: Job 0 finished: count at /Users/seokyunha/xxxx/hello-spark.py:10, took 0.848853 s
10000
```

ì•„ì§ ë¡œê·¸ ì½ëŠ” ë²•ì€ ì˜ ëª¨ë¥´ê² ë‹¤. ë‚˜ì¤‘ì— ì°¾ì•„ë³¼ ê²ƒ!!

# ë§ˆë¬´ë¦¬

![](/images/meme/thumbs-up.png){: .align-center style="max-height: 280px" }

ì—¬ê¸°ê¹Œì§€ê°€ ë‹¨ì¼ ë¨¸ì‹ ì—ì„œ Spark ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” Local Mode ì˜€ë‹¤. í¬ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´, 2í¸ì—ì„œëŠ” ë‹¤ì¤‘ ë¨¸ì‹  êµ¬ì¡°ì¸ Cluster Modeì—ì„œ Spark ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì!!

ğŸ‘‰ [ë¡œì»¬ ë§¥ë¶ì—ì„œ Spark ì‹¤í–‰í•˜ê¸° - 2í¸: Cluster Mode](/2024/08/18/run-spark-on-local-2/)
