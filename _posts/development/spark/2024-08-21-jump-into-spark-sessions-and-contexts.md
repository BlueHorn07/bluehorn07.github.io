---
title: "Jump into Spark Sessions and Contexts"
toc: true
toc_sticky: true
categories: ["Spark"]
excerpt: "Spark Sessionì´ë€ ë¬´ì—‡ì¸ê°€? Spark Sessionì— ë“¤ì–´ìˆëŠ”, `SparkContext`, `SQLContext` ë“±ì— ëŒ€í•´ ğŸ‡"
---

Databricks Certification ì·¨ë“ì„ ëª©í‘œë¡œ Apache Sparkë¥¼ â€œì œëŒ€ë¡œâ€ ê³µë¶€í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. íšŒì‚¬ì—ì„  Databricks Unity Catalogë¥¼ ë„ì…í•˜ë ¤ê³  ë¶„íˆ¬í•˜ê³  ìˆëŠ”ë°ìš”. Sparkì™€ ì¢€ ì¹œí•´ì§ˆ ìˆ˜ ìˆì„ê¹Œìš”? ğŸ‡ ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Development - Spark](/topic/development#apache-spark)ì—ì„œ í™•ì¸í•´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}


# SparkSession? SparkContext?

ì´ ê¸€ì€ Apache Sparkë¥¼ ê³µë¶€í•˜ë©´ì„œ `SparkSession`ê³¼ `SparkContext`ë¥¼ í—·ê°ˆë ¤ í•˜ë˜ ë‚˜ì˜ ì´ì•¼ê¸°ë¥¼ ë‹´ì€ í¬ìŠ¤íŠ¸ì´ë‹¤.

Sparkë¥¼ ì²˜ìŒ ê³µë¶€í•˜ë©´, ê°•ì˜ë‚˜ ê³µì‹ë¬¸ì„œë¥¼ í†µí•´ì„  `SparkContext`ë¡œ RDDë¥¼ ë§Œë“œëŠ” ê±¸ ë¨¼ì € ë°°ì› ë‹¤. ê·¸ëŸ°ë°, íšŒì‚¬ì—ì„  Databricksë¥¼ í†µí•´ "*spark*"ë€ ì´ë¦„ì˜ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” `SparkSession`ë¥¼ ë¨¼ì € ì‚¬ìš©í–ˆë‹¤.

ìµœê·¼ì—” íšŒì‚¬ì—ì„œ Databricksì—ì„œ Unity Catalogë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ í–ˆëŠ”ë°, UCì—ì„œëŠ” "`SparkContext`ë¥¼ ì‚¬ìš©í•˜ì§€ ëª» í•œë‹¤" Limitationë„ ìˆì—ˆë‹¤.

![](/images/development/spark/databricks-unity-catalog-shared-mode-limitation.png){: .align-center style="max-height: 400px" }
[Databricks: Shared access mode limitations on Unity Catalog](https://docs.databricks.com/en/compute/access-mode-limitations.html)
{: .align-caption .text-center .small .gray }

ë¬¸ì„œë¥¼ ì½ì–´ë³´ë©´ `SparkContext`, `SQLContext`ì™€ ê°™ì´ Context APIë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê³ , ê·¸ì™€ ê´€ë ¨ëœ `.parallelize()` í•¨ìˆ˜ë¥¼ í¬í•¨í•´ RDD APIì™€ Dataset APIë„ ì‚¬ìš© ëª» í•˜ê²Œ ëœë‹¤;;

![](/images/development/spark/databricks-unity-catalog-shared-mode-not-support-spark-context.png){: .align-center style="max-height: 300px" }
sparkContextì— ì ‘ê·¼í•˜ëŠ” ê±´ ê°€ëŠ¥í•˜ì§€ë§Œ, `.parallelize()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ê±´ ì•ˆ ëœë‹¤.
{: .align-caption .text-center .small .gray }

ì´ëŸ° ë‚˜ì˜ ê²½í—˜ë“¤ì€ `SparkContext`ì™€ RDDê°€ 'êµ¬ì‹œëŒ€ì˜ ìœ ë¬¼'ì´ ì•„ë‹Œì§€ ìƒê°í•˜ê²Œ í–ˆê³ , ê´€ë ¨ëœ ê°œë…ë“¤ì„ ë°›ì•„ ë“¤ì´ëŠ” ë°ì—, ì•Œê²Œëª¨ë¥´ê²Œ ì¥ë²½ì´ ë˜ì—ˆë˜ ê²ƒ ê°™ë‹¤.

ì•”íŠ¼ ì§€ê¸ˆì€ DatabricksëŠ” Databricks, SparkëŠ” Sparkë¼ëŠ” ìƒê°ìœ¼ë¡œ `SparkContext`ì— ëŒ€í•´ ë°›ì•„ë“¤ì´ê³ , ì‚´í´ë³´ë ¤ê³  í•œë‹¤.

# Jump into SparkSession

íšŒì‚¬ì—ì„œ ì œì¼ ë¨¼ì € ìµí˜”ë˜ ë°©ì‹ì´ ìš” `SparkSession`ì´ë‹ˆ ìš”ê±¸ ë¨¼ì € ì‚´í´ë³´ì.

![](/images/development/spark/databricks-spark-session.png){: .align-center style="max-height: 300px" }

Databricksì—ì„  `spark`ë¼ëŠ” ë³€ìˆ˜ë¡œ pysparkë“  scala sparkë“  ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. ìš”ê¸°ì—ì„œ ì£¼ë¡œ ì“°ë˜ ê¸°ëŠ¥ë“¤ì€

- `spark.sql()`
  - SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰ì‹œí‚¬ ë•Œ ì‚¬ìš©
- `spark.udf.register()`
  - Spark UDF í•¨ìˆ˜ë¥¼ ë“±ë¡í•  ë•Œ ì‚¬ìš©
- `spark.table()`
  - Hive í…Œì´ë¸”ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” Spark DataFrameì„ ë°˜í™˜.
  - ì•„ì§ Spark-Hiveì˜ ê´€ê³„ì— ëŒ€í•´ì„œ ì œëŒ€ë¡œ ì‚´í´ë³´ì§„ ëª» í–ˆìŒ. TODO!
- `spark.catalog`
  - Spark Sessionì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì™€ ê·¸ ì•ˆì˜ ê°ì²´ë“¤(Table, View, Function)ì„ ê´€ë¦¬í•˜ëŠ” API
    - `spark.catalog.listTables()`: í…Œì´ë¸” ëª©ë¡ì„ ì¡°íšŒ
    - `spark.catalog.setCurrentDatabase()`: í˜„ì¬ ì‚¬ìš©í•˜ëŠ” Databaseë¥¼ ì§€ì •
- `spark.read`
  - ë°ì´í„°ë¥¼ ë‹¤ì–‘í•­ í˜•ì‹(csv, jso, parquet ë“±)ì—ì„œ ì½ì–´ë“¤ì—¬ Spark DataFrameìœ¼ë¡œ ë°˜í™˜.
- `spark.write`
  - Spark DataFrameì„ ë‹¤ì–‘í•œ í˜•ì‹(ìœ„ì™€ ë™ì¼)ìœ¼ë¡œ ì €ì¥.
- `spark.conf`
  - í˜„ì¬ Spark Sessionì˜ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” API
- `spark.streams`
  - Spark Structured Streaming ê´€ë ¨ API
- `spark.SparkContext`, `spark.sqlContext`
  - SparkSessionì— ë‚´ì¥ëœ Spark/Sql Context
- ê·¸ì™¸ ë“±ë“±

ë³¸ì¸ì´ ì§€ê¸ˆ Databricksì—ì„œ `spark`ë¡œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë“¤ì„ ë³¸ ê²ƒë§Œ ì´ ì •ë„ë‹¤. ì—¬ê¸°ì— ëª‡ ê°€ì§€ë¥¼ ê¸°ëŠ¥ì„ ë” ì œê³µí•˜ëŠ”ë°, ë‚˜ë¨¸ì§€ ê¸°ëŠ¥ë“¤ì€ [Spark ê³µì‹ ë¬¸ì„œ](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html) ì°¸ê³ .

> Unified entry point for interacting with Spark

Spark Sessionì„ ì„¤ëª…í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë¬¸ì¥ì´ë‹¤. `spark`ë¼ëŠ” ë³€ìˆ˜ë¥¼ í†µí•´ì„œ SQLë¡œ ì‹¤í–‰í•˜ì§€, Hive í…Œì´ë¸” ëª©ë¡ë„ ì¡°íšŒí•˜ì§€, UDFë„ ë“±ë¡í•˜ì§€, Parquet íŒŒì¼ì„ read/write í•˜ê¸°ë„ í•˜ì§€... ì •ë§ ë§ì€ ì‘ì—…ì„ Spark Sessionì„ í†µí•´ì„œ ìˆ˜í–‰í•œë‹¤.

ì•”íŠ¼ ì§€ê¸ˆê¹Œì§€ Spark Sessionì— ëŒ€í•´ ì£¼ì €ë¦¬ ì£¼ì €ë¦¬ ë– ë“¤ì—ˆëŠ”ë°, ë‹¤ìŒì—” ì´ í¬ìŠ¤íŠ¸ì˜ ë‘ë²ˆì§¸ ì£¼ì œì¸ `SparkContext`ë¡œ ë„˜ì–´ê°€ë³´ì.

# Jump into SparkContext

![](/images/development/spark/databricks-spark-context.png){: .align-center style="max-height: 300px" }

Spark ê³µë¶€ë¥¼ ì‹œì‘í•  ë•Œ, RDDì™€ í•¨ê»˜ ê°€ì¥ ë¨¼ì € ë§ˆì£¼ì¹˜ëŠ” ë…€ì„ì¸ `SparkContext`ë‹¤. ì‚¬ì‹¤ ì˜ˆì „ì—ëŠ” `SparkContext`ë¡œ Sparkë¥¼ ì ‘í•˜ëŠ”ë°, ë‹¹ì—° í–ˆì„ ì§€ë„ ëª¨ë¥¸ë‹¤. ì™œëƒí•˜ë©´, `SparkContext`ê°€ `SparkSession`ë³´ë‹¤ ë¨¼ì € ë¦´ë¦¬ì¦ˆ ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.

- `SparkContext`
  - spark 1.x
- `SparkSession`
  - spark 2.0
- ì°¸ê³ ë¡œ ì‘ì„±ì¼(24.08.27) ê¸°ì¤€ SparkëŠ” `3.5.2` ë²„ì „ê¹Œì§€ ë‚˜ì™”ë‹¤. spark 3.0ì˜ ê°€ì¥ ì£¼ìš”í•œ ë³€ê²½ì ì€ AQE(Adaptive Query Execution)ì´ê³  ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¤„ë³¼ ì˜ˆì •ì´ë‹¤.

`SparkContext`ëŠ” RDD(Resilient Distributed Data)ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ entry pointì´ë‹¤. RDDëŠ” ì´ˆê¸° Sparkë¥¼ ì´ë£¨ëŠ” ê°€ì¥ ê¸°ì´ˆì ì¸ ë°ì´í„° êµ¬ì¡°ì´ë‹¤. RDDì— ëŒ€í•´ì„œë„ ì§€ê¸ˆ ìì„¸íˆ ì–¸ê¸‰í•˜ê¸°ëŠ” ì–´ë ¤ì›Œì„œ ë³„ë„ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¤„ë³´ê² ë‹¤. ëŒ€ì¶© ì›ì‹œì ì¸ í˜•íƒœì˜ DataFrameì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.

ëª‡ê°€ì§€ ì½”ë“œë¥¼ í†µí•´ `SparkSession`ì™€ `SparkContext`ì˜ ì°¨ì´ë¥¼ ì‚´í´ë³´ì.

## Empty Data

Spark Sessionì—ì„œëŠ”...

```scala
scala> spark.emptyDataFrame
org.apache.spark.sql.DataFrame = []
```

Spark Contextì—ì„œëŠ”...

```scala
scala> sc.emptyRDD
org.apache.spark.rdd.RDD[Nothing] = EmptyRDD[0] at emptyRDD at <console>:24
```

## Range

Spark Sessionì—ì„œëŠ”...

```scala
scala> spark.range(10)
org.apache.spark.sql.Dataset[Long] = [id: bigint]
```

Spark Contextì—ì„œëŠ”...

```scala
scala> sc.parallelize(1 to 9)
org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24
```

Spark Session(ì´í•˜ SS)ì˜ `.range()`ê°€ Spark Context(ì´í•˜ SC)ì˜ `.parallelize()`ì™€ ëŒ€ì‘í•œë‹¤.

ì°¸ê³ ë¡œ SCì—ë„ `.range()` í•¨ìˆ˜ê°€ ìˆëŠ”ë°, ë°˜í™˜í•˜ëŠ” RDDê°€ `.parallelize()`ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.

```scala
scala> sc.range(0, 10)
org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[3] at range at <console>:24
```

## Read CSV

Spark Sessionì—ì„œëŠ”...

```scala
val df = spark.read
  .option("header", "true")  // ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
  .option("inferSchema", "true")  // ë°ì´í„° íƒ€ì…ì„ ìë™ìœ¼ë¡œ ì¶”ë¡ 
  .csv("/path/to/your/file.csv")

// DataFrame ë‚´ìš© ì¶œë ¥
df.show()
```

Spark Context ì—ì„ ...

```scala
val conf = new SparkConf().setAppName("CSV Reader Example").setMaster("local[*]")
val sc = new SparkContext(conf)

// CSV íŒŒì¼ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì½ê¸°
val rdd = sc.textFile("/path/to/your/file.csv")

// CSV íŒŒì¼ì˜ ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì²˜ë¦¬í•˜ê³ , ë°ì´í„°ë¥¼ ë¶„í• 
val header = rdd.first()  // ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì¶”ì¶œ
val data = rdd.filter(row => row != header)  // í—¤ë”ë¥¼ ì œì™¸í•œ ë°ì´í„°ë§Œ í•„í„°ë§

// ê° í–‰ì„ ì½¤ë§ˆë¡œ ë¶„í• í•˜ì—¬ RDDë¡œ ë³€í™˜
val parsedData = data.map(line => line.split(","))

// RDD ë‚´ìš© ì¶œë ¥
parsedData.collect().foreach(row => println(row.mkString(", ")))
```

í™•ì‹¤íˆ Spark Contextë¥¼ ì‚¬ìš©í•˜ëŠ” ìª½ì´ low-level APIë¼ì„œ ê·¸ëŸ°ì§€ CSV íŒŒì¼ì„ ì½ëŠ” ê°„ë‹¨í•œ ì‘ì—…ë„, ì²˜ë¦¬ë¥¼ ë§ì´ í•´ì£¼ëŠ” ëª¨ìŠµì´ ë³´ì¸ë‹¤.

----

ê²°êµ­ RDD vs. Dataframeê¹Œì§€ ë‹¤ë¤„ì•¼ í•˜ë‚˜?

RDD -> DataFrame ë³€í™˜ë„ ê°€ëŠ¥?
