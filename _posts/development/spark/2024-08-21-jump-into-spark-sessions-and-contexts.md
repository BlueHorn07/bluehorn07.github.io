---
title: "Jump into Spark Sessions and Contexts"
toc: true
toc_sticky: true
categories: ["Spark"]
excerpt: "Spark Sessionì´ë€ ë¬´ì—‡ì¸ê°€? Spark Sessionì— ë“¤ì–´ìˆëŠ”, `SparkContext`, `SQLContext` ë“±ì— ëŒ€í•´ ğŸ‡"
---

Databricks Certification ì·¨ë“ì„ ëª©í‘œë¡œ Apache Sparkë¥¼ â€œì œëŒ€ë¡œâ€ ê³µë¶€í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. íšŒì‚¬ì—ì„  Databricks Unity Catalogë¥¼ ë„ì…í•˜ë ¤ê³  ë¶„íˆ¬í•˜ê³  ìˆëŠ”ë°ìš”. Sparkì™€ ì¢€ ì¹œí•´ì§ˆ ìˆ˜ ìˆì„ê¹Œìš”? ğŸ‡ ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Development - Spark](/topic/development#apache-spark)ì—ì„œ í™•ì¸í•´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}


# SparkSession? SparkContext?

ì´ ê¸€ì€ Apache Sparkë¥¼ ê³µë¶€í•˜ë©´ì„œ `SparkSession`ê³¼ `SparkContext`ë¥¼ í—·ê°ˆë ¤ í•˜ë˜ ë‚˜ì˜ ì´ì•¼ê¸°ë¥¼ ë‹´ì€ í¬ìŠ¤íŠ¸ì´ë‹¤.

Sparkë¥¼ ì²˜ìŒ ê³µë¶€í•˜ë©´, ê°•ì˜ë‚˜ ê³µì‹ë¬¸ì„œë¥¼ í†µí•´ì„  `SparkContext`ë¡œ RDDë¥¼ ë§Œë“œëŠ” ê±¸ ë¨¼ì € ë°°ì› ë‹¤. ê·¸ëŸ°ë°, íšŒì‚¬ì—ì„  Databricksë¥¼ í†µí•´ "*spark*"ë€ ì´ë¦„ì˜ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” `SparkSession`ë¥¼ ë¨¼ì € ì‚¬ìš©í–ˆë‹¤.

ìµœê·¼ì—” íšŒì‚¬ì—ì„œ Databricksì—ì„œ Unity Catalogë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ í–ˆëŠ”ë°, UCì—ì„œëŠ” "`SparkContext`ë¥¼ ì‚¬ìš©í•˜ì§€ ëª» í•œë‹¤" Limitationë„ ìˆì—ˆë‹¤.

![](/images/development/spark/databricks-unity-catalog-shared-mode-limitation.png){: .align-center style="max-height: 400px" }
[Databricks: Shared access mode limitations on Unity Catalog](https://docs.databricks.com/en/compute/access-mode-limitations.html)
{: .align-caption .text-center .small .gray }

ë¬¸ì„œë¥¼ ì½ì–´ë³´ë©´ `SparkContext`, `SQLContext`ì™€ ê°™ì´ Context APIë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê³ , ê·¸ì™€ ê´€ë ¨ëœ `.parallelize()` í•¨ìˆ˜ë¥¼ í¬í•¨í•´ RDD APIì™€ Dataset APIë„ ì‚¬ìš© ëª» í•˜ê²Œ ëœë‹¤;;

![](/images/development/spark/databricks-unity-catalog-shared-mode-not-support-spark-context.png){: .align-center style="max-height: 300px" }
sparkContextì— ì ‘ê·¼í•˜ëŠ” ê±´ ê°€ëŠ¥í•˜ì§€ë§Œ, `.parallelize()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ê±´ ì•ˆ ëœë‹¤.
{: .align-caption .text-center .small .gray }

ì´ëŸ° ë‚˜ì˜ ê²½í—˜ë“¤ì€ `SparkContext`ì™€ RDDê°€ 'êµ¬ì‹œëŒ€ì˜ ìœ ë¬¼'ì´ ì•„ë‹Œì§€ ìƒê°í•˜ê²Œ í–ˆê³ , ê´€ë ¨ëœ ê°œë…ë“¤ì„ ë°›ì•„ ë“¤ì´ëŠ” ë°ì—, ì•Œê²Œëª¨ë¥´ê²Œ ì¥ë²½ì´ ë˜ì—ˆë˜ ê²ƒ ê°™ë‹¤.

ì•”íŠ¼ ì§€ê¸ˆì€ DatabricksëŠ” Databricks, SparkëŠ” Sparkë¼ëŠ” ìƒê°ìœ¼ë¡œ `SparkContext`ì— ëŒ€í•´ ë°›ì•„ë“¤ì´ê³ , ì‚´í´ë³´ë ¤ê³  í•œë‹¤.

# Jump into SparkSession

íšŒì‚¬ì—ì„œ ì œì¼ ë¨¼ì € ìµí˜”ë˜ ë°©ì‹ì´ ìš” `SparkSession`ì´ë‹ˆ ìš”ê±¸ ë¨¼ì € ì‚´í´ë³´ì.

![](/images/development/spark/databricks-spark-session.png){: .align-center style="max-height: 300px" }

Databricksì—ì„  `spark`ë¼ëŠ” ë³€ìˆ˜ë¡œ pysparkë“  scala sparkë“  ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. ìš”ê¸°ì—ì„œ ì£¼ë¡œ ì“°ë˜ ê¸°ëŠ¥ë“¤ì€

- `spark.sql()`
  - SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰ì‹œí‚¬ ë•Œ ì‚¬ìš©
- `spark.udf.register()`
  - Spark UDF í•¨ìˆ˜ë¥¼ ë“±ë¡í•  ë•Œ ì‚¬ìš©
- `spark.table()`
  - Hive í…Œì´ë¸”ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” Spark DataFrameì„ ë°˜í™˜.
  - ì•„ì§ Spark-Hiveì˜ ê´€ê³„ì— ëŒ€í•´ì„œ ì œëŒ€ë¡œ ì‚´í´ë³´ì§„ ëª» í–ˆìŒ. TODO!
- `spark.catalog`
  - Spark Sessionì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì™€ ê·¸ ì•ˆì˜ ê°ì²´ë“¤(Table, View, Function)ì„ ê´€ë¦¬í•˜ëŠ” API
    - `spark.catalog.listTables()`: í…Œì´ë¸” ëª©ë¡ì„ ì¡°íšŒ
    - `spark.catalog.setCurrentDatabase()`: í˜„ì¬ ì‚¬ìš©í•˜ëŠ” Databaseë¥¼ ì§€ì •
- `spark.read`
  - ë°ì´í„°ë¥¼ ë‹¤ì–‘í•­ í˜•ì‹(csv, jso, parquet ë“±)ì—ì„œ ì½ì–´ë“¤ì—¬ Spark DataFrameìœ¼ë¡œ ë°˜í™˜.
- `spark.write`
  - Spark DataFrameì„ ë‹¤ì–‘í•œ í˜•ì‹(ìœ„ì™€ ë™ì¼)ìœ¼ë¡œ ì €ì¥.
- `spark.conf`
  - í˜„ì¬ Spark Sessionì˜ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” API
- `spark.streams`
  - Spark Structured Streaming ê´€ë ¨ API
- `spark.SparkContext`, `spark.sqlContext`
  - SparkSessionì— ë‚´ì¥ëœ Spark/Sql Context
- ê·¸ì™¸ ë“±ë“±

ë³¸ì¸ì´ ì§€ê¸ˆ Databricksì—ì„œ `spark`ë¡œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë“¤ì„ ë³¸ ê²ƒë§Œ ì´ ì •ë„ë‹¤. ì—¬ê¸°ì— ëª‡ ê°€ì§€ë¥¼ ê¸°ëŠ¥ì„ ë” ì œê³µí•˜ëŠ”ë°, ë‚˜ë¨¸ì§€ ê¸°ëŠ¥ë“¤ì€ [Spark ê³µì‹ ë¬¸ì„œ](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html) ì°¸ê³ .

> Unified entry point for interacting with Spark

Spark Sessionì„ ì„¤ëª…í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë¬¸ì¥ì´ë‹¤. `spark`ë¼ëŠ” ë³€ìˆ˜ë¥¼ í†µí•´ì„œ SQLë¡œ ì‹¤í–‰í•˜ì§€, Hive í…Œì´ë¸” ëª©ë¡ë„ ì¡°íšŒí•˜ì§€, UDFë„ ë“±ë¡í•˜ì§€, Parquet íŒŒì¼ì„ read/write í•˜ê¸°ë„ í•˜ì§€... ì •ë§ ë§ì€ ì‘ì—…ì„ Spark Sessionì„ í†µí•´ì„œ ìˆ˜í–‰í•œë‹¤.

ì•”íŠ¼ ì§€ê¸ˆê¹Œì§€ Spark Sessionì— ëŒ€í•´ ì£¼ì €ë¦¬ ì£¼ì €ë¦¬ ë– ë“¤ì—ˆëŠ”ë°, ë‹¤ìŒì—” ì´ í¬ìŠ¤íŠ¸ì˜ ë‘ë²ˆì§¸ ì£¼ì œì¸ `SparkContext`ë¡œ ë„˜ì–´ê°€ë³´ì.

# Jump into SparkContext

![](/images/development/spark/databricks-spark-context.png){: .align-center style="max-height: 300px" }

Spark ê³µë¶€ë¥¼ ì‹œì‘í•  ë•Œ, RDDì™€ í•¨ê»˜ ê°€ì¥ ë¨¼ì € ë§ˆì£¼ì¹˜ëŠ” ë…€ì„ì¸ `SparkContext`ë‹¤. ì˜ˆì „ì—ëŠ” `SparkContext`ë¡œ Sparkë¥¼ ì ‘í•˜ëŠ”ë°, ë‹¹ì—° í–ˆì„ ì§€ë„ ëª¨ë¥¸ë‹¤. ì™œëƒí•˜ë©´, `SparkSession`ì€ spark 2.0ë¶€í„° ë„ì…ëœ ë°©ì‹ì´ê¸° ë•Œë¬¸ì´ë‹¤.

- `SparkContext`
  - spark 1.x
- `SparkSession`
  - spark 2.0 (2016-07-26 ì¶œì‹œ)

ì°¸ê³ ë¡œ ì‘ì„±ì¼(24.08.27) ê¸°ì¤€ SparkëŠ” `3.5.2` ë²„ì „ê¹Œì§€ ë‚˜ì™”ë‹¤. Spark 3.0 ë²„ì „ì—ì„œ ê°€ì¥ ì£¼ìš”í•œ ë³€ê²½ì ì€ AQE(Adaptive Query Execution)ì´ê³  ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¤„ë³¼ ì˜ˆì •ì´ë‹¤. [link](/2024/08/29/spark-adpative-query-execution/)

`SparkContext`ëŠ” RDD(Resilient Distributed Data)ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ entry pointì´ë‹¤. RDDëŠ” ì´ˆê¸° Sparkë¥¼ ì´ë£¨ëŠ” ê°€ì¥ ê¸°ì´ˆì ì¸ ë°ì´í„° êµ¬ì¡°ì´ë‹¤. RDDì— ëŒ€í•´ì„œë„ ì§€ê¸ˆ ìì„¸íˆ ì–¸ê¸‰í•˜ê¸°ëŠ” ì–´ë ¤ì›Œì„œ ë³„ë„ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¤„ë³´ê² ë‹¤. ëŒ€ì¶© ì›ì‹œì ì¸ í˜•íƒœì˜ DataFrameì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.

ëª‡ê°€ì§€ ì½”ë“œë¥¼ í†µí•´ `SparkSession`ì™€ `SparkContext`ì˜ ì°¨ì´ë¥¼ ì‚´í´ë³´ì.

## Empty Data

Spark Sessionì—ì„œëŠ”...

```scala
scala> spark.emptyDataFrame
org.apache.spark.sql.DataFrame = []
```

Spark Contextì—ì„œëŠ”...

```scala
scala> sc.emptyRDD
org.apache.spark.rdd.RDD[Nothing] = EmptyRDD[0] at emptyRDD at <console>:24
```

## Range

Spark Sessionì—ì„œëŠ”...

```scala
scala> spark.range(10)
org.apache.spark.sql.Dataset[Long] = [id: bigint]
```

Spark Contextì—ì„œëŠ”...

```scala
scala> sc.parallelize(1 to 9)
org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24
```

Spark Session(ì´í•˜ SS)ì˜ `.range()`ê°€ Spark Context(ì´í•˜ SC)ì˜ `.parallelize()`ì™€ ëŒ€ì‘í•œë‹¤.

ì°¸ê³ ë¡œ SCì—ë„ `.range()` í•¨ìˆ˜ê°€ ìˆëŠ”ë°, ë°˜í™˜í•˜ëŠ” RDDê°€ `.parallelize()`ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.

```scala
scala> sc.range(0, 10)
org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[3] at range at <console>:24
```

## Read CSV

Spark Sessionì—ì„œëŠ”...

```scala
val df = spark.read
  .option("header", "true")  // ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
  .option("inferSchema", "true")  // ë°ì´í„° íƒ€ì…ì„ ìë™ìœ¼ë¡œ ì¶”ë¡ 
  .csv("/path/to/your/file.csv")

// DataFrame ë‚´ìš© ì¶œë ¥
df.show()
```

Spark Context ì—ì„ ...

```scala
// CSV íŒŒì¼ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì½ê¸°
val rdd = sc.textFile("/path/to/your/file.csv")

// CSV íŒŒì¼ì˜ ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì²˜ë¦¬í•˜ê³ , ë°ì´í„°ë¥¼ ë¶„í• 
val header = rdd.first()  // ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì¶”ì¶œ
val data = rdd.filter(row => row != header)  // í—¤ë”ë¥¼ ì œì™¸í•œ ë°ì´í„°ë§Œ í•„í„°ë§

// ê° í–‰ì„ ì½¤ë§ˆë¡œ ë¶„í• í•˜ì—¬ RDDë¡œ ë³€í™˜
val parsedData = data.map(line => line.split(","))

// RDD ë‚´ìš© ì¶œë ¥
parsedData.collect().foreach(row => println(row.mkString(", ")))
```

Spark Context ë°©ì‹ì€ low-level APIë¼ì„œ ê·¸ëŸ°ì§€ CSV íŒŒì¼ì„ ì½ëŠ” ê°„ë‹¨í•œ ì‘ì—…ë„, ëª‡ë²ˆì˜ ì²˜ë¦¬ë¥¼ ê±°ì³ì„œ ìˆ˜í–‰ë˜ëŠ” ëª¨ìŠµì´ë‹¤.

# SQLContext

```scala
scala> val sqlContext = spark.sqlContext
res3: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@59aba9b3

scala> val df = sqlContext.read.json("./people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.createOrReplaceTempView("people")

scala> sqlContext.sql("SELECT name, age FROM people WHERE age > 21").show()
+----+---+
|name|age|
+----+---+
|Andy| 30|
+----+---+
```

SQL ContextëŠ” sparkê°€ ì½ì€ ë°ì´í„°ë¥¼ SQLì„ ì‚¬ìš©í•´ ì¿¼ë¦¬í•˜ê±°ë‚˜ ì¡°ì‘í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. ì½”ë“œì— ì‚¬ìš©í•œ json íŒŒì¼ì€ [spark example](https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json)ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.


# HiveContext

ì‚¬ì‹¤ ì•„ì§ Hiveë¥¼ ì œëŒ€ë¡œ ê³µë¶€í•˜ì§€ ì•Šì•˜ì•„ì„œ, ì´ ë¶€ë¶„ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§„ ëª» í–ˆì§€ë§Œ, í˜„ì¬ íŒ€ì—ì„œ Databricksì—ì„œ Hive Metastoreë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë³´ë©´, ëŒ€ì¶© Spark SQL ì¿¼ë¦¬ê°€ Hive í…Œì´ë¸”ì„ ì¿¼ë¦¬í•˜ëŠ” ê±°ë¼ê³  ì§ì‘í•˜ê³  ìˆë‹¤.

Sparkì˜ `HiveContext`ëŠ” Hiveì— ì €ì¥ëœ í…Œì´ë¸”ì„ Sparkì—ì„œ ì¿¼ë¦¬í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë¼ê³  ì´í•´í•˜ê³  ìˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë©´, `spark-shell`ì„ ì‹¤í–‰í•œ ê²½ë¡œì— Hive metastoreë¥¼ ë¡œì»¬ ì„¸íŒ… í•´ë³¼ ìˆ˜ ìˆë‹¤.

```scala
scala> val df = spark.read.json("./people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.write.mode("overwrite").saveAsTable("people")
24/08/29 17:54:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/08/29 17:54:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/08/29 17:54:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/08/29 17:54:57 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore seokyunha@127.0.2.2
24/08/29 17:54:57 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/08/29 17:54:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
24/08/29 17:54:58 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
24/08/29 17:54:58 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/08/29 17:54:58 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/08/29 17:54:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
```

ìœ„ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´, ë¡œì»¬ì— ë‘ í´ë”ê°€ ìë™ìœ¼ë¡œ ìƒì„±ëœë‹¤.

- `./metastore_db/`
  - ìë™ìœ¼ë¡œ ìƒì„±ëœ Hive Metastoreë‹¤.
  - Apache Derbyë¼ëŠ” ìˆœìˆ˜ ìë°”ë¡œ êµ¬í˜„ëœ RDBMSë‹¤.
    - ì•½ê°„ sqlite ê°™ì€ ë…€ì„ì„.
  - ì´ê³³ì— Database, Table ê·¸ë¦¬ê³  ì»¬ëŸ¼ ì •ë³´ ë“±ì´ ì €ì¥ëœë‹¤.
- `./spark-warehouse/people`
  - `.saveAsTable()`ë¡œ ì €ì¥í•œ Hive í…Œì´ë¸”ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì €ì¥ëœ ê²½ë¡œ

ë¡œì»¬ì— Hive Metastoreë¥¼ êµ¬ì¶•í–ˆê¸° ë•Œë¬¸ì—, `spark-shell`ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ë„ ê°™ì€ ê²½ë¡œì—ì„œ ì‹¤í–‰ë§Œ í•œë‹¤ë©´, `metastore_db` í´ë”ë¥¼ í†µí•´ ì´ì „ì— ì €ì¥í–ˆë˜ Hive í…Œì´ë¸” ì •ë³´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.


```scala
scala> spark.sql("SELECT * FROM default.people").show
+----+-------+
| age|   name|
+----+-------+
|NULL|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
```

`HiveContext`ë¥¼ ì‚¬ìš©í•´ì„œ HiveQLì„ ë‚ ë¦¬ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ í•˜ë©´ ëœë‹¤.

```scala
scala> import org.apache.spark.sql.hive.HiveContext
scala> val hiveContext = new HiveContext(sc)
scala> hiveContext.sql("SELECT * FROM default.people")
```

# Multiple SparkSession, but only one SparkContext

í•˜ë‚˜ì˜ Spark Cluster ì•ˆì— SparkSessionì€ ë™ì‹œì— ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ê±°ë‚˜ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, SparkContextì€ ì˜¤ì§ í•˜ë‚˜ë§Œ ì¡´ì¬í•œë‹¤.

ì¦‰, ì—¬ëŸ¬ SparkSessionì´ í•˜ë‚˜ì˜ ê°™ì€ SparkContextë¥¼ ì„œë¡œ ê³µìœ í•œë‹¤ëŠ” ëœ»ì´ë‹¤. Sparkì—ì„œ Multiple Spark Sessionì„ ì§€ì›í•˜ê¸° ë•Œë¬¸ì—, ê°ìì˜ Databricks ë…¸íŠ¸ë¶(ë˜ëŠ” Jupyter ë…¸íŠ¸ë¶)ì—ì„œ ìƒì„±í•˜ëŠ” `TEMP VIEW`ì˜ ì´ë¦„ì´ ê°™ì•„ë„ ì„œë¡œ ê°„ì„­ í•˜ì§€ ì•ŠëŠ”ë‹¤.

Databricksì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‘ ë…¸íŠ¸ë¶ì—ì„œ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶™ì—¬ í™•ì¸í•´ë³´ë©´

![](/images/development/spark/multiple-spark-session-but-one-spark-context.png){: .align-center style="max-height: 400px" }
Spark Sessionì€ ì„œë¡œ ë‹¤ë¥¸ IDë¥¼ ê°™ì§€ë§Œ, Spark ContextëŠ” ê°™ì€ IDë¥¼ ê°–ëŠ”ë‹¤.
{: .align-caption .text-center .small .gray }

ìœ„ì™€ ê°™ì´ `sc`ì˜ IDëŠ” ë™ì¼í•œ ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë•Œ IDëŠ” ì–´ë–¤ `YYYYMMDDHHmmss` í¬ë§·ì¸ë°, Databricks í´ëŸ¬ìŠ¤í„°ê°€ ì‹œì‘ëœ UTC ì‹œê°„ì´ ID ê°’ì´ ë˜ì—ˆë‹¤.


# ë§ºìŒë§

ì´ë²ˆ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ë©´ì„œ, `SparkSession`ê³¼ `SparkContext`, `SQLContext`, `HiveContext`ê¹Œì§€, ëª¨í˜¸í•˜ê²Œ ì•Œë˜ ê°œë…ì„ ì´í•´í•˜ê²Œ ëœ ê²ƒ ê°™ë‹¤. Sparkì™€ Hiveë„ ë‘˜ì´ ì™œ ë§¨ë‚  ì—®ì´ëŠ”ì§€ ê¶ê¸ˆí–ˆëŠ”ë°, ì´ê²ƒë„ ì–´ë ´í’‹ì´ ì•Œê²Œ ëœ ê²ƒ ê°™ë‹¤. "`RDD`ì™€ `DataFrame`ì˜ ì°¨ì´ì ì€?" ê°™ì€ ì§ˆë¬¸ë„ ìƒˆë¡­ê²Œ ìƒê²¨ë‚¬ë‹¤.

ë¬¼ë¡  ì•„ì§ë„ Databricksì˜ Shared-modeì—ì„œ Unity Catalogì—ì„œ ì™œ `SparkContext`, `SQLContext`, `RDD`ë¥¼ ì œí•œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ, Sparkë¥¼ ê³µë¶€í•˜ë‹¤ë³´ë©´ ê³§ ì•Œê²Œ ë˜ê² ì§€...!?
