---
title: "Spark Jobs, Stages and Tasks"
toc: true
toc_sticky: true
categories: ["Spark"]
excerpt: "Sparkê°€ Lazy Evaluationì„ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •. Narrow Transformationê³¼ Wide Transformation. RDD Partitionê³¼ Spark Taskì— ëŒ€í•´."
---

Databricks Certification ì·¨ë“ì„ ëª©í‘œë¡œ Apache Sparkë¥¼ â€œì œëŒ€ë¡œâ€ ê³µë¶€í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. íšŒì‚¬ì—ì„  Databricks Unity Catalogë¥¼ ë„ì…í•˜ë ¤ê³  ë¶„íˆ¬í•˜ê³  ìˆëŠ”ë°ìš”. Sparkì™€ ì¢€ ì¹œí•´ì§ˆ ìˆ˜ ìˆì„ê¹Œìš”? ğŸ‡ ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Development - Spark](/topic/development#apache-spark)ì—ì„œ í™•ì¸í•´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}

# ë“¤ì–´ê°€ë©°

![](/images/development/spark/databricks-spark-jobs.png){: .align-center style="max-height: 300px" }
Databricks
{: .align-caption .text-center .small .gray }

íšŒì‚¬ì—ì„œ Sparkë¥¼ Databricksì—ì„œ ì‚¬ìš©í•˜ë©´ì„œ, ìš”ëŸ° Spark Jobs í™”ë©´ê³¼ Stage ë“¤ì´ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ ëŠ˜ ê¶ê¸ˆí–ˆë‹¤.


# Spark Actionê³¼ Spark Job

RDD ë˜ëŠ” DataFrameì— ì–´ë–¤ ì—°ì‚°ì„ ìˆ˜í–‰í•´ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì—°ì‚°ì´ë‹¤. Actionì„ í˜¸ì¶œí•˜ë©´, Sparkê°€ ì—°ì‚°ì— ëŒ€í•œ DAG ê·¸ë˜í”„ë¥¼ ìƒì„±í•´ ì‹¤í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë¡œì»¬ ë³€ìˆ˜ì— ë‹´ëŠ”ë‹¤.

Spark Actionì€ Sparkì—ì„œ ë§Œë“  Query Execution Planì´ ì‹¤ì œë¡œ ì‹¤í–‰ë˜ëŠ” ì¦‰, "Lazy Evaluation"ì„ ì‹¤í–‰í•˜ëŠ” í–‰ë™ì´ë‹¤. Spark Actionì˜ ì˜ˆì‹œë¡œëŠ” ì•„ë˜ ì‘ì—…ë“¤ì´ ìˆë‹¤.

- `.collect()`
- `.count()`
- `.take(n)`
- `.reduce(func)`
- `.saveAsTextFile(path)`

# Spark Transformationê³¼ Spark Stage

Spark Transformationì€ ê¸°ì¡´ RDDì—ì„œ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•˜ëŠ” ì—°ì‚°ì´ë‹¤. ëª¨ë“  Transformationì€ ì¦‰ì‹œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤. Query Planì— ë°˜ì˜ë§Œ ë˜ê³ , Spark Actionì´ íŠ¸ë¦¬ê±° ë  ë•Œì— ê³„ì‚°ë˜ëŠ” ê²ƒì´ë‹¤. Spark Transformationì˜ RDD ì—°ì‚°ì˜ ì˜ˆì‹œë¡œëŠ” ì•„ë˜ ì—°ì‚°ë“¤ì´ ìˆë‹¤.

- `.map()`
- `.filter()`
- `.groupByKey()`
- `.sortByKey()`
- `.join()`

ê·¸ë¦¬ê³  Transformation ì—°ì‚°ì€, ê·¸ê²ƒì´ í•˜ë‚˜ì˜ ë…¸ë“œì—ì„œ ë°”ë¡œ ìˆ˜í–‰ í•  ìˆ˜ ìˆëŠ”ì§€, ì•„ë‹ˆë©´ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚°ë˜ì–´ ìˆì–´ ì—°ì‚°ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë…¸ë“œ ì‚¬ì´ì— ë°ì´í„° ì´ë™ì´ í•„ìš”í•œì§€ì— ë”°ë¼ Narrow Trans.ì™€ Wide Trans.ë¡œ ë‚˜ë‰œë‹¤.

## Narrow Transformation

![](/images/development/spark/narrow-transformation.png){: .align-center style="max-height: 300px" }
[Databricks: Data Engineers Guide to Apache Spark and Delta Lake](https://www.databricks.com/resources/ebook/the-data-engineers-guide-to-apache-spark-and-delta-lake)
{: .align-caption .text-center .small .gray }

ì—°ì‚°ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê° input íŒŒí‹°ì…˜ì˜ ì˜¤ì§ í•˜ë‚˜ì˜ output íŒŒí‹°ì…˜ì˜ ê²°ê³¼ì—ë§Œ ê¸°ì—¬í•˜ëŠ” ì—°ì‚°ì´ë‹¤. ëŒ€í‘œì ìœ¼ë¡œ `.filter()` ë˜ëŠ” `.where()` ì—°ì‚°ì´ ì´ì— í•´ë‹¹í•œë‹¤.

RDDì˜ `.map()` í•¨ìˆ˜ë„ 1-to-1 ì—°ì‚°ì´ê¸° ë•Œë¬¸ì— Narrow Trans.ì— í•´ë‹¹í•œë‹¤.

`.union()` ì—°ì‚°ë„ ê° input íŒŒí‹°ì…˜ì´ union RDDì˜ í•œ íŒŒí‹°ì…˜ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ì´ë™í•˜ê¸° ë•Œë¬¸ì— Narrow Trans.ë¡œ ë¶„ë¥˜í•œë‹¤.

ë˜, ë§Œì•½ Join í•˜ëŠ” ë‘ íŒŒí‹°ì…˜ì´ ì„œë¡œ "co-partitioned" ë˜ì–´ ìˆë‹¤ë©´, Joinì„ ìœ„í•œ ì…”í”Œë§ì´ ë°œìƒí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— Narrow Trans.ê°€ ëœë‹¤. ë°ì´í„°ê°€ "co-partitioned" ë˜ì–´ ìˆë‹¤ëŠ” ë§ì€ Join í•˜ëŠ” ë°ì´í„°ê°€ Join Jeyë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…˜ ë˜ì–´ ìˆëŠ” ê²½ìš°ë¥¼ ë§í•œë‹¤.

## Wide Transformation

![](/images/development/spark/wide-transformation.png){: .align-center style="max-height: 300px" }
[Databricks: Data Engineers Guide to Apache Spark and Delta Lake](https://www.databricks.com/resources/ebook/the-data-engineers-guide-to-apache-spark-and-delta-lake)
{: .align-caption .text-center .small .gray }

WideëŠ” Narrowì™€ ë°˜ëŒ€ë¡œ ì—°ì‚°ì„ ìœ„í•´ ë°ì´í„°ì˜ ì´ë™ì´ ë°œìƒí•œë‹¤. ì´ê²ƒì„ "ì…”í”Œë§(Shuffling)"ì´ë¼ê³  í•œë‹¤.

`.groupByKey()`ë¡œ ê° íŒŒí‹°ì…˜ì— í©ì–´ì§„ ë°ì´í„°ë¥¼ ê·¸ë£¹í•‘ í•˜ê±°ë‚˜, ì„œë¡œ co-partitioned ë˜ì§€ ì•Šì€ ë‘ ë°ì´í„°ë¥¼ Join í•  ë•Œë„ ì…”í”Œë§ì´ ë°œìƒí•˜ë©°, ì´ë“¤ì„ Wide Trans.ë¡œ ë¶„ë¥˜í•œë‹¤.

ë˜, ë°ì´í„°ë¥¼ ì •ë ¬í•˜ëŠ” `.sortByKey()` ì—­ì‹œ í•œ íŒŒí‹°ì…˜ì— ìˆë˜ ë°ì´í„°ê°€ ë‹¤ë¥¸ Nê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ í©ì–´ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, Wide Trans.ì´ë‹¤.

## Skipped Stage

![](/images/development/spark/skipped-stage.png){: .align-center style="width: 100%" }

> The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they donâ€™t need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. - [Databricks](https://docs.databricks.com/en/compute/troubleshooting/debugging-spark-ui.html)

ì¦‰, SparkëŠ” ì´ë¯¸ ê³„ì‚°ë˜ì–´ checkpointì— ë‹´ì•˜ê±°ë‚˜ ìºì‹±í•œ Stageì˜ ë°ì´í„°ëŠ” ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•Šê³  ì¬ì‚¬ìš©í•œë‹¤.


# RDDì˜ Partitionê³¼ Spark Task

Partitionì€ RDDì˜ ê°œë…ìœ¼ë¡œ, Spark Executorê°€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìµœì†Œ ë‹¨ìœ„ì´ë‹¤. ê´€ì ì€ ë…¼ë¦¬ì , ë¬¼ë¦¬ì  2ê°€ì§€ ê´€ì ì—ì„œ ëª¨ë‘ í•´ì„í•˜ëŠ”ë°,

**[ë…¼ë¦¬ì  ê´€ì ]**
- RDD ë°ì´í„°ëŠ” íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• ë˜ì–´ ì²˜ë¦¬ëœë‹¤.
- ê° íŒŒí‹°ì…˜ì€ ë¶ˆë³€ì„±ì„ ê°€ì§€ê³ , Transformationì„ ìˆ˜í–‰í•˜ë©´ ê¸°ì¡´ RDDì—ì„œ ìƒˆë¡œìš´ RDDê°€ ìƒê¸°ëŠ” ê²ƒì´ë‹¤.
- íŒŒí‹°ì…˜ì´ ì²˜ë¦¬ë˜ëŠ” ë°©ì‹ì— ë”°ë¼ Narrow Trans.ì™€ Wide Trans.ë¡œ ë¶„ë¥˜í•œë‹¤.

**[ë¬¼ë¦¬ì  ê´€ì ]**
- Spark Executorì—ëŠ” ë°ì´í„°ê°€ Partition ë‹¨ìœ„ë¡œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¡´ì¬í•œë‹¤.
- ë…¼ë¦¬ì  ê´€ì ì—ì„œëŠ” íŒŒí‹°ì…˜ í¬ê¸°ë¥¼ ë…¼í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ë¬¼ë¦¬ì  í™˜ê²½ì—ì„œëŠ” íŒŒí‹°ì…˜ í¬ê¸°ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- ê°™ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë”ë¼ë„
  - íŒŒí‹°ì…˜ ì‚¬ì´ì¦ˆê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¦ì€ I/Oë¡œ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•˜ë©°
  - íŒŒí‹°ì…˜ ì‚¬ì´ì¦ˆê°€ ë„ˆë¬´ í¬ë©´, íŠ¹ì • ë…¸ë“œì— ë¶€í•˜ê°€ ì»¤ì§ˆ ìˆ˜ë„ ìˆê³ , ë” ë§ì€ Memoryê°€ í•„ìš”í•´ Spillì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

<br/>

í•˜ë‚˜ì˜ Partitionì€ í•˜ë‚˜ì˜ Taskì— ëŒ€ì‘ë˜ê³ , í•˜ë‚˜ì˜ TaskëŠ” Executorì˜ CPU 1 coreë¥¼ ì‚¬ìš©í•´ ì²˜ë¦¬í•œë‹¤. ì •í™•íˆ ë§í•˜ìë©´, `spark.task.cpus`ì— ëª…ì‹œëœ ê°’ë§Œí¼ì˜ CPU coreë¥¼ ì‚¬ìš©í•´ ì²˜ë¦¬í•˜ëŠ”ë°, ì´ê²ƒì˜ default ê°’ì´ `1`ì´ê¸° ë•Œë¬¸ì—, í•˜ë‚˜ì˜ Taskë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 1 coreë¥¼ ì‚¬ìš©í•œë‹¤ê³  ë§í•œë‹¤. ì°¸ê³ ë¡œ `spark.task.cpus` ê°’ì€ ì •ìˆ˜ ë‹¨ìœ„ë¡œë§Œ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.

Executor ìª½ì—ë„ íŠœë‹í•  ìˆ˜ ìˆëŠ” ê°’ë“¤ì´ ì¢€ ìˆëŠ”ë°,

- `spark.executor.cores`
  - Executorì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ CPU ì½”ì–´ ê°¯ìˆ˜.
  - Standalone ëª¨ë“œë¥¼ ì‚¬ìš©í•œë‹¤ë©´, Executorì˜ ê°€ìš© ì½”ì–´ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.
- `spark.executor.memory`
  - í•˜ë‚˜ì˜ Taskê°€ ì‚¬ìš© ê°€ëŠ¥í•œ Memory ì‚¬ì´ì¦ˆ
  - ê¸°ë³¸ê°’ì€ `1g`

<br/>

ìœ„ì—ì„œ ì‚´í´ë³¸ StageëŠ” Nê°œ Taskë¥¼ ë¬¶ì€ ê°œë…ì´ë‹¤. ê·¸ë¦¬ê³  ê° TaskëŠ” ì„œë¡œ ë‹¤ë¥¸ íŒŒí‹°ì…˜ì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ê°™ì€ Stageì— ë¬¶ì¸ TaskëŠ” ê° ë…¸ë“œì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰ëœë‹¤.

Spark Taskì™€ RDD Partitionì€ 1-to-1 ê´€ê³„ì´ê¸° ë•Œë¬¸ì—, **ê²°êµ­ Task ê°¯ìˆ˜ëŠ” RDDë¥¼ ì–¼ë§ˆë‚˜ íŒŒí‹°ì…˜ í•˜ëŠëƒì— ë‹¬ë ¸ë‹¤.** ê·¸ë¦¬ê³  ì´ ê°’ì€ ì•„ë˜ì˜ ë‘ Spark Configì— ì˜í•´ ê²°ì •ëœë‹¤.

- `spark.default.parallelism`
  - RDD ë°ì´í„°ì— Transformation ì—°ì‚°ì„ í–ˆì„ ë•Œ, ë¦¬í„´ë˜ëŠ” íŒŒí‹°ì…˜ ê°¯ìˆ˜.
  - ê¸°ë³¸ìœ¼ë¡œ ì„¸íŒ…ëœ ê°’ì€ ì•„ë˜ì˜ ê·œì¹™ì„ ë”°ë¥¸ë‹¤.
    - `.reduceByKey()`, `.join()`ê³¼ ê°™ì€ ì—°ì‚°ì€ parent RDD ì¤‘ ê°€ì¥ í° íŒŒí‹°ì…˜ ê°¯ìˆ˜ë¥¼ parallelismìœ¼ë¡œ ê°–ëŠ”ë‹¤.
    - `.parallelize()`ì™€ ê°™ì´ parent RDDê°€ ì—†ëŠ” ê²½ìš°ëŠ”
      - Local Modeì—ì„œëŠ” local machineì˜ ì½”ì–´ ê°¯ìˆ˜ë¡œ
      - Standalone Modeì—ì„œëŠ” Worker Nodeì˜ ì½”ì–´ ê°¯ìˆ˜ë¡œ
      - Mesosì™€ ê·¸ì™¸ ëª¨ë“œì—ì„œëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ .
- `spark.sql.shuffle.partitions`
  - ë°ì´í„°ë¥¼ ì…”í”Œë§ í•  ë•Œì˜ ê¸°ë³¸ íŒŒí‹°ì…˜ ê°¯ìˆ˜ì´ë‹¤.
  - ê¸°ë³¸ê°’ì€ `200`

<br/>

![](/images/development/spark/spark-ui-executors.png){: .align-center style="width: 100%" }
Databricks
{: .align-caption .text-center .small .gray }

Spark UIì˜ í•œ í™”ë©´ ì¤‘ í•˜ë‚˜ì¸ë°, Spark ì‘ì—…ì„ ìµœì í™” í•˜ë©´ì„œ ë§ì´ ë³´ë˜ í™”ë©´ì´ë‹¤. ë‘ íƒœìŠ¤í¬ë¥¼ ë¹„êµí•˜ì—¬, ë°ì´í„° I/Oê³¼ Spark Configê°€ ëª¨ë‘ ê°™ë‹¤ë©´, Task ê°¯ìˆ˜ê°€ ì ì–´ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ, ê·¸ë¦¬ê³  Total Task Timeì´ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™” ë˜ê³  ìˆëŠ”ì§€ ì²´í¬ í–ˆì—ˆë‹¤.

# References

- [Spark: RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)
- [Databricks: Debugging with the Apache Spark UI](https://docs.databricks.com/en/compute/troubleshooting/debugging-spark-ui.html)