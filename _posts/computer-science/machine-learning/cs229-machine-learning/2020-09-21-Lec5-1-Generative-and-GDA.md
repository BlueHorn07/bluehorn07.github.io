---
title: "Generative Learning Algorithm, and GDA"
layout: post
tags: [machine_learning]
---


ë³¸ ê¸€ì€ 2018-2í•™ê¸° Stanford Univ.ì˜ Andrew Ng êµìˆ˜ë‹˜ì˜ Machine Learning(CS229) ìˆ˜ì—…ì˜ ë‚´ìš©ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

GDA(Gaussian Discriminant Analysis)ë¼ëŠ” ê¸°ë²•ì´ ë“±ì¥í•œë‹¤. ì´ë¦„ì´ í›„ëœëœ í•˜ê²Œ ìƒê²¼ì§€ë§Œ, ì´ë¡ ì€ ë³„ê±° ì—†ë‹¤. ì•ˆì‹¬í•˜ê³  ë‹¤ì´ë¸ŒğŸ¤¿í•˜ì!

-- [lecture 5](https://youtu.be/nt63k3bfXS0)

<hr>

## Generative Learning Algorithm

ìš°ë¦¬ê°€ ì‚´í´ë³¸ **Logistic Regression** ëª¨ë¸ë“¤ì€  Discriminative Learningì— ì†í•˜ëŠ” ëª¨ë¸ì´ì—ˆë‹¤.

Discriminative ëª¨ë¸ì—ì„œëŠ” $p(y \vert x)$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë°˜ë©´ì— **Generative ëª¨ë¸**ì€ $p(x \vert y)$ì™€ $p(y)$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.[^1]

Generative Learningì€ **Bayes Rule**ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ëŠ” ì´ë¡ ì´ë‹¤. 

<div>
$$p(y \vert x) = \frac{p(x \vert y) p(y)}{p(x)}$$
</div>

ìš°ë¦¬ê°€ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ $p(y \vert x)$ì´ë‹¤. Discriminative ëª¨ë¸ì€ $p(y \vert x)$ë¥¼ í•™ìŠµí•˜ëŠ” ë°˜ë©´, Generative ëª¨ë¸ì€ $p(x \vert y)$ì™€ $p(y)$ë¥¼ ì •ì˜í•˜ê³  í•™ìŠµí•˜ì—¬ $p(y \vert x)$ì˜ ê°’ì„ ê°„ì ‘ì ìœ¼ë¡œ ìœ ë„í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<div>
$$
\begin{aligned}
\arg{ \max_{y} {p(y \vert x)}} &= \arg{ \max_{y} {\frac{p(x \vert y)p(y)}{p(x)}}} \\
&= \arg{ \max_{y} {p(x \vert y)p(y)}}
\end{aligned}
$$
</div>

ê²°êµ­ Discriminativeë‚˜ Generativeë‚˜ í° íë¦„ì€ ë™ì¼í•˜ì§€ë§Œ, êµ¬í•˜ëŠ” ê³¼ì •ì´ directì´ë‚˜ indirectì´ë‚˜ì˜ ì°¨ì´ì¼ ë¿ì´ë‹¤.

### (ì‚¬ì „ì§€ì‹) Bayes Rule

<div>
$$p(y \vert x) = \frac{p(x \vert y) p(y)}{p(x)}$$
</div>

Bayes Ruleì˜ ìš©ì–´ë¥¼ ì •ë¦¬í•´ë³´ì.
- $p(y \vert x)$: **posterior probability**
  - ë°ì´í„° Xì— ëŒ€í•œ ë ˆì´ë¸” Yì˜ í™•ë¥ ì´ë‹¤.
  - Classificationì˜ ê¸°ì¤€ì´ ëœë‹¤.
- $p(y)$: **prior probability**
  - ì •ë‹µ ë ˆì´ë¸”ì˜ ë¶„í¬ë¥¼ í†µí•´ ì–»ëŠ”ë‹¤.
  - ë ˆì´ë¸” yì˜ ìˆ˜ / ì „ì²´ ë°ì´í„° ìˆ˜
- $p(x \vert y)$: **likelihood**
  - ë ˆì´ë¸” Yë¥¼ ê°–ëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤.
  - Generative Modelì€ ì´ í™•ë¥ ì„ ëª¨ë¸ë§í•˜ê³  ë˜ í•™ìŠµí•œë‹¤.
- $p(x)$
  - ë³´í†µ ê°’ì„ êµ¬í•  ìˆ˜ë„ ì—†ê³ , êµ¬í•  í•„ìš”ë„ ì—†ë‹¤.
  - ê·¸ë˜ì„œ ì˜ ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤.

<br/>

ë¶„ë¥˜ ë¬¸ì œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§í•˜ìë©´, *Logistic Regression*ì´ Discriminative Modelì— ì†í•œë‹¤. [*Naive Bayes Classifier*](https://poapper.github.io/pytorch-seminar/2021/12/26/seminar-6.html)ëŠ” Generative Modelì— ì†í•œë‹¤. ë˜ ì•„ë˜ì— ì–¸ê¸‰ë˜ëŠ” GDA(Gaussian Discriminant Analysis)ë„ Generative Modelì´ë‹¤.

<hr>

### Gaussian Discriminant Analysis<sub>GDA</sub>

GDAëŠ” ì´ë¦„ì— 'Discriminant'ê°€ ë“¤ì–´ê°€ì§€ë§Œ, Generative Modelì´ë‹¤. GDAì—ì„œëŠ” $p(x \vert y)$ê°€ **multivariate normal distribution**ì„ ë§Œì¡±í•œë‹¤ê³  'ê°€ì •'í•œë‹¤. GDAì— ëŒ€í•´ ë³¸ê²©ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ì „ì— multivariate normal distributionì„ ê°€ë³ê²Œ ì‚´í´ë³´ì.

#### (ì‚¬ì „ì§€ì‹) Multi-variate normal distribution

ë°”íƒ•ì´ ë˜ëŠ” **uni-variate Gaussian ë¶„í¬**ë¥¼ ë¨¼ì € ì‚´í´ë³´ì.

<div>
$$\mathcal{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\left[ -\frac{(x-\mu)^2}{2\sigma^2}\right]}$$
</div>

- $E[x]=\mu$
- $\textrm{Cov}(x) = E[(x-\mu)^2]$

ì´ì œ **multivariate Gaussian ë¶„í¬**ì˜ ê²½ìš°ë¥¼ ì‚´í´ë³´ì. multivariate Gaussianì˜ ê²½ìš° í‰ê· ì€ **mean vector** $\mu \in \mathbb{R}^n$ë¡œ, ë¶„ì‚°ì€ **ê³µë¶„ì‚°**<sub>Covariance</sub>ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ covariance matrix $\Sigma \in \mathbb{R}^{n \times n}$ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

<div>
$$\mathcal{N}(X; \mu, \Sigma) = \frac{1}{\sqrt{2\pi}{\lvert \Sigma \rvert}^{1/2}} \exp{\left[ -\frac{1}{2}(X - \mu)^{T}\Sigma^{-1}(X-\mu) \right]}$$
</div>

ì´ë•Œ, $\lvert \Sigma \rvert$ëŠ” Covariance Matrix $\Sigma$ì˜ determinant ê°’ì´ë‹¤. ê·¸ë¦¬ê³  í‰ê· ê³¼ ê³µë¶„ì‚°ì— ëŒ€í•œ ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- $E[X] = \int_{x}{x p(x; \mu, \Sigma) dx}$
- $\textrm{Cov}(X) = E[(X-E[X])(X-E[X])^{T}]$
- $\mu$ê°€ zero-vector(=zero mean)ì´ê³  $\Sigma = I$(=identity covariance)ì¸ ê²½ìš°ë¥¼ **standard normal distribution**ì´ë¼ê³  í•œë‹¤.

### GDA Modeling

binary classification ë¬¸ì œë¥¼ GDAë¡œ ëª¨ë¸ë§ í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê°€ì •ì„ í•œë‹¤.

- $y \sim \textrm{Bernoulli}(\phi)$
  - ì´ ë¶€ë¶„ì€ ê°€ì •ì´ ì•„ë‹ˆë‹¤. ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¼ì„œ $y$ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ì¼ ìˆ˜ ë°–ì— ì—†ë‹¤.
  - $\phi = 0.5$ë¼ë©´ uniform distributionì´ ë  ê²ƒì´ë‹¤.
  - ì°¸ê³ ë¡œ $y$ì— ëŒ€í•œ ë¶„í¬ëŠ” ì–´ë–¤ ë¬¸ì œë¥¼ í‘¸ëŠ”ì§€ì— ë”°ë¼ ìë™ìœ¼ë¡œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì— ê°€ì •ì„ ë„ì…í•˜ëŠ” ë¶€ë¶„ì´ ì•„ë‹ˆë‹¤.

- $x \vert y = 0 \sim \mathcal{N}(\mu_0, \Sigma)$
- $x \vert y = 1 \sim \mathcal{N}(\mu_1, \Sigma)$

<br/>

ë¶„í¬ë¥¼ ì‹ìœ¼ë¡œ ê¸°ìˆ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

- $p(y) = \phi^y (1-\phi)^{(1-y)}$
- $p(x \vert y=0) = \frac{1}{\sqrt{2\pi}{\lvert \Sigma \rvert}^{1/2}}\exp{\left[ -\frac{1}{2}(x - \mu_0)^{T}\Sigma^{-1}(x-\mu_0) \right]}$
- $p(x \vert y=1) = \frac{1}{\sqrt{2\pi}{\lvert \Sigma \rvert}^{1/2}}\exp{\left[ -\frac{1}{2}(x - \mu_1)^{T}\Sigma^{-1}(x-\mu_1) \right]}$

ìš°ë¦¬ì˜ GDA ëª¨ë¸ì˜ parameterë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- $\phi \in \mathbb{R}$
- $\mu_0, \mu_1 \in \mathbb{R}^n$
- $\Sigma \in \mathbb{R}^{n \times n}$ [^3]

ìš°ë¦¬ëŠ” ìœ„ì˜ $\phi$, $\mu_0$, $\mu_1$, $\Sigma$ë¥¼ í•™ìŠµì‹œí‚¬ ê²ƒì´ë‹¤!

<br/>

Joint Likelihood $L(\phi, \mu_0, \mu_1, \Sigma)$ë¥¼ ì •ì˜í•´ë³´ì.

<div>
$$
\begin{aligned}
  L(\phi, \mu_0, \mu_1, \Sigma) &= \prod_{i=1}^{m}{p(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma)} \\
  &= \prod_{i=1}^{m}{ p(x^{(i)} \vert y^{(i)})p(y^{(i)}) }
\end{aligned}
$$
</div>

ì ì‹œ Discriminant Learningì—ì„œì˜ Conditional Likelihoodì™€ ë¹„êµí•´ë³´ì.

<div>
$$L(\theta) = \prod_{i=1}^{m}{p(y^{(i)} \vert x^{(i)}; \theta)}$$
</div>

parameterì˜ ì¸¡ë©´ì—ì„œ $\theta$ì™€ $\phi$, $\mu_0$, $\mu_1$, $\Sigma$ë¡œ ì°¨ì´ê°€ ìˆê³ , Maximize ëŒ€ìƒë„ Discriminant Learningì˜ ê²½ìš° $p(y \vert x)$ë¥¼ Maximizeí•˜ëŠ” ë°˜ë©´ Generative Learningì€ $p(x \vert y)p(y)$ë¥¼ Maximizeí•˜ê³  ìˆë‹¤.

### MLE on GDA

ì •ì˜í•œ $L(\phi, \mu_0, \mu_1, \Sigma)$ë¥¼ Maximize í•˜ì. ì´ë•Œ, $L(\phi, \mu_0, \mu_1, \Sigma)$ì— $\log$ë¥¼ ì·¨í•œ $l(\phi, \mu_0, \mu_1, \Sigma)$ë¥¼ ëŒ€ì‹  Maximizeí•œë‹¤.

<div>
$$\max_{\{ \phi, \mu_0, \mu_1, \Sigma \}} {\left[ l(\phi, \mu_0, \mu_1, \Sigma) \right]}$$
</div>

$l$ì„ Maximizing í•˜ëŠ” parameterì˜ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ê°•ì˜ì—ì„œë„ ìœ ë„ ê³¼ì •ì€ ìƒëµí•˜ì˜€ë‹¤. (ì•„ë§ˆ parameter í•˜ë‚˜ ì¡ê³  ë¯¸ë¶„í•´ì„œ ìœ ë„í•  ë“¯?)

- $\phi = \frac{\sum_{i=1}^{m} {y^{(i)}}}{m} = \frac{\sum_{i=1}^{m} {1\\{y^{(i)}=1\\}}}{m}$

<br/>

- $\mu_0 = \frac{\sum_{i=1}^{m} { 1\\{y^{(i)}=0\\} x^{(i)} }}{\sum_{i=1}^{m} {1\\{y^{(i)}=0\\}}}$
- $\mu_1 = \frac{\sum_{i=1}^{m} { 1\\{y^{(i)}=1\\} x^{(i)} }}{\sum_{i=1}^{m} {1\\{y^{(i)}=1\\}}}$
- $\Sigma = \frac{\sum_{i=1}^{m} {(x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^{T}}}{m}$


$\mu_0$ì„ ì˜ ì‚´í´ë³´ì. $\mu_0$ì˜ ê²°ê³¼ë¥¼ ë§ë¡œ í’€ì–´ì“°ë©´, "$y=0$ì¸ feacture vectorë“¤ì˜ í•©ì„ $y=0$ì˜ ìˆ˜ë¡œ ë‚˜ëˆˆ ê²ƒ" ì¦‰, í‰ê· ì´ë‹¤!! ì´ ê²°ê³¼ëŠ” $\mu_0$ê°€ $y=0$ì¸ ì •ë‹µì— ëŒ€í•œ í‰ê· ì´ë¼ëŠ” ì •ì˜ì™€ë„ ì˜ë¯¸ê°€ í†µí•œë‹¤.

ì´ ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<div style="text-align: center;">
<img src="{{ "/images/CS229/GDA1.png" | relative_url }}"  style="width: 50%;">
</div>

ìœ„ ê·¸ë¦¼ì— ê·¸ë ¤ì§„ ì§ì„ ì€ $p(y=1 \vert x)=0.5$ê°€ ë˜ëŠ” decision boundaryì˜ ì—­í• ì„ í•œë‹¤!!

MLEì˜ ê²°ê³¼ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” $\phi$, $\mu_0$, $\mu_1$, $\Sigma$ì˜ ì •í™•í•œ ê°’ì„ ì–»ê²Œ ë˜ì—ˆë‹¤. ì´ parameterë“¤ì„ í™œìš©í•´ prediction í•  ìˆ˜ ìˆë‹¤.

<div>
$$\arg{ \max_{y} {p(y \vert x)}} = \arg{ \max_{y} {p(x \vert y)p(y)}}$$
</div>

<hr>

## GDA vs. Logistic Regression

ê³ ì •ëœ $\phi$, $\mu_0$, $\mu_1$, $\Sigma$ì— ëŒ€í•´ $p(y=1 \vert \phi, \mu_0, \mu_1, \Sigma)$ë¥¼ $x$ì˜ í•¨ìˆ˜ë¡œ ê·¸ë ¤ë³´ì.

ê·¸ëŸ¬ë©´,

<div style="text-align: center;">
<img src="{{ "/images/CS229/GDA2.png" | relative_url }}"  style="width: 50%;">
</div>

ì¦‰, $p(y=1 \vert \phi, \mu_0, \mu_1, \Sigma)$ëŠ” sigmoidì˜ shapeì´ ë‚˜ì˜¨ë‹¤!!

<hr>

ìœ„ì˜ ì‚¬ì‹¤ì€ GDAì™€ Logistic Regressionì´ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ì–¸ì œ GDAë¥¼ ì“°ê³ , ì–¸ì œ Logistic Regressionì„ ì¨ì•¼ í• ê¹Œ??

<div style="text-align: center;">
<img src="{{ "/images/CS229/GDA3.png" | relative_url }}"  style="width: 80%;">
</div>


GDAì—ì„œ í•˜ëŠ” ê°€ì •ë“¤ì€ Logistic Regressionì—ì„œ í•˜ëŠ” hypothesis $h_{\theta}(x)$ì˜ sigmoid ê°€ì •ë³´ë‹¤ ë” ê°•ë ¥í•˜ë‹¤. ê·¸ë˜ì„œ GDAëŠ” Logistic Regressionì„ ì•”ì‹œ(imply)í•œë‹¤. ê·¸ëŸ¬ë‚˜ ë°˜ëŒ€ ë°©í–¥ì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì¦‰, $p(y \vert x)$ì´ sigmoidë¼ê³  í•´ì„œ $p(x \vert y)$ê°€ multivariate normal distributionì¸ ê²ƒì€ ì•„ë‹ˆë‹¤.

GDAëŠ” ë” ê°•ë ¥í•œ ê°€ì •ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í˜„ì‹¤ì˜ datasetì´ ê·¸ ê°€ì •ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì•ˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ê·¸ë˜ì„œ datasetì˜ ë¶„í¬ë¥¼ ì •í™•íˆ ì•Œê³  ìˆë‹¤ë©´, GDAë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆì§€ë§Œ ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ Logistic Regressionì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸¸ ê¶Œì¥í•œë‹¤. Logistic Regressionì€ ë” ì ì€ ê°€ì •ì„ ì±„ìš©í•˜ëŠ” ëŒ€ì‹ ì— ë” *robust* í•˜ê³  ì˜ëª»ëœ ëª¨ë¸ë§ì— ëœ ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Logistic Regressionê³¼ ë¹„êµí–ˆì„ ë•Œ, GDAëŠ” small datasetì—ì„œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤ê³  í•œë‹¤. ë°˜ë©´, huge datasetì—ì„œëŠ” Logistic Regressionì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. ë‹¨, ë§Œì•½ $p(x \vert y)$ì— ëŒ€í•œ GDAì˜ ê°€ì •ì´ ì˜³ë‹¤ë©´, huge datasetì—ì„œë„ GDAê°€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤.

ìš”ì¦˜ì€ CIFAR, ImageNetê³¼ ê°™ì€ huge datasetì´ ì˜ êµ¬ì¶•ë˜ì–´ ìˆì–´, Logistic Regressionì´ ë” ê°•ì„¸ë¥¼ ë³´ì¸ë‹¤. ê·¸ëŸ¬ë‚˜ datasetì´ ì˜ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ê±°ë‚˜ datasetì˜ í¬ê¸°ë¥¼ 100ê°œë¡œ ì œí•œí•œ ìƒí™©ì´ë¼ë©´, GDAê°€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤.

<hr>

[^1]: ì‹¤ì œë¡œëŠ” $p(x \vert y)$ë§Œ í•™ìŠµí•˜ê³  $p(y)$ëŠ” í•™ìŠµí•˜ì§€ ì•ŠëŠ”ë‹¤.
[^3]: mean vectorëŠ” $\mu_0$, $\mu_1$ìœ¼ë¡œ ë‘ ê°œì¸ ë°˜ë©´ Covariance matrix $\Sigma$ë¡œ í•˜ë‚˜ì´ë‹¤. ì´ê²ƒ ì—­ì‹œ GDAë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ê³¼ì •ì—ì„œ ë„ì…í•œ ê°€ì • ì¤‘ í•˜ë‚˜ì´ë‹¤. ì¼ì¢…ì˜ design choice! ì›í•œë‹¤ë©´ $\Sigma_1$, $\Sigma_2$ë¡œ ë¶„ë¦¬í•  ìˆ˜ë„ ìˆë‹¤.
