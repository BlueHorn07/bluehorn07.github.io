---
title: "Gaussian Process Regression"
layout: post
tags: ["machine_learning", "time_series_analysis"]
readtime: 30 Minutes
---



"Machine Learning"ì„ ê³µë¶€í•˜ë©´ì„œ ê°œì¸ì ì¸ ìš©ë„ë¡œ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

ë³¸ ê¸€ì„ ì½ê¸° ì „ì— "[Distribution over functions & Gaussian Process]({{"/2021/07/01/Gaussian-process.html" | relative_url}})"ì— ëŒ€í•œ ê¸€ì„ ë¨¼ì € ì½ê³  ì˜¬ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤ ğŸ˜‰

<div class="proof" markdown="1">

**ê¸°íš ì‹œë¦¬ì¦ˆ: Gaussian Process Regression**

1. [ê¸°íš ì‹œë¦¬ì¦ˆ: Bayesian Regression]({{"/2021/09/06/bayesian-regression.html" | relative_url}})
2. [Distribution over functions & Gaussian Process]({{"/2021/07/01/Gaussian-process.html" | relative_url}})
3. [Gaussian Process Regression]({{"/2021/09/21/Gaussian-Process-Regression.html" | relative_url}}) ğŸ‘€

</div>

<hr/>

"[Distribution over functions & Gaussian Process]({{"/2021/07/01/Gaussian-process.html" | relative_url}})"ë¥¼ í†µí•´ Gaussian Processë¡œ í•¨ìˆ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬(distribution over functions)ë¥¼ ì–´ë–»ê²Œ ëª¨ë¸ë§í•˜ëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” distribution over functionsê°€ **Bayesian Regression**ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì•„ë˜ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ë¥¼ ì‚´í´ë³¸ë‹¤ğŸ™Œ

<hr/>

### Gaussian Process Regression

<div class="img-wrapper">
  <img src="https://lh3.googleusercontent.com/-QSlMIVtiVFU/X7SgGoqKYwI/AAAAAAAAOOo/u8vbYa-QeVUW5ppkpHQPQ4hV_CH0vF33wCLcBGAsYHQ/w514-h360/image.png" width="500px">
</div>

ë¨¼ì € \<Gaussian Process Regression; ì´í•˜ GP Regression\>ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì…‹ì—…ì„ í•´ë³´ì.

i.i.d. sampleì˜ ëª¨ì„ì¸ train set $S = \\{ (x_i, y_i)\\}^m_{i=1} = (X, y)$ì€ unknown distributionì—ì„œ ì¶”ì¶œëœ ìƒ˜í”Œì´ë‹¤. ì´ë•Œ, GP Regressionì€ ì•„ë˜ì™€ ê°™ì´ Regression modelì„ êµ¬ì¶•í•œë‹¤.

$$
y_i = h(x_i) + \epsilon_i \quad (i = 1, \dots, m)
$$

ì´ë•Œ $\epsilon_i$ëŠ” i.i.d noiseë¡œ $\epsilon_i \sim N(0, \sigma^2)$ì´ë‹¤. 

[\<Bayesian Regression\>]({{"/2021/09/06/bayesian-regression.html" | relative_url}})ì—ì„  $y_i = \theta^T x_i + \epsilon_i$ ëª¨ë¸ë§í•œ ê²ƒê³¼ ì°¨ì´ì ì´ ìˆë‹¤.

<br/>

ì´ì œ $h(\cdot)$ì— ëŒ€í•´ **prior distribution over function**ì— ëŒ€í•œ ê°€ì •ì„ ë„ì…í•œë‹¤.[^1] 'prior'ê°€ ë¶™ì€ ê²ƒì„ ëˆˆì¹˜ì±˜ë‹¤ë©´ ì´ê²ƒì„ 
'posterior'ë¡œ ê°±ì‹ í•˜ë¦¬ë¼ëŠ” ê²ƒë„ ì•Œì•„ì±Œ ê²ƒì´ë‹¤ ğŸ™Œ ë¨¼ì € $h(\cdot)$ê°€ zero-mean GPë¼ê³  ê°€ì •í•œë‹¤.

$$
h(\cdot) \sim \mathcal{GP}(0, \; k(\cdot, \cdot))
$$

â€» NOTE: $k(x, x')$ is a valid covariance function.

<br/>

ì´ë²ˆì—ëŠ” $S$ì™€ ë™ì¼í•œ unknown distributionì—ì„œ ì¶”ì¶œí•œ i.i.d. sampleì˜ ëª¨ì„ì¸ test set $$T = \left\{ x^{*}_i, y^{*}_i\right\}^{m_{*}}_{i=1} = (X^{*}, y^{*})$$ë¥¼ ì‚´í´ë³´ì. ì´ì „ì˜ \<Bayesian Regression\>ì—ì„œëŠ” Bayes' ruleì„ ì´ìš©í•´ \<parameter posterior\> $p(\theta \mid S)$ë¥¼ ìœ ë„í•˜ê³ , ì´ê²ƒì„ í†µí•´ \<posterior predictive distribution\> $p(y^{\*} \mid x^{\*}, S)$ë¥¼ ìœ ë„í–ˆë‹¤. ê·¸ëŸ°ë° GP Regressionì—ì„œëŠ” í›¨ì”¬ ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ posterior predictive distributionì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤!! ğŸ˜²

<hr/>

### Prediction

ìš°ë¦¬ëŠ” **prior distribution over function** $h(\cdot) \sim \mathcal{GP}(0, \; k(\cdot, \cdot))$ì„ ì •ì˜í–ˆë‹¤. GPì˜ ì„±ì§ˆì— ë”°ë¼ $\mathcal{X}$ì˜ subsetì¸ $X, X^{\*} \subset \mathcal{X}$ì— ëŒ€í•´ joint distribution $p(\vec{h}, \vec{h^{\*}} \mid X, X^{\*})$ì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\begin{bmatrix}
  \vec{h} \\
  \vec{h^{*}}
\end{bmatrix} \mid X, X^{*}
\sim 
\mathcal{N} \left( \vec{0}, \; \begin{bmatrix}
K(X, X) & K(X, X^{*}) \\
K(X^{*}, X) & K(X^{*}, X^{*})
\end{bmatrix}\right)
$$

matrix-formì˜ í‘œê¸°ê°€ ë§ì´ ë“±ì¥í–ˆì§€ë§Œ ë”°ë¡œ í‘œê¸°ë¥¼ ì„¤ëª…í•˜ì§€ëŠ” ì•Šê² ë‹¤ ğŸ™

ë˜ i.i.d. noiseì— ëŒ€í•´ì„  ì•„ë˜ê°€ ì„±ë¦½í•œë‹¤.

$$
\begin{bmatrix}
  \vec{\epsilon} \\
  \vec{\epsilon^{*}}
\end{bmatrix}
\sim
\mathcal{N} \left( \vec{0}, \; \begin{bmatrix}
  \sigma^2 I & O \\
  O & \sigma^2 I
\end{bmatrix}\right)
$$

ì´ì œ ì´ê±¸ ì¢…í•©í•˜ë©´,

$$
\begin{bmatrix}
  \vec{y} \\
  \vec{y^{*}}
\end{bmatrix} \mid X, X^{*} 
=
\begin{bmatrix}
  \vec{h} \\
  \vec{h^{*}}
\end{bmatrix} \mid X, X^{*}
+
\begin{bmatrix}
  \vec{\epsilon} \\
  \vec{\epsilon^{*}}
\end{bmatrix}
$$

ê°€ ë˜ëŠ”ë°, independent Gaussian random variableì˜ í•©ì€ ì—­ì‹œ Gaussianì´ë¯€ë¡œ

$$
\begin{bmatrix}
  \vec{y} \\
  \vec{y^{*}}
\end{bmatrix} \mid X, X^{*}
\sim
\mathcal{N} \left(\vec{0} , \; \begin{bmatrix}
K(X, X) + \sigma^2 I & K(X, X^{*}) \\
K(X^{*}, X) & K(X^{*}, X^{*}) + \sigma^2 I
\end{bmatrix}\right)
$$

ìœ„ì˜ ì‹ì€ $p(\vec{y}, \vec{y^{\*}} \mid X, X^{\*})$ì— ëŒ€í•œ ì‹ìœ¼ë¡œ "joint distribution of the observed values and testing points"ì´ë‹¤. regressionì€ testing pointsì— ëŒ€í•œ ë¶„í¬ë¥¼ ì›í•˜ë¯€ë¡œ conditional distribution $p(\vec{y^{\*}}, \mid \vec{y}, X, X^{\*})$ì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\vec{y^{*}}, \mid \vec{y}, X, X^{*} \sim \mathcal{N} \left( \mu^{*}, \; \Sigma^{*} \right)
$$

where ($K^{\*} = K(X, X^{*})$)

$$
\begin{aligned}
\mu^{*} &= K^{*} \left( K + \sigma^2 I \right)^{-1}\vec{y} \\
\Sigma^{*} &= K^{**} + \sigma^2 I - {K^{*}}^T \left( K + \sigma^2 I \right)^{-1} K^{*}
\end{aligned}
$$

ìœ ë„ ê³¼ì •ì€ [conditional distribution of multi-variate Guassiaion distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions)ì— ëŒ€í•œ ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ğŸ™Œ

Boom! ì´ê²ƒìœ¼ë¡œ ìš°ë¦¬ëŠ” **posterior predictive distribution**ì„ ì–»ì—ˆë‹¤!! ğŸ¤© ì´ì „ì˜ [Bayesian Linear Regression]({{"/2021/09/06/bayesian-regression.html#bayesian-linear-regression" | relative_url}})ì˜ ê²ƒê³¼ ë¹„êµí•´ë³´ë©´ GP Regressionì€ ì •ë§ ê³„ì‚°ì ìœ¼ë¡œë„ ì •ë§ ê°„ë‹¨í•œ í˜•íƒœì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ ğŸ‘

<details class="math-statement" markdown="1">
<summary>ë³´ì¶©</summary>

ì•ì—ì„œ $h(\cdot)$ê°€ 'prior' distribution over functions ë¼ê³  í–ˆë‹¤. ê·¸ëŸ¼ 'posterior' distribution over functionì„ ìœ ë„í•˜ë©´, ìœ„ì—ì„œ ì–¸ê¸‰í•œ joint distribution $p(\vec{h}, \vec{h^{\*}} \mid X, X^{\*})$ì—ì„œ conditional distributionì„ êµ¬í•˜ë©´ ëœë‹¤.

$$
\begin{bmatrix}
  \vec{h} \\
  \vec{h^{*}}
\end{bmatrix} \mid X, X^{*}
\sim 
\mathcal{N} \left( \vec{0}, \; \begin{bmatrix}
K(X, X) & K(X, X^{*}) \\
K(X^{*}, X) & K(X^{*}, X^{*})
\end{bmatrix}\right)
$$

then, the conditional distribution is

$$
\vec{h^{*}} \mid \vec{h}, X, X^{*} \sim \mathcal{N} \left( {K^{*}}^T K^{-1} \vec{h}, \; K^{**} - {K^{*}}^TK^{-1}K^{*}\right)
$$

ê°€ ëœë‹¤. ì´ê²ƒì´ posterior distribution over function $h(\cdot \mid X)$ì´ë‹¤!

</details>

<div class="img-wrapper">
  <img src="{{ "/images/machine-learning/gaussian-process-regression-1.png" | relative_url }}" width="100%">
  <p markdown="1">ref. [Gaussian processes - Chuong B. Do](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)</p>
</div>

<hr/>

### Insights

ì´ë²ˆ ë¬¸ë‹¨ì—ì„œëŠ” GP Regressionì— ëŒ€í•œ í†µì°°ë“¤ì— ëŒ€í•´ ì‚´í´ë³¼ ê²ƒì´ë‹¤. locally-weighted linear regressionì²˜ëŸ¼ GP Regressionì€ <span style="color: red"><b>non-parameteric regression model</b></span>ì´ë‹¤. ì´ëŠ” input dataì˜ í•¨ìˆ˜ì— ì„ í˜•ì— ëŒ€í•œ ê°€ì •ì´ë‚˜ ë‹¤í•­ì‹ì— ëŒ€í•œ ê°€ì •ì„ í•  í•„ìš”ê°€ ì—†ìœ¼ë©° arbitrary functionì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤! ğŸ¤© ëŒ€í•™ì—ì„œ ë“¤ì—ˆë˜ "í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹(IMEN472)" ìˆ˜ì—…ì—ì„œ [non-parameteric model]({{"/category/statistical-data-mining" | relative_url}})ì— ëŒ€í•´ ë‹¤ë£¨ê¸´ í–ˆëŠ”ë°, \<GP Regression\>ì€ ë‹¤ë£¨ì§€ ì•Šì•˜ë‹¤.

<br/>

GPì—ì„œ ì‚¬ìš©í–ˆë˜ \<squared exponential kernel\> $k_{SE}(x, x')$ì— ëŒ€í•´ ì‚´í´ë³´ì.

$$
k_{SE}(x, x') = \exp \left( - \frac{1}{2\tau^2} (x - x')^2 \right) \quad (\tau > 0)
$$

hyper-parameterì¸ $\tau$ëŠ” smoothnessë¥¼ ì¡°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ $\tau$ ê°’ì´ ì‘ì„ìˆ˜ë¡ ê°€ê¹Œì´ ìˆëŠ” ìƒ˜í”Œì„ ì£¼ë¡œ ë³¸ë‹¤. ê·¸ë˜ì„œ modelì˜ fluctuationì´ ì‹¬í•´ì§„ë‹¤. ë°˜ëŒ€ë¡œ $\tau$ ê°’ì´ ì»¤ì§€ë©´, ë©€ë¦¬ ìˆëŠ” ìƒ˜í”Œë„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— modelì´ smooth í•´ì§„ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/machine-learning/gaussian-process-regression-2.png" | relative_url }}" width=600 height=200>
  <p markdown="1">ref. ['ì†ì“°'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://sonsnotation.blogspot.com/2020/11/11-2-gaussian-progress-regression.html)</p>
</div>

<br/>

ë‹¤ìŒìœ¼ë¡œ regression noiseì¸ $\sigma$(ê·¸ë¦¼ì—ì„œëŠ” $\sigma_y$)ê°€ ìˆë‹¤. ì´ ë…€ì„ì€ uncertaintyì˜ ì •ë„ë¥¼ ê²°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ $\sigma$ ê°’ì´ í´ìˆ˜ë¡ ë°ì´í„°ì˜ noiseê°€ í¬ë‹¤ê³  íŒë‹¨í•œë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/machine-learning/gaussian-process-regression-3.png" | relative_url }}" width=600 height=200>
  <p markdown="1">ref. ['ì†ì“°'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://sonsnotation.blogspot.com/2020/11/11-2-gaussian-progress-regression.html)</p>
</div>

<hr/>

### ë§ºìŒë§

ì§€ê¸ˆê¹Œì§€ \<GP Regression\>ì— ëŒ€í•´ ì‚´í´ë³´ì•˜ë‹¤. ì´ ë…€ì„ì€ <span style="color: red"><b>bayesian regression model</b></span>ì´ë©´ì„œ <span style="color: red"><b>non-parameteric model</b></span>ì¸ ë…€ì„ì´ì—ˆë‹¤. ê²Œë‹¤ê°€ \<GP Regression\>ì„ anomaly detectionì— ì‚¬ìš©í•œë‹¤ë©´, anomaly setì— ëŒ€í•œ labeling ì—†ì´ <span style="color: red"><b>unsupervised learning</b></span>ë¡œ anomaly detectionì— í™œìš©í•  ìˆ˜ ìˆë‹¤ ğŸ˜ ê°œì¸ì ìœ¼ë¡œ GP Regressionì€ ì‹¤ì „ì—ì„œë„ êµ‰ì¥í•œ ì„±ëŠ¥ê³¼ ê·¸ëŸ´ë“¯í•œ ê²°ê³¼ë¥¼ ë„ì¶œí–ˆì–´ì„œ ê½¤ ë§Œì¡±í–ˆë‹¤ ğŸ‘ ìœ„í‚¤í”¼ë””ì•„ì—ì„œëŠ” GP Regressionì„ "kriging"ì´ë¼ê³  ë¶€ë¥´ë˜ë°, ë¬¸ì„œë¥¼ ì½ì–´ë³´ë‹ˆ GP Regressionì— ëŒ€í•œ ë” ê¹Šê³  ë§ì€ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆë‹¤. GP Regressionì´ ë” ê¶ê¸ˆí•˜ë‹¤ë©´ í•´ë‹¹ ë¬¸ì„œë¥¼ ì½ì–´ë³´ì! ğŸ™Œ

ğŸ‘‰ [Kriging(GP Regression)](https://en.wikipedia.org/wiki/Kriging)

<br/>

ë‹¤ìŒ ì‹œë¦¬ì¦ˆë¡œëŠ” MCMC(Markov Chain Monte Carlo)ë¥¼ ìƒê°í•˜ê³  ìˆë‹¤. ëŒ€í•™ì—ì„œ 'ì¸ê³µì§€ëŠ¥' ê³¼ëª© ë“¤ì„ ë•Œ ë³´ê¸´ í–ˆëŠ”ë° ê·¸ë•ŒëŠ” ì œëŒ€ë¡œ ì´í•´ë¥¼ ëª» í–ˆì—ˆë‹¤ ğŸ˜¥

<hr/>

#### references

- [Gaussian processes - Chuong B. Do](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)
- ['ì†ì“°'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://sonsnotation.blogspot.com/2020/11/11-2-gaussian-progress-regression.html)
- [An Intuitive Tutorial to Gaussian Processes Regression](https://arxiv.org/pdf/2009.10862.pdf)

<hr/>

[^1]: Bayesian Linear Regressionì—ì„œë„ prior distributionì„ ì‚¬ìš©í–ˆëŠ”ë°, ê·¸ë•ŒëŠ” parameter $\theta$ì— ëŒ€í•œ \<parameter prior\> ì˜€ë‹¤! ê·¸ëŸ¬ë‚˜ Bayesian Regressionê³¼ ë‹¬ë¦¬ <span class="half_HL">GP Regressionì€ parameter $\theta$ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” non-parameteric ëª¨ë¸ì´ë‹¤!!</span> ì´ì— ëŒ€í•´ì„  ë’· ë¬¸ë‹¨ì—ì„œ ë‘˜ì„ ë¹„êµí•˜ë©° í•œë²ˆ ë” ì‚´í´ë³´ê² ë‹¤ ğŸ˜‰

[^2]: space of functionì´ continuous domainì´ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ë§í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ì•”íŠ¼ ê·¸ë ‡ë‹¤.
