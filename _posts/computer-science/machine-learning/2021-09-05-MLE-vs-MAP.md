---
title: "MLE vs. MAP"
layout: post
use_math: true
tags: ["Machine Learning"]
readtime: 20 Minutes
---



"Machine Learning"을 공부하면서 개인적인 용도로 정리한 포스트입니다. 지적은 언제나 환영입니다 :)

<div class="proof" markdown="1">

**기획 시리즈: Bayesian Regression**

1. [MLE vs. MAP]({{"/2021/09/05/MLE-vs-MAP.html" | relative_url}}) 👀
2. [Predictive Distribution]({{"/2021/09/05/predictive-distribution.html" | relative_url}})
3. [Bayesian Regression]({{"/2021/09/06/bayesian-regression.html" | relative_url}})

</div>

<hr/>

### MLE vs. MAP

- MLE = Maximum Likelihood Estimation
- MAP = Maximum A Posteriori

MLE, MAP 둘다 \<statistical inference\>의 방법론 중 하나이다. 둘다 최적의 $\theta$ 값을 찾는 것을 목표로 한다.

1\. **MLE**

MLE에 대한 introduction은 [이 포스트](https://bluehorn07.github.io/mathematics/2021/05/17/maximum-likelihood-estimation.html)를 참고하길 바란다. 
MLE는 간단하게 말해 아래의 값을 구하는 것이다.

$$
\theta_{\text{MLE}} = \underset{\theta}{\text{argmax}} \; p(X \mid \theta) = \underset{\theta}{\text{argmax}} \prod_{i} p(x_i \mid \theta)
$$

이때, $P(X\mid \theta)$를 "likelihood"라고 하는데, 여러분이 생각하는 Bayesian Rule의 likelihood가 맞다! MLE의 식에 production $\prod$ 텀이 있기 때문에 보통의 MLE 문제는 log-likelihood에서 최대값을 구하는 방식으로 전개한다.

$$
\theta_{\text{MLE}} = \underset{\theta}{\text{argmax}} \sum_{i} \log \left( p(x_i \mid \theta) \right)
$$

MLE는 위의 log-likelihood 식을 미분한 미분방정식을 풀어 $\theta_{\text{MLE}}$를 구한다!

<br/>

2\. **MAP**

MAP는 Bayesian Rule에서부터 출발한다.

$$
P(\theta \mid X) = \frac{P(\theta)P(X \mid \theta)}{P(X)} \propto P(\theta)P(X\mid\theta)
$$

MAP는 그 이름에서부터 알 수 있듯이 posterior를 사용해 $\theta$를 추정한다. 위의 식을 살펴보면 posterior는 prior와 likelihood의 곱으로 유도할 수 있다.
그래서 식을 적어보면 MAP는 MLE를 유도하는 식에서 likelihood를 posterior로 바꿔주기만 하면 된다!

$$
\begin{aligned}
\theta_{\text{MAP}} 
&= \underset{\theta}{\text{argmax}} P(X \mid \theta) P(\theta) \\
&= \underset{\theta}{\text{argmax}} \left( \log P(X \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \log  \prod_{i} P(x_i \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \log P(\theta) \right)
\end{aligned}
$$

<br/>

MLE와 MAP의 식을 비교하면 딱 하나가 다른데 바로 <span class="half_HL">MAP에는 prior $P(\theta)$가 존재한다</span>는 것이다! 이것은 optimization 과정에서 $\theta$에 대한 prior까지 함께 고려한다는 것이다. 그리고 $p(\theta)$의 값에 따라 최적화의 target equation의 값이 달라지는데, 이것은 prior $p(\theta)$가 target equstion에 가중치를 주는 것으로 이해할 수도 있다. 이전의 MLE가 $\theta$를 deterministic 한 값으로 여겼던 것과는 달리 MAP에서는 $\theta$r가 prior $p(\theta)$를 갖는 RV로 취급한다는 시각도 돋보인다.

MAP를 좀더 살펴보기 위해 prior $p(\theta)$를 가장 간단한 형태인 uniform prior라고 가정해보자. 이것은 모든 likelihood에 const로 동일한 weight를 주는 것과 같다. 그래서

$$
\begin{aligned}
\theta_{\text{MAP}} 
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \text{const} \right) \\
&= \underset{\theta}{\text{argmax}} \sum_{i} \log P(x_i \mid \theta) \\
&= \theta_{\text{MLE}}
\end{aligned}
$$

Boom! uniform prior 아래에서는 $\theta_{\text{MLE}} = \theta_{\text{MAP}}$라는 결과를 얻었다! 물론 prior를 Gaussian이나 다른 확률 분포로 가정한다면, 전혀 다른 결과를 얻을 것이다. 보통 prior에 대해 어떤 가정을 취한다면 MAP로 풀고, 그렇지 않다면 MLE로 문제를 해결한다.

사실 MLE와 MAP는 목표로 하는 바가 다른데, 이것을 아래와 같이 기술한다.

<div class="statement" markdown="1">

Formally **MLE** produces the choice that is <u><b>most likely to generated the observed data</b></u>.

A **MAP** estimated is the choice that is <u><b>most likely given the observed data</b></u>. In contrast to MLE, MAP estimation applies Bayes's Rule, so that our estimate can take into account prior knowledge about what we expect our parameters to be in the form of a prior probability distribution.

</div>

<br/>

MLE와 MAP에 대해 충분히 이해했다면, 아래의 아티클을 읽어보는 것을 추천한다. Linear Regression을 Frequntist와 Bayesian의 관점에서 잘 풀어냈다.

- [Linear Regression: A Bayesian Point of View](https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/)

<hr/>

이어지는 포스트에서는 \<predictive distribution; 예측 분포\>에 대해서 살펴본다.

👉 [Predictive Distribution]({{"/2021/09/05/predictive-distribution.html" | relative_url}})

<hr/>

### reference

- [MLE vs MAP](https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/)
- [Linear Regression: A Bayesian Point of View](https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/)
