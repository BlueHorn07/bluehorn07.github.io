---
title: "MLE vs. MAP"
toc: true
toc_sticky: true
categories: ["Machine Learning"]
readtime: 20 Minutes
---



"Machine Learning"ì„ ê³µë¶€í•˜ë©´ì„œ ê°œì¸ì ì¸ ìš©ë„ë¡œ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<div class="proof" markdown="1">

**ê¸°íš ì‹œë¦¬ì¦ˆ: Bayesian Regression**

1. [MLE vs. MAP]({{"/2021/09/05/MLE-vs-MAP.html" | relative_url}}) ğŸ‘€
2. [Predictive Distribution]({{"/2021/09/05/predictive-distribution.html" | relative_url}})
3. [Bayesian Regression]({{"/2021/09/06/bayesian-regression.html" | relative_url}})

</div>

<hr/>

### MLE vs. MAP

- MLE = Maximum Likelihood Estimation
- MAP = Maximum A Posteriori

MLE, MAP ë‘˜ë‹¤ \<statistical inference\>ì˜ ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ì´ë‹¤. ë‘˜ë‹¤ ìµœì ì˜ $\theta$ ê°’ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

1\. **MLE**

MLEì— ëŒ€í•œ introductionì€ [ì´ í¬ìŠ¤íŠ¸]({{"/2021/05/17/maximum-likelihood-estimation.html" | relative_url}})ë¥¼ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤.
MLEëŠ” ê°„ë‹¨í•˜ê²Œ ë§í•´ ì•„ë˜ì˜ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.

$$
\theta_{\text{MLE}} = \underset{\theta}{\text{argmax}} \; p(X \mid \theta) = \underset{\theta}{\text{argmax}} \prod_{i} p(x_i \mid \theta)
$$

ì´ë•Œ, $P(X\mid \theta)$ë¥¼ "likelihood"ë¼ê³  í•˜ëŠ”ë°, ì—¬ëŸ¬ë¶„ì´ ìƒê°í•˜ëŠ” Bayesian Ruleì˜ likelihoodê°€ ë§ë‹¤! MLEì˜ ì‹ì— production $\prod$ í…€ì´ ìˆê¸° ë•Œë¬¸ì— ë³´í†µì˜ MLE ë¬¸ì œëŠ” log-likelihoodì—ì„œ ìµœëŒ€ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì „ê°œí•œë‹¤.

$$
\theta_{\text{MLE}} = \underset{\theta}{\text{argmax}} \sum_{i} \log \left( p(x_i \mid \theta) \right)
$$

MLEëŠ” ìœ„ì˜ log-likelihood ì‹ì„ ë¯¸ë¶„í•œ ë¯¸ë¶„ë°©ì •ì‹ì„ í’€ì–´ $\theta_{\text{MLE}}$ë¥¼ êµ¬í•œë‹¤!

<br/>

2\. **MAP**

MAPëŠ” Bayesian Ruleì—ì„œë¶€í„° ì¶œë°œí•œë‹¤.

$$
P(\theta \mid X) = \frac{P(\theta)P(X \mid \theta)}{P(X)} \propto P(\theta)P(X\mid\theta)
$$

MAPëŠ” ê·¸ ì´ë¦„ì—ì„œë¶€í„° ì•Œ ìˆ˜ ìˆë“¯ì´ posteriorë¥¼ ì‚¬ìš©í•´ $\theta$ë¥¼ ì¶”ì •í•œë‹¤. ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ë©´ posteriorëŠ” priorì™€ likelihoodì˜ ê³±ìœ¼ë¡œ ìœ ë„í•  ìˆ˜ ìˆë‹¤.
ê·¸ë˜ì„œ ì‹ì„ ì ì–´ë³´ë©´ MAPëŠ” MLEë¥¼ ìœ ë„í•˜ëŠ” ì‹ì—ì„œ likelihoodë¥¼ posteriorë¡œ ë°”ê¿”ì£¼ê¸°ë§Œ í•˜ë©´ ëœë‹¤!

$$
\begin{aligned}
\theta_{\text{MAP}}
&= \underset{\theta}{\text{argmax}} P(X \mid \theta) P(\theta) \\
&= \underset{\theta}{\text{argmax}} \left( \log P(X \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \log  \prod_{i} P(x_i \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \log P(\theta) \right)
\end{aligned}
$$

<br/>

MLEì™€ MAPì˜ ì‹ì„ ë¹„êµí•˜ë©´ ë”± í•˜ë‚˜ê°€ ë‹¤ë¥¸ë° ë°”ë¡œ <span class="half_HL">MAPì—ëŠ” prior $P(\theta)$ê°€ ì¡´ì¬í•œë‹¤</span>ëŠ” ê²ƒì´ë‹¤! ì´ê²ƒì€ optimization ê³¼ì •ì—ì„œ $\theta$ì— ëŒ€í•œ priorê¹Œì§€ í•¨ê»˜ ê³ ë ¤í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  $p(\theta)$ì˜ ê°’ì— ë”°ë¼ ìµœì í™”ì˜ target equationì˜ ê°’ì´ ë‹¬ë¼ì§€ëŠ”ë°, ì´ê²ƒì€ prior $p(\theta)$ê°€ target equstionì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ë„ ìˆë‹¤. ì´ì „ì˜ MLEê°€ $\theta$ë¥¼ deterministic í•œ ê°’ìœ¼ë¡œ ì—¬ê²¼ë˜ ê²ƒê³¼ëŠ” ë‹¬ë¦¬ MAPì—ì„œëŠ” $\theta$rê°€ prior $p(\theta)$ë¥¼ ê°–ëŠ” RVë¡œ ì·¨ê¸‰í•œë‹¤ëŠ” ì‹œê°ë„ ë‹ë³´ì¸ë‹¤.

MAPë¥¼ ì¢€ë” ì‚´í´ë³´ê¸° ìœ„í•´ prior $p(\theta)$ë¥¼ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì¸ uniform priorë¼ê³  ê°€ì •í•´ë³´ì. ì´ê²ƒì€ ëª¨ë“  likelihoodì— constë¡œ ë™ì¼í•œ weightë¥¼ ì£¼ëŠ” ê²ƒê³¼ ê°™ë‹¤. ê·¸ë˜ì„œ

$$
\begin{aligned}
\theta_{\text{MAP}}
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \log P(\theta) \right) \\
&= \underset{\theta}{\text{argmax}} \left( \sum_{i} \log P(x_i \mid \theta) + \text{const} \right) \\
&= \underset{\theta}{\text{argmax}} \sum_{i} \log P(x_i \mid \theta) \\
&= \theta_{\text{MLE}}
\end{aligned}
$$

Boom! uniform prior ì•„ë˜ì—ì„œëŠ” $\theta_{\text{MLE}} = \theta_{\text{MAP}}$ë¼ëŠ” ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤! ë¬¼ë¡  priorë¥¼ Gaussianì´ë‚˜ ë‹¤ë¥¸ í™•ë¥  ë¶„í¬ë¡œ ê°€ì •í•œë‹¤ë©´, ì „í˜€ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì–»ì„ ê²ƒì´ë‹¤. ë³´í†µ priorì— ëŒ€í•´ ì–´ë–¤ ê°€ì •ì„ ì·¨í•œë‹¤ë©´ MAPë¡œ í’€ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ MLEë¡œ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

ì‚¬ì‹¤ MLEì™€ MAPëŠ” ëª©í‘œë¡œ í•˜ëŠ” ë°”ê°€ ë‹¤ë¥¸ë°, ì´ê²ƒì„ ì•„ë˜ì™€ ê°™ì´ ê¸°ìˆ í•œë‹¤.

<div class="notice" markdown="1">

Formally **MLE** produces the choice that is <u><b>most likely to generated the observed data</b></u>.

A **MAP** estimated is the choice that is <u><b>most likely given the observed data</b></u>. In contrast to MLE, MAP estimation applies Bayes's Rule, so that our estimate can take into account prior knowledge about what we expect our parameters to be in the form of a prior probability distribution.

</div>

<br/>

MLEì™€ MAPì— ëŒ€í•´ ì¶©ë¶„íˆ ì´í•´í–ˆë‹¤ë©´, ì•„ë˜ì˜ ì•„í‹°í´ì„ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. Linear Regressionì„ Frequntistì™€ Bayesianì˜ ê´€ì ì—ì„œ ì˜ í’€ì–´ëƒˆë‹¤.

- [Linear Regression: A Bayesian Point of View](https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/)

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” \<predictive distribution; ì˜ˆì¸¡ ë¶„í¬\>ì— ëŒ€í•´ì„œ ì‚´í´ë³¸ë‹¤.

ğŸ‘‰ [Predictive Distribution]({{"/2021/09/05/predictive-distribution.html" | relative_url}})

<hr/>

### reference

- [MLE vs MAP](https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/)
- [Linear Regression: A Bayesian Point of View](https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/)
