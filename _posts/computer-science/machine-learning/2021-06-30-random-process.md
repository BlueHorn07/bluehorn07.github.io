---
title: "Random Process"
toc: true
toc_sticky: true
categories: ["Machine Learning"]
modified_date: 2021.09.21
readtime: 20 minutes
---

"Machine Learning"ì„ ê³µë¶€í•˜ë©´ì„œ ê°œì¸ì ì¸ ìš©ë„ë¡œ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<hr/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” "í™•ë¥ ë¡ (Probability Theory)"ê³¼ "Machine Learning"ì—ì„œ ë“±ì¥í•˜ëŠ” *"Process"* ê°€ ë¶™ì€ ëª¨ë“  ê°œë…ì„ ë„“ì€ ì‹œì•¼ë¡œ ì‚´í´ë³´ê¸° ìœ„í•´ ì‘ì„±í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ë£¨ëŠ” ì£¼ì œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- [Random Process; Stochastic Process](#introduction-to-random-process)
- [Bernoulli Process](#bernoulli-process-2)
- [Poisson Process](#poisson-process)
- [Gaussian Process](#gaussian-process)
- [Markov Process](#markov-process)

<hr/>

## Introduction to Random Process

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Random Process<br>

A **random process** is a time-varying function, that assigns the outcome of a random experiment to each time instant.

</div>

ëŒ€ì¶©, (time instant)ì— random experimentì˜ ê²°ê³¼(outcome)ì„ ë§¤í•‘í•œë‹¤ëŠ” ëœ»ì´ë‹¤.

ë˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•˜ê¸°ë„ í•˜ëŠ”ë°,

<div class="definition" markdown="1">

A (infinite) sequence of random variables $X_1, X_2, \dots, X_n, \dots$

</div>

ì¦‰, RVì˜ infinite sequenceë¥¼ \<**random process**\>ë¼ê³  í•œë‹¤. ì²«ë²ˆì§¸ ì •ì˜ë³´ë‹¤ëŠ” ë‘ë²ˆì§¸ ì •ì˜ê°€ ì¢€ë” ì™€ë‹¿ëŠ” í¸ì´ë‹¤. ğŸ‘

\<random process\>ë¥¼ ì •ì˜í•  ë•Œ, RV $X_i$ê°€ ë“±ì¥í–ˆìœ¼ë‹ˆ ìì—°ìŠ¤ëŸ½ê²Œ ì•„ë˜ì˜ ì„±ì§ˆë“¤ì„ ê°€ì§ˆ ê²ƒì´ë‹¤.

- $E[X_i]$: mean of RV
- $\text{Var}(X_i)$: variance of RV
- $p_{X_i} (x_i)$: marginal probability distribution of RV

ìš°ë¦¬ëŠ” \<random process\> ìì²´ì˜ ë¶„í¬ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ë„ ìˆëŠ”ë°, ì´ê²ƒì€ ì•„ë˜ì™€ ê°™ì€ joint probability distributionì´ ëœë‹¤.

$$
p_{X_1, \dots, X_n, \dots} (x_1, \dots, x_n, \dots)
$$

ë˜, ìš°ë¦¬ëŠ” \<random process\>ì˜ Sample Space $\Omega$ì— ëŒ€í•´ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.

\<random process\> ì¤‘ í•˜ë‚˜ì¸ \<Bernoulli Process\>ì˜ ê²½ìš°, Sample SapceëŠ” 0-1ì˜ infinite sequenceê°€ ëœë‹¤.

<div class="theorem" markdown="1">

<span class="statement-title">Property.</span> Sample Space of Bernoulli Process<br>

$$
\Omega_{\text{BP}}
= \left\{
  (b_1, \dots, b_n, \dots ) \mid b_i \in \{ 0, 1 \}
  \right\}
$$

</div>

<hr/>

### Some Properties of Random Process

\<Random Process\>ëŠ” ì•„ë˜ì™€ ê°™ì€ ëª‡ê°€ì§€ íŠ¹ì§•ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. <span style="color: red">ì´ê²ƒì€ í•„ìˆ˜ì ì¸ ê²ƒì€ ì•„ë‹ˆë©°, ëª‡ëª‡ \<random process\>ì— ê³µí†µì ìœ¼ë¡œ ë³´ì´ëŠ” íŠ¹ì§•ì´ê±°ë‚˜ ê°€ì •ì´ë‹¤.</span>

<big>**1. Independence btw trials**</big>

ê°œë³„ trialsì€ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. ì¦‰, ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.

- Bernoulli Process, Poisson Process, ...

<big>**2. Memoeryless Property**</big>

$$
P(X = x + k \mid x > k) = P(X = x)
$$

- Bernoulli Process, Poisson Process, ...
- Markov Process

<hr/>

### Bernoulli Process (2)

ì´ë²ˆ ë¬¸ë‹¨ì—ì„œëŠ” \<[Bernoulli Process]({{"/2021/03/25/poisson-distribution#bernoulli-process" | relative_url}})\>ì— ëŒ€í•œ ë‚´ìš©ì—ì„œ ì¶”ê°€ì ì¸ ì£¼ì œë“¤ì„ ë‹¤ë£¬ë‹¤. ì•„ì§ \<Bernoulli Process\>ê°€ ë­”ì§€ ëª¨ë¥¸ë‹¤ë©´, ìœ„ì˜ í¬ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì½ì–´ë³´ì!

\<Bernoulii Process\>ì—ì„œ ì–´ë–¤ random variable $Y$ë¥¼ ì¡°ê±´ê³¼ í•¨ê»˜ ì •ì˜í•˜ë©´ ìƒˆë¡œìš´ í™•ë¥  ë¶„í¬ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤! ìš°ë¦¬ëŠ” \<Binomial distribution\>, \<Geometric distribution\>, \<Negative BIN distribution\>ì„ \<Bernoulli Process\>ë¡œë¶€í„° ìœ ë„í•´ë³´ê² ë‹¤ ğŸ˜

<div class="theorem" markdown="1">

<big>1. Number of Success $S_n$ in $n$ trials.</big>

Let's derive a random variable $S_n = X_1 + \cdots + X_n$ from the Bernoulli Process.

Then, $S_n$ follows the \<**Binomial Distribution**\>!

$$
P(S_n = x) = \binom{n}{x} p^x (1-p)^{n-x} \quad \text{for} \; x=0, 1, \dots, n
$$

</div>

<div class="theorem" markdown="1">

<big>2. Time until the first success</big>

Let's derive a randome variable $T_1 = \min \\{ i \in \mathbb{N} : X_i = 1\\}$ from the Bernoulli Process.

Then, $T_1$ follows the \<**Geometric Distribution**\>!

$$
P(T_1 = x) = P(\underbrace{0, 0, \dots, 0}_{x-1}, 1) = (1-p)^{x-1} p \quad \text{for} \; x=1, 2, \dots
$$

</div>

<div class="theorem" markdown="1">

<big>3. Time until the first $k$ success</big>

\<Geometric Random Variable\>ì¸ $T_1$ì„ í™•ì¥í•œ ê°œë…ì´ë‹¤.

Let's derive a randome variable $T_k = \min \\{ i \in \mathbb{N} : \| \\{ X_i : X_i = 1 \\} \| = k\\}$ from the Bernoulli Process.

Then, $T_n$ follows the \<**Negative Binomial Distribution**\>!

$$
P(T_k = x) = P(\underbrace{0, 1, \dots, 1, \dots, 0}_{k-1 \text{ success}}, 1) = \binom{x-1}{k-1} (1-p)^{x-k} p^k \quad \text{for} \; x=k, k+1, \dots
$$

</div>

<hr/>

### Poisson Process

\<Poisson Process\>ëŠ” \<Bernoulli Process\>ì—ì„œ ê·¹í•œì„ ì·¨í•´ time intervalì˜ ê°„ê²©ì„ ì•„ì£¼ì•„ì£¼ ì¤„ì—¬ì„œ continuous domain ìœ„ì—ì„œ ì •ì˜í•œ Random Processì´ë‹¤. <span class="half_HL">BPê°€ $\mathbb{N}$ ìœ„ì—ì„œ ì •ì˜ë˜ì—ˆë‹¤ë©´, PPëŠ” $\mathbb{R^{+}}$ ìœ„ì—ì„œ ì •ì˜ë˜ëŠ” Random Process</span>ì¸ ì…ˆ!

PPì— ëŒ€í•œ ë‚´ìš©ì€ ì•„ë˜ í¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤ ğŸ™

ğŸ‘‰ [Poisson Process]({{"/2021/03/25/poisson-distribution#bernoulli-process" | relative_url}})

<hr/>

### Gaussian Process

A sequence of Gaussian distributionìœ¼ë¡œ, multi-variate Gaussian distributionì˜ ì¼ë°˜í™”ëœ ë²„ì „ì´ë‹¤. "distribution over functions"ìœ¼ë¡œ ì·¨ê¸‰í•œë‹¤! ğŸ’ª

ğŸ‘‰ [Distribution over functions & Gaussian Process]({{"2021/07/01/Gaussian-process" | relative_url}})

<hr/>

### Markov Process

ğŸ‘‰ [Markov Process]({{"/2021/07/03/markov-process" | relative_url}})

<hr/>

### references

- [[MIT OCW] Introduction to Probability: Lecture 21](https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/MITRES_6_012S18_L21AS.pdf)



