---
title: "Bayesian Regression"
toc: true
toc_sticky: true
categories: ["Machine Learning"]
modified_date: 2021.09.20
readtime: 30 Minutes
---



"Machine Learning"ì„ ê³µë¶€í•˜ë©´ì„œ ê°œì¸ì ì¸ ìš©ë„ë¡œ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<div class="notice" markdown="1">

**ê¸°íš ì‹œë¦¬ì¦ˆ: Bayesian Regression**

1. [MLE vs. MAP]({{"/2021/09/05/MLE-vs-MAP" | relative_url}})
2. [Predictive Distribution]({{"/2021/09/05/predictive-distribution" | relative_url}})
3. [Bayesian Regression]({{"/2021/09/06/bayesian-regression" | relative_url}}) ğŸ‘€

</div>

<hr/>

## Bayesian Linear Regression

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì•ì—ì„œ ë‹¤ë£¬ \<parameter posterior\>, \<posterior predictive distribution\>ì„ Regression Problemì— ì ìš©í•œë‹¤. ì‚¬ì‹¤ \<**Bayesian Linear Regression**\>ì€ ë‹¨ìˆœíˆ \<posterior predictive distribution under the regression problem\>ì¼ ë¿ì´ë‹¤! ğŸ™Œ

ê´€ì¸¡ëœ ë°ì´í„° $S = (X, y)$ê°€ ì¡´ì¬í•´ ì´ê²ƒìœ¼ë¡œ \<parameter prior\>ë¥¼ ê°±ì‹ í•´ë³´ì. Bayes Ruleì— ë”°ë¥´ë©´ ì•„ë˜ì™€ ê°™ì´ \<parameter posterior\>ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
p(\theta \mid S)
&= \frac{p(S \mid \theta) p(\theta)}{p(S)} = \frac{p(S \mid \theta) p(\theta)}{\int_{\theta'} p(S \mid \theta') p(\theta') d\theta'} \\
&= \frac{p(\theta) \prod^m_{i=1} p(y^{(i)} \mid x^{(i)}, \theta)}{\int_{\theta'} p(\theta') \prod^m_{i=1} p(y^{(i)} \mid x^{(i)}, \theta') d\theta'}
\end{aligned}
$$

ì´ë•Œ, likelihoodì˜ $p(y^{(i)} \mid x^{(i)}, \theta)$ í…€ì€ ì•„ë˜ì™€ ê°™ì´ ê¸°ìˆ í•  ìˆ˜ ìˆë‹¤.

$$
p(y^{(i)} \mid x^{(i)}, \theta) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right)
$$

ì´ê²ƒì€ ì•„ë˜ì™€ ê°™ì€ regressionì˜ ê°€ì •ì„ í†µí•´ ìœ ë„ëœ ê²ƒìœ¼ë¡œ <span class="half_HL">$y$ë¥¼ í•˜ë‚˜ì˜ í™•ë¥  ë³€ìˆ˜ë¡œ ì·¨ê¸‰í•œë‹¤</span>ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ë˜í•œ, ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œëŠ” likelihood functionì´ ì´í•­ ë¶„í¬, ì •ê·œ ë¶„í¬ ë“±ë“±ì˜ ë¶„í¬ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆì—ˆëŠ”ë°, <span class="half_HL">regression problemì˜ ìƒí™©ì—ì„œëŠ” likelihoodë¥¼ Gaussian distributionë¡œ ì„¤ì •í•  ìˆ˜ ë°–ì— ì—†ë‹¤!</span> ğŸ™Œ

<div class="notice" markdown="1">

<div class="img-wrapper">
  <img src="{{ "/images/computer-science/machine-learning/bayesian_likelihood_from_train_set.png" | relative_url }}" width="100%">
  <p markdown="1">ref. [Gaussian processes: Sec. 2 - Chuong B. Do](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)</p>
</div>

</div>

<br/>

ì´ë²ˆì—ëŠ” Regression problemì— ëŒ€í•œ \<Predictive Distribution\>ì„ ì‚´í´ë³´ì. observed data $S = (X, y)$(train-set)ì™€ unobserved data $S^{\*} = (X^{\*}, y^{\*})$(test-set)ê°€ ìˆì„ ë•Œ, unobserved data $x^{\*} \in X^{\*}$ì— ëŒ€í•œ predictionì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì—ì„œ ìœ ë„í•˜ëŠ” ë¶„í¬ì´ë‹¤.
<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Prior Predictive Distribution (Regression)<br>

Let $S = \\{ (X, y) \\}$ be a set of observed data, $X^{\*}$ be a set of unobsersed data, and $x^{\*} \in X^{\*}$.

Then, the \<**prior predictive distribution**\> is

$$
p(y^{*} \mid x^{*}) = \int p(y^{*}, \theta \mid x^{*}) d\theta = \int p(y^{*} \mid x^{*}, \theta) p(\theta) d\theta
$$

</div>

ê·¸ëŸ¬ë‚˜ \<prior predictive distribution\>ì€ observed data $S$ë¥¼ ì „í˜€ ì“°ê³  ìˆì§€ ì•Šë‹¤. observed dataë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ë ¤ë©´ parameter posterior $p(\theta \mid S)$ë¡œ ìœ ë„í•œ \<**posterior predictive distribution**\>ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤!

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Posterior Predictive Distribution (Regression)<br>

$$
p(y^{*} \mid x^{*}, S) = \int p(y^{*}, \theta \mid x^{*}, S) d\theta = \int p(y^{*} \mid x^{*}, S, \theta) p(\theta \mid S) d\theta
$$

ë³´í†µ $x^{\*}$ì™€ $S$ë¥¼ ë…ë¦½ì´ë¼ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì— ë˜ëŠ” iidë¥¼ ê°€ì •í•˜ë¯€ë¡œ,

$$
p(y^{*} \mid x^{*}, S) = \int p(y^{*} \mid x^{*}, S, \theta) p(\theta \mid S) d\theta = \int p(y^{*} \mid x^{*}, \theta) p(\theta \mid S) d\theta
$$

</div>

<br/>

ì¼ë°˜ì ìœ¼ë¡œ regression problemì—ì„œ ì •ì˜í•œ parameter poster $p(\theta \mid S)$ì™€ posterior predictive distribution $p(y^{\*} \mid x^{\*}, S)$ëŠ” ì ë¶„ ê³„ì‚°ì´ ë§¤ìš° ì–´ë µë‹¤. ê·¸ë˜ì„œ ê·¼ì‚¬ë¥¼ ì´ìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë°, ì‹œë¦¬ì¦ˆì˜ ë§¨ ì²˜ìŒì— ë‹¤ë¤˜ë˜ [MAP(Maximum a Posterior)]({{"/2021/09/05/MLE-vs-MAP" | relative_url}})ë„ ì´ëŸ° ê·¼ì‚¬ ë°©ì‹ ì¤‘ í•˜ë‚˜ì´ë‹¤.

ë‹¤í–‰ì¸ ì ì€ \<Bayesian Linear Regression\>ì€ $p(\theta \mid S)$ì™€ $p(y^{\*} \mid x^{\*}, S)$ì— ëŒ€í•œ ë¶„í¬í•´ê°€ ì•Œë ¤ì ¸ ìˆìœ¼ë©° ì•„ë˜ì™€ ê°™ë‹¤.

$$
\begin{aligned}
\theta \mid S &\sim \mathcal{N} \left( \frac{1}{\sigma^2} A^{-1}X^T\vec{y}, \; A^{-1}\right) \\
y^{*} \mid x^{*}, S &\sim \mathcal{N} \left( \frac{1}{\sigma^2} {x^{*}}^T A^{-1} X^{T} \vec{y}, \; {x^{*}}^T A^{-1} x^{*} + \sigma^2 \right)
\end{aligned}
$$

where $A = \frac{1}{\sigma^2}X^TX + \frac{1}{\tau^2}I$.

ìœ„ì˜ ì‹ì´ ì–´ë–»ê²Œ ìœ ë„ ë˜ëŠ”ì§€ëŠ” ì•„ì§ ë³¸ì¸ë„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•´ì„œ ì¶”í›„ì— ë³„ë„ì˜ í¬ìŠ¤íŠ¸ë¡œ ìœ ë„ ê³¼ì •ì„ ê¸°ìˆ í•˜ë„ë¡ í•˜ê² ë‹¤ ğŸ™Œ

ê·¸ë˜ë„ ìœ„ì˜ ì‹ì„ í†µí•´ <span class="half_HL">parameter posteriorì™€ posterior predictive distributionì´ Gaussian distributionì„ ë”°ë¥¸ë‹¤</span>ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ <span class="half_HL">prediction $y^{\*}$, $y^{\*} = \theta^T x^{\*} + \epsilon^{\*}$ì— ëŒ€í•œ **uncertainty**ì™€ parameter $\theta$ì˜ ì„ íƒì— ëŒ€í•œ **uncertainty**ë„ ë‘ ì‹ì˜ variance ê°’ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤!</span> ğŸ‘

<hr/>

### ë§ºìŒë§

ì´ë²ˆ í¬ìŠ¤íŠ¸ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ Bayesian Approach ì‹œë¦¬ì¦ˆê°€ ëì´ ë‚¬ë‹¤. ìš©ì–´ì— 'Bayesian'ì´ë¼ëŠ” ë§ì´ ë“¤ì–´ê°€ë©´ ì–´ë µê²Œë§Œ ëŠê»´ì¡ŒëŠ”ë°, ì´ë²ˆ ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ ì¡°ê¸ˆì€ Bayesian Theoryë¥¼ ê·¹ë³µí•œ ê²ƒ ê°™ë‹¤ ğŸ™Œ

\<Bayesian Regression\>ì´ bayesian parameteric regressionì´ë¼ë©´, bayesian regressionì´ì§€ë§Œ non-parameteric modelì¸ [\<Gaussian Process Regression\>]({{"/2021/09/21/Gaussian-Process-Regression" | relative_url}})ë„ ìˆë‹¤. ê¶ê¸ˆí•˜ë‹¤ë©´, í•´ë‹¹ í¬ìŠ¤íŠ¸ë¥¼ ë°©ë¬¸í•´ë³´ì ğŸ‘

<hr/>

#### reference

- [Gaussian processes - Chuong B. Do](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)

