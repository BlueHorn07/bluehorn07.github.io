---
title: "Distribution over functions & Gaussian Process"
toc: true
toc_sticky: true
categories: ["Machine Learning", "Time Series Analysis"]
modified_date: 2021.09.21
readtime: 30 Minutes
---



"Machine Learning"ì„ ê³µë¶€í•˜ë©´ì„œ ê°œì¸ì ì¸ ìš©ë„ë¡œ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

ë³¸ ê¸€ì„ ì½ê¸° ì „ì— "[Random Process]({{"/2021/06/30/random-process" | relative_url}})"ì— ëŒ€í•œ ê¸€ì„ ë¨¼ì € ì½ê³  ì˜¬ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤ ğŸ˜‰

<div class="proof" markdown="1">

**ê¸°íš ì‹œë¦¬ì¦ˆ: Gaussian Process Regression**

1. [ê¸°íš ì‹œë¦¬ì¦ˆ: Bayesian Regression]({{"/2021/09/06/bayesian-regression" | relative_url}})
2. [Distribution over functions & Gaussian Process]({{"/2021/07/01/Gaussian-process" | relative_url}}) ğŸ‘€
3. [Gaussian Process Regression]({{"/2021/09/21/Gaussian-Process-Regression" | relative_url}})

</div>

<br><span class="statement-title">TOC.</span><br>

- [Introduction to Gaussian Process](#introduction-to-gaussian-process)
- [Probability distribution over functions with finite domains](#probability-distribution-over-functions-with-finite-domains)
- [Probability distribution over functions with infinite domains](#probability-distribution-over-functions-with-infinite-domains)
- [mean \& convariance function for GP](#mean--convariance-function-for-gp)
- [zero-mean GP](#zero-mean-gp)
- [references](#references)

<hr/>

### Introduction to Gaussian Process

ë¨¼ì € \<Gaussian distribution\>ì„ ë³µìŠµí•´ë³´ì.

**1\. 1D Gaussian Distribution**

$$
f(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left( - \frac{(x-\mu)^2}{2\sigma^2}\right)
$$

**2\. 2D Gaussian Distribution**

$$
f(\mathbf{x}) = \frac{1}{2\pi \left| \Sigma \right|^{1/2}} \exp \left( - \frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)
$$

**3\. Multi-variate Gaussian Distribution**

Distribution over random vectors!

$$
f(\mathbf{x}) = \frac{1}{(2\pi)^{n/2} \left| \Sigma \right|^{1/2}} \exp \left( - \frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)
$$

ì´ì œ \<Gaussian Process\>ì˜ ì •ì˜ë¥¼ ì‚´í´ë³´ì.

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Gaussian Process<br>

A sequence of Gaussian distributions! \<**Gaussian Process**\> is a generlization of multi-variate Gaussian distribution. It is a distribution over random functions!

$$
\mathcal{GP} (m(x), k(x, x'))
$$

- distribution over random functions
  - mean function $m(x)$[^1]
  - covariance function $k(x, x')$
- Every finite subset of RVs in GP is a multi-variate Gaussian distribution![^2]

</div>

ìœ„ì˜ ì •ì˜ë§Œ ë´ì„œëŠ” \<Gaussian Process\>ê°€ ë­”ì§€ ì˜ ì´í•´ê°€ ì•ˆ ëœë‹¤ ğŸ˜¥ ë¨¼ì € "distribution over random functions"ë¼ëŠ” í‘œí˜„ë¶€í„° ì´í•´í•´ë³´ì. \<random function\>ì´ë¼ëŠ” ë‚¯ì„  ê°œë…ì´ ë“±ì¥í–ˆëŠ”ë° \<random variable\>ê³¼ëŠ” ë‹¤ë¥¸ ê²ƒì¼ê¹Œ?

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> random function<br>

Let $\mathcal{H}$ be a class of functions mapping $\mathcal{X} \rightarrow \mathcal{Y}$. A random function $h(\cdot)$ from $\mathcal{H}$ is a function which is randomly drawn from $\mathcal{H}$, according to some probability distribution over $\mathcal{H}$.

Once a random function $h(\cdot)$ is selected from $\mathcal{H}$ probabilistically, it implies a deterministic mapping from inputs in $\mathcal{X}$ to outputs in $\mathcal{Y}$.

</div>

ìœ„ì—ì„œ ì •ì˜í•œ \<random function\>ì€ ë‹¨ìˆœíˆ random numberë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ê°€ ì•„ë‹ˆë‹¤! ğŸ‘Š

<hr/>

### Probability distribution over functions with finite domains

ë¨¼ì € í™•ë¥  ë¶„í¬ê°€ ì–´ë–»ê²Œ í•¨ìˆ˜ ìœ„ì—ì„œ ì •ì˜ë˜ëŠ”ì§€ ì•Œê¸° ìœ„í•´ $\mathcal{X}$ê°€ finite setì¸ ê°„ë‹¨í•œ ìƒí™©ë¶€í„° ì‚´í´ë³´ì.

<div class="math-statement" markdown="1">

Let $\mathcal{X} = \\{x_1, \dots, x_m\\}$ be any finite set of elements. Now consider the set $\mathcal{H}$ of all possible functions mapping from $\mathcal{X}$ to $\mathbb{R}$.

Since the domain of any $h(\cdot) \in \mathcal{H}$ has only finite $m$ elts, we can represent $h(\cdot)$ as an $m$-dimensional vector, $\vec{h} = [h(x_1), \dots, h(x_m)]^T$.

In order to specify a probability distribution over functions $h(\cdot)$, we must associate some 'probability density' with each function in $\mathcal{H}$. Note that we've represent function $h(\cdot)$ as a vector $\vec{h}$. Then we can give a prob. distribution like gaussian as follows

$$
\vec{h} \sim \mathcal{N} \left( \vec{\mu}, \; \sigma^2 I \right)
$$

Boom! this implies a prob. distribution over functions $h(\cdot)$, whose probability density function is given by

$$
p(h) = \prod^m_{i=1} \frac{1}{\sqrt{2\pi} \sigma} \exp \left( - \frac{1}{2\sigma^2} (h(x_i) - \mu_i)^2 \right)
$$

</div>

ìœ„ì˜ finite domainì˜ ì˜ˆì‹œë¥¼ í†µí•´ ìš°ë¦¬ëŠ” prob. distribution over functions with finite domainsê°€ finite-dimensional multi-variate Gaussianìœ¼ë¡œ í‘œí˜„ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤! ğŸ˜² ì—¬ê¸°ì„œ function domain $\mathcal{X}$ë¥¼ infinite dimensionìœ¼ë¡œ í™•ì¥í•˜ë©´, ìš°ë¦¬ëŠ” \<**Gaussian Process**\>ë¥¼ ì–»ê²Œ ëœë‹¤! ğŸ’ª

<hr/>

### Probability distribution over functions with infinite domains

ì´ë²ˆì—ëŠ” $\mathcal{X}$ì—ì„œ ì¶”ì¶œí•œ collectionì„ ì´ìš©í•´ random variableì˜ ì§‘í•© $\\{ h(x) : x \in \mathcal{X}\\}_m$ë¥¼ ì •ì˜í•´ë³´ì. $h(\cdot)$ê°€ probabilistic í•˜ê²Œ ê²°ì •ë˜ëŠ” random functionì´ê¸° ë•Œë¬¸ì— $h(x)$ë„ random variable ì´ë‹¤. ğŸ˜‰ ì´ë•Œ domain set $\mathcal{X}$ì— ëŒ€í•´ ë³„ë„ë¡œ íŠ¹ì •í•˜ì§€ëŠ” ì•Šì•˜ë‹¤. ì´ì „ê³¼ ê°™ì€ finite domainì„ ìƒê°í•´ë„ ì¢‹ê³ , $\mathbb{R}$ì™€ ê°™ì€ infinite dimensionì„ ìƒê°í•´ë„ ì¢‹ë‹¤.

ìš°ë¦¬ëŠ” finite collection of random variable $\\{ h(x) : x \in \mathcal{X}\\}_m$ë¡œ multi-variate Gaussian distributionì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, $\mathcal{X}$ë¥¼ domainìœ¼ë¡œ ê°–ëŠ” $m(x)$ì™€ $k(x, x')$ëŠ” mean function, covariance functionì„ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

$$
\begin{aligned}
m(x) &= E \left[h(x)\right] \\
k(x, x') &= E \left[ (h(x) - m(x)) (h(x') - m(x'))\right]
\end{aligned}
$$

ë”°ë¼ì„œ collection of random variable $\\{ h(x) : x \in \mathcal{X}\\}_m$ ìœ„ì—ì„œì˜ multi-variate Gaussian distributionì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\vec{h}_m =
\begin{bmatrix}
h(x_1) \\ \vdots \\ h(x_m)
\end{bmatrix}
\sim
\mathcal{N}
\left(
\begin{bmatrix}
m(x_1) \\ \vdots \\ m(x_m)
\end{bmatrix}
,\;
\begin{bmatrix}
k(x_1, x_1) & \dots & k(x_1, x_m) \\
\vdots & \ddots & \vdots \\
k(x_m, x_1) & \dots & k(x_m, x_m)
\end{bmatrix}
\right)
$$

\<Gaussian Process\>ì˜ ì •ì˜ë¥¼ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ì.

<div class="notice" markdown="1">

"A **Gaussian process** is a stochastic process s.t. any finite subcollection of random variables has a multivariate Gaussian distribution."

</div>

Boom! ìš°ë¦¬ê°€ ìœ„ì—ì„œ ì •ì˜í•œ domain $\mathcal{X}$ì—ì„œ ì¶”ì¶œí•œ collection of random variable $\\{ h(x) : x \in \mathcal{X}\\}_m$ë¡œ ì ì ˆí•œ multi-variate Gaussian distributionì„ ë§Œë“œëŠ” ê³¼ì •ì€ ì‚¬ì‹¤ \<Gaussian Process\>ì˜ ì •ì˜ë¥¼ ì¬í˜„í•˜ëŠ” ê²ƒì´ì—ˆë‹¤! finite collectionì—ì„œ ìœ ë„í•œ ìœ„ì˜ í‘œí˜„ì„ ì¼ë°˜í™”í•˜ë©´ \<Gaussian Process\>ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì ì„ ìˆ˜ ìˆë‹¤.

$$
h(\cdot) \sim \mathcal{GP} (m(\cdot), \; k(\cdot, \cdot))
$$

finite domainì—ì„œ $h(x)$ë¥¼ finite random vectorë¡œ ì´í•´í•œ ê²ƒì²˜ëŸ¼, infinite domainì—ì„œì˜ $h(x)$ëŠ” infinite random vectorë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤! ğŸ™Œ

<hr/>

### mean & convariance function for GP

ì´ì œ GPê°€ distribution over random functionì´ë¼ëŠ” ì , ê·¸ë¦¬ê³  distribution over infinite random vectorë¼ëŠ” ê²ƒì„ ì´í•´í–ˆë‹¤. ìš°ë¦¬ì˜ ë‹¤ìŒ ê´€ì‹¬ì‚¬ëŠ” GPë¥¼ an function $m(x)$ê³¼ covariance function $k(x, x')$ì´ë‹¤ ğŸ™Œ ì‚¬ì‹¤ ì•ì˜ ë¬¸ë‹¨ì—ì„œ $m(x)$ì™€ $k(x, x')$ì˜ ì •ì˜ë¥¼ ì ê¸´ í–ˆë‹¤ë§Œ, $h(\cdot)$ê°€ random functionì´ê¸° ë•Œë¬¸ì— ìœ„ì˜ ì •ì˜ë¥¼ ê°€ì§€ê³ ëŠ” $m(x)$, $k(x, x')$ê°€ ì •í™•íˆ ì–´ë–¤ í•¨ìˆ˜ì¸ì§€ ê°ì„ ì¡ì„ ìˆœ ì—†ì—ˆì„ ê²ƒì´ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ <span class="half_HL">mean function $m(x)$ëŠ” ì–´ë–¤ real-valued functionë„ ê°€ëŠ¥í•˜ë‹¤</span>. ê·¸ëŸ¬ë‚˜ covariance function $k(x, x')$ëŠ” GPë¥¼ marginalization í–ˆì„ ë•Œ ìœ ë„ë˜ëŠ” Covariance Matrixê°€ semi-positive definite ê°™ì€ covarianceì˜ ì„±ì§ˆë“¤ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.

<div class="math-statement" markdown="1">

For covariance function $k(x, x')$ and for any set of elts $x_1, \dots, x_m \in \mathcal{X}$, the resulting covariance matrix must be statisfy the properties of covariance matrix.

$$
K = \begin{bmatrix}
k(x_1, x_1) & \dots & k(x_1, x_m) \\
\vdots & \ddots & \vdots \\
k(x_m, x_1) & \dots & k(x_m, x_m)
\end{bmatrix}
$$

For example, all $k(x, x') \ge 0$ and $K$ is a non-negative definite matrix.

</div>

ìœ„ì˜ ì¡°ê±´ì„ ë³´ë©´ ìœ íš¨í•œ $k(x, x')$ë¥¼ ì°¾ëŠ” ê²ƒì€ ê¹Œë§ˆë“í•´ ë³´ì¸ë‹¤ ğŸ˜¥ ê·¸.ëŸ¬.ë‚˜. Chuong B. Doì˜ [ì•„í‹°í´](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)ì— ë”°ë¥´ë©´ valid convariance functionì— ëŒ€í•œ ì¡°ê±´ì´ ê³§ [\<Mercer's theorem; ë¨¸ì„œì˜ ì •ë¦¬\>](https://en.wikipedia.org/wiki/Mercer%27s_theorem)ì—ì„œ ìš”êµ¬í•˜ëŠ” kernelì˜ ì¡°ê±´ê³¼ ë™ì¼í•˜ë‹¤ê³  ë§í•œë‹¤! ğŸ˜² ê·¸ë˜ì„œ \<Mercer's theorem\>ì´ ë³´ì¥í•˜ëŠ” valid kernel function $k(x, x')$ë¥¼ ì‚¬ìš©í•˜ë©´ convarianceì˜ ì„±ì§ˆì„ ê³ ë¯¼í•˜ì§€ ì•Šê³ ë„ convariance function $k(x, x')$ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤!! ğŸ¤© ì•ìœ¼ë¡œëŠ” convariance function ëŒ€ì‹  "kernel function"ì´ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

<hr/>

### zero-mean GP

ì´ì œ GPì— ì¹œí•´ì§€ê¸° ìœ„í•´ mean function $m(x) = 0$ì¸ **zero-mean Gaussian process**ë¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.

$$
h(\cdot) \sim \mathcal{GP}(0, \; k(\cdot, \cdot))
$$

ì´ë•Œ, function $h$ëŠ” $h: \mathbb{R} \rightarrow \mathbb{R}$ì˜ í•¨ìˆ˜ì´ë‹¤. ê·¸ë¦¬ê³  kernel function $k(\cdot, \cdot)$ì€ \<**squared exponential kernel function**\>[^3]ì„ ì‚¬ìš©í•œë‹¤.

$$
k_{SE}(x, x') = \exp \left( - \frac{1}{2\tau^2} (x - x')^2 \right) \quad (\tau > 0)
$$

ì´ë•Œ, ìœ„ì™€ ê°™ì€ GPì—ì„œ sampleí•œ function $h(x)$ëŠ” ì–´ë–»ê²Œ ìƒê²¼ì„ê¹Œ? ë¨¼ì € í•¨ìˆ˜ê°’ì˜ í‰ê· ì´ 0ì´ê¸° ë•Œë¬¸ì— í•¨ìˆ˜ê°’ì´ 0 ì£¼ë³€ì— ë¶„í¬í•  ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ë˜, $x, x' \in \mathcal{X}$ì¸ ë‘ ì›ì†Œì— ëŒ€í•œ í•¨ìˆ˜ê°’ì€

- $x$ì™€ $x'$ê°€ ê°€ê¹ë‹¤(nearby)ë©´, $k_{SE}(x, x') \approx 1$ì´ ë˜ë¯€ë¡œ $h(x)$ì™€ $h(x')$ëŠ” high covarianceë¥¼ ê°€ì§„ë‹¤.
- ë°˜ëŒ€ë¡œ $x$ì™€ $x'$ê°€ ë©€ë‹¤(far apart)ë©´, $k_{SE}(x, x') \approx 0$ì´ ë˜ë¯€ë¡œ $h(x)$ì™€ $h(x')$ëŠ” low covarianceë¥¼ ê°€ì§„ë‹¤.

ì´ëŸ° ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œë¡œ ìƒ˜í”Œë§ í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•œë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/computer-science/machine-learning/gaussian-process-1.png" | relative_url }}" width="100%">
</div>

<hr/>

ì! ì—¬ê¸°ê¹Œì§€ \<Gaussian Process\>ì— ëŒ€í•´ ì‚´í´ë³´ì•˜ë‹¤. distribution over random vectorì˜ ê°œë…ì„ í™•ì¥í•œ distribution over random function ê·¸ë¦¬ê³  ê·¸ê²ƒì„ infinite dimensionê¹Œì§€ í™•ì¥í•œ **Gaussian Process**ê¹Œì§€!! ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì´ ê²°ì½” ì‰½ì§€ëŠ” ì•Šì§€ë§Œ, ê³µë¶€í•  ê°€ì¹˜ëŠ” ì¶©ë¶„í•œ ì£¼ì œì˜€ë‹¤ ğŸ’ª

ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„  GPë¥¼ ì´ìš©í•´ Regression modelì„ ë§Œë“œëŠ” \<Gaussian Process Regresssion\>ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤!!

ğŸ‘‰ [Gaussian Process Regression]({{"/2021/09/21/Gaussian-Process-Regression" | relative_url}})

<hr/>

### references

- [Gaussian processes - Chuong B. Do](http://cs229.stanford.edu/section/cs229-gaussian_processes.pdf)

<hr/>

[^1]: ì´ì „ì˜ \<Bernoulli Process\>ì˜ ê²½ìš°, ê° trialì—ì„œ ëª¨ë‘ ë™ì¼í•œ \<Bernoulli distribution\>ì„ ê°€ì •í–ˆëŠ”ë°, \<Gaussian Process\>ì˜ ê²½ìš° $x$ ê°’ì— ë”°ë¼ ë‹¤ë¥¸ í‰ê· /ë¶„ì‚°ì„ ê°€ì§„ Gaussian distributionìœ¼ë¡œ ì´ë¤„ì§ˆ ìˆ˜ ìˆìŒì— ì£¼ëª©í•˜ì!

[^2]: ë³´í†µ \<Gaussian Process\>ì˜ ì •í™•í•œ ì •ì˜ëŠ” ì´ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„í•œë‹¤. "A Gaussian process is a stochastic process s.t. any finite subcollection of random variables has a multivariate Gaussian distribution."

[^3]: ì‚¬ì‹¤ SE kernelì€ gaussian kernelì˜ í•œ ì¢…ë¥˜ì´ë‹¤. ë‹¤ë§Œ, ì—¬ê¸°ì„œëŠ” Gaussian Processì™€ ì´ë¦„ì´ ê²¹ì³ì„œ squared-exponentialë¼ëŠ” ì´ë¦„ì„ ì“°ê²Œ ë˜ì—ˆë‹¤.