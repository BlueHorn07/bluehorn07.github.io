---
title: "Naive Bayes Classifier"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---

2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'ë°ì´í„° ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<div class="statement" markdown="1" align="center">

<span class="statement-title" style="font-size: large">Bayes' Theorem</span><br>

$$
\frac{P(\text{data} | \text{parameter}) \cdot P(\text{parameter})}{P(\text{data})} = P(\text{parameter} \mid \text{data})
$$

$$
\frac{\text{Likelihood} \cdot \text{Prior probability}}{\text{Evidence}} = \text{Posterior probability}
$$

</div>

<div class="statement" markdown="1" align="center">

"One assumption taken is the <span style="color:red;">strong independence</span> assumptions btw the features."

</div>

### Naive Assumption

"In a supervised learning situation, Naive Bayes classifiers are trained very efficiently. Naive Bayes classifiers need a small training data to estimate the parameters needed for classification. Naive Bayes classifiers have simple design and implementation, and they can applied to many real life situations."

\<**Naive Bayes Classifer**\>ëŠ” Supervised Learning Modelì´ë‹¤. Conditional Probabilityë¥¼ í™œìš©í•´ ë¶„ë¥˜ë¥¼ ì§„í–‰í•˜ë©° Maximum Likelihoodë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤!

<hr/>

### Naive Bayes Classifier

ìš°ë¦¬ì˜ ëª©í‘œëŠ” ë°ì´í„°ì˜ featureê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê° labelì— ëŒ€í•œ í™•ë¥ ì¸ Posterior Probability $p(c_k \mid \mathbf{x})$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.

ì´ë•Œ, ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•´ Posterior Probabilityë¥¼ ì•„ë˜ì™€ ê°™ì´ Conditional Probabilityë¡œ ë¶„í•´í•  ìˆ˜ ìˆë‹¤.

$$
p(c_k \mid \mathbf{x}) = \frac{p(c_k) \cdot p(\mathbf{x} \mid c_k)}{p(\mathbf{x})}
$$

ì´ë•Œ, Evidenceì— í•´ë‹¹í•˜ëŠ” $p(\mathbf{x})$ëŠ” output $y$ì™€ ë…ë¦½ì´ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” Evidenceë¥¼ ë¬´ì‹œí•˜ê³  ì•„ë˜ì™€ ê°™ì´ $y$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
y
&= \underset{c_k}{\text{argmax}} \; p(c_k \mid \mathbf{x}) \\
&= \underset{c_k}{\text{argmax}} \; p(c_k) \cdot p(\mathbf{x} \mid c_k)
\end{aligned}
$$

ë§Œì•½ Prior $p(c_k)$ì˜ í™•ë¥ ì´ ëª¨ë‘ ê°™ë‹¤ë©´, ì‚¬ì „ í™•ë¥ ì— ëŒ€í•œ ë¶€ë¶„ë„ ìƒëµí•  ìˆ˜ ìˆë‹¤ ğŸ˜

Data $\mathbf{x}$ê°€ feature $a_1, a_2, \dots, a_n$ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ìœ„ì˜ ì‹ë“¤ì„ ë‹¤ì‹œ ì¨ë³´ì.

$$
\begin{aligned}
p(c_k \mid a_1, \dots, a_n)
&= \frac{p(c_k) \cdot p(a_1, \dots, a_n \mid c_k)}{p(a_1, \dots, a_n)} \\
&\propto p(c_k) \cdot p(a_1, \dots, a_n \mid c_k)
\end{aligned}
$$

ì´ë•Œ, \<NB Classifier\>ì—ì„œëŠ” ê° featureê°€ **ë…ë¦½(independent)**í•˜ë‹¤ê³  ê°€ì •í•œë‹¤. ë”°ë¼ì„œ,

$$
\begin{aligned}
p(a_1, \dots, a_n) &= p(a_1) \cdots p(a_n) \\
p(a_1, \dots, a_n \mid c_K) &= p(a_1 \mid c_k) \cdots p(a_n \mid c_k)
\end{aligned}
$$

ì‹ì„ ë‹¤ì‹œ ì¨ë³´ë©´,

$$
\begin{aligned}
p(c_k \mid a_1, \dots, a_n)
&\propto p(c_k) \cdot p(a_1, \dots, a_n \mid c_k) \\
&= p(c_k) \cdot p(a_1 \mid c_k) \cdots p(a_n \mid c_k) \\
&= p(c_k) \cdot \prod^n_{i=1} p(a_n \mid c_k)
\end{aligned}
$$

ë§Œì•½ ìœ„ì˜ ì‹ì—ì„œ $\propto$ë¥¼ ì œê±°í•˜ê³  ë“±ì‹ìœ¼ë¡œ ë°”ê¾¸ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
p(c_k \mid a_1, \dots, a_n)
= \frac{\displaystyle p(c_k) \cdot \prod^n_{i=1} p(a_n \mid c_k)}{\displaystyle\sum_i p(c_i) p(\mathbf{x} \mid c_i)}
$$

<hr/>

### Gaussian Naive Bayes

\<Gaussian NB\> ëª¨ë¸ì„ Likelihood $p(\mathbf{x} \mid c_k)$ê°€ Gaussian ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤. ê·¸ëŸ°ë°, ì´ë•Œ $\mathbf{x}$ì˜ ê° featureê°€ ëª¨ë‘ ë…ë¦½ì´ë¯€ë¡œ ìš°ë¦¬ëŠ” ê° featureì— ëŒ€í•œ Gaussian Likelihoodë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•´ ì‚¬ìš©í•œë‹¤.

$$
p(a_i \mid c_k) = \frac{1}{\sqrt{2\pi \sigma_{i, c_k}^2}} \cdot \exp \left( - \frac{(a_i - \mu_{c_k})}{2 \sigma_{i, c_k}^2} \right)
$$

ìœ„ì˜ Likelihood functionì„ í™œìš©í•´ ì‹ì„ ë‹¤ì‹œ ê¸°ìˆ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\begin{aligned}
p(c_k \mid a_1, \dots, a_n)
&\propto p(c_k) \cdot p(a_1, \dots, a_n \mid c_k) \\
&= p(c_k) \cdot \prod^n_{i=1} p(a_n \mid c_k) \\
&= p(c_k) \cdot \prod^n_{i=1} \frac{1}{\sqrt{2\pi \sigma_{i, c_k}^2}} \cdot \exp \left( - \frac{(a_i - \mu_{c_k})}{2 \sigma_{i, c_k}^2} \right)
\end{aligned}
$$

<hr/>

#### ì°¸ê³ ìë£Œ

- ['opengenus'ì˜ í¬ìŠ¤íŠ¸](https://iq.opengenus.org/gaussian-naive-bayes/)
- ['ratsgo'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://ratsgo.github.io/machine%20learning/2017/05/18/naive/)
- ['heung-bae-lee'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://heung-bae-lee.github.io/2020/04/14/machine_learning_08/)