---
title: "Supp-1: Linear Algebra - 4"
toc: true
toc_sticky: true
categories: ["Linear Algebra", "Applied Statsitcs"]
---

2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

### Set-up

ì–´ë–¤ í–‰ë ¬ $A$ì— ëŒ€í•´ ê·¸ í–‰ë ¬ì˜ ì œê³±ê·¼ í–‰ë ¬ì„ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œ? ê·¸ëŸ¬ë‹ˆê¹Œ ì–´ë–¤ í–‰ë ¬ $B$ê°€ ìˆì–´ $B B = A$ê°€ ë˜ëŠ” ê·¸ëŸ° í–‰ë ¬ì„ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‹¤.

ìš°ì„  í–‰ë ¬ì´ ì œê³±ê·¼ì„ ê°€ì§€ëŠ” ì¢‹ì€ ì„±ì§ˆì„ ê°€ì§€ë ¤ë©´, í–‰ë ¬ $A$ê°€ **symmetric** matrixê°€ ë˜ì–´ì•¼ í•  ê²ƒì´ë‹¤; $A \in \mathbb{R}^{n\times n}$.

ì‹¤ìˆ˜ ì˜ì—­ì—ì„œëŠ” ì–´ë–¤ ìˆ˜ $x$ê°€ ì œê³±ê·¼ì„ ê°€ì§€ë ¤ë©´, ê·¸ ìˆ˜ê°€ $x \ge 0$ ì—¬ì•¼ í–ˆë‹¤. (ë³µì†Œ ì œê³±ê·¼ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹¤ìˆ˜ ì˜ì—­ ìœ„ì— ì •ì˜ëœ ì œê³±ê·¼ ì—°ì‚°ì„ ìƒê°í•˜ì.) ì¦‰, ì œê³±ê·¼ì„ ì •ì˜í•˜ê¸° ìœ„í•´ì„  non-negative"ì— ëŒ€í•œ ê°œë…ì„ í–‰ë ¬ì˜ ì˜ì—­ìœ¼ë¡œ ëŒì–´ì˜¬ë ¤ì•¼ í•œë‹¤. ê·¸ëŸ° ì ì—ì„œ ì´ë²ˆì— ì‚´í´ë³¼ \<Nonnegative Definite\>ëŠ” "non-negative"ë¥¼ í™•ì¥í•œ ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ì¢‹ë‹¤.

## Nonnegative Definite Matrices

<span class="statement-title">Theorem.</span><br>

For a **symmetric matrix** $A \in \mathbb{R}^{n \times n}$, T.F.A.E.

(1) $A$ is \<non-negative definite\>, denoted $A \succeq 0$:

$$
\mathbf{x}^T A \mathbf{x} \ge 0 \quad \text{for every} \quad \mathbf{x} \in \mathbb{R}^{n}
$$

(2) All eigenvalues of $A$ are non-negative.

(3) $A = B^T B$ for some $B$.

(4) $A$ is **variance-covariance matrix** of some random variable.

<br/>

\<Nonnegative Definite\>ì˜ ì˜ë¯¸ë¥¼ ì¢€ë” ì‚´í´ë³´ì. $A$ê°€ symmetric matrixì´ë¯€ë¡œ \<**spectral decomposition**\>ì´ ê°€ëŠ¥í•˜ë‹¤; $A = UDU^T$

$$
\begin{aligned}
  A = UDU^T = d_1 \mathbf{u}_1 \mathbf{u}_1^T + \cdots + d_n \mathbf{u}_n \mathbf{u}_n^T
\end{aligned}
$$

ì—¬ê¸°ì— ì¢Œìš°ì— ë²¡í„° $\mathbf{x}$ë¥¼ ê³±í•´ì£¼ì.

$$
\begin{aligned}
  \mathbf{x}^T A \mathbf{x} &= \mathbf{x}^T \left( d_1 \mathbf{u}_1 \mathbf{u}_1^T + \cdots + d_n \mathbf{u}_n \mathbf{u}_n^T \right) \mathbf{x} \\
  &= d_1 (\mathbf{u}_1^T \mathbf{x})^2 + \cdots + d_n (\mathbf{u}_n^T \mathbf{x})^2 \quad (\because \mathbf{x}^T \mathbf{u}_i {\mathbf{u}_i}^T \mathbf{x} = \mathbf{x}^T \mathbf{u}_i \cdot {\mathbf{u}_i}^T \mathbf{x} )
\end{aligned}
$$

ë§Œì•½ $\mathbf{x}^T A \mathbf{x} \ge 0$ì´ ë˜ê¸° ìœ„í•´ì„  ì–´ë–¤ ì¡°ê±´ì´ í•„ìš”í• ê¹Œ? ìš°ì„  ì‹ì˜ ìš°ë³€ì—ì„œ $(\mathbf{u}_1^T \mathbf{x})^2 \ge 0$ì´ ìˆë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•˜ì. ë”°ë¼ì„œ, ë§Œì•½ ëª¨ë“  $d_i$ê°€ non-negativeë¼ë©´, ìš°ë³€ì€ ë‹¹ì—°íˆ non-negativeê°€ ë  ê²ƒì´ë‹¤! ì´ê²ƒìœ¼ë¡œ ($\impliedby$) ë°©í–¥ì„ ì¦ëª…í–ˆë‹¤.

($\implies$) ë°©í–¥ì„ ì¦ëª…í•˜ë ¤ë©´, $\mathbf{x} = \mathbf{u}_i$ë¡œ ì„¤ì •í•´ë³´ë©´ ëœë‹¤.

$$
{\mathbf{u}_i}^T A \mathbf{u}_i = d_i ({\mathbf{u}_i}^T \mathbf{u}_i)^2 \ge 0
$$

ìœ„ì˜ ë¶€ë“±ì‹ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” $d_i \ge 0$ì´ ë˜ì–´ì•¼ í•œë‹¤.

$\blacksquare$

<hr/>

## Positive Definite Matrices

<span class="statement-title">Theorem.</span><br>

For a **symmetric matrix** $A \in \mathbb{R}^{n \times n}$, T.F.A.E.

(1) $A$ is \<positive definite\>, denoted $A \succ 0$:

$$
\mathbf{x}^T A \mathbf{x} > 0 \quad \text{for every} \quad \mathbf{x} \in \mathbb{R}^{n} \setminus \{\mathbf{0}\}
$$

[^1]

(2) All eigenvalues of $A$ are positive.

(3) $A = B^T B$ for some non-singular $B$.

(4) $A$ is non-negative definite and non-singular.

(5) There exist linearly independent vectors $\mathbf{u}_1, \cdots, \mathbf{u}_n \in \mathbb{R}^n$ s.t. $$\displaystyle A = \sum^n_{j=1} {\mathbf{u}_j}^T \mathbf{u}_j$$

<br/>

(4)ë²ˆ ëª…ì œë¥¼ ì¢€ë” ì‚´í´ë³´ì. í–‰ë ¬ $A$ê°€ **SDP**(symmetric and positive definite)ë¼ë©´, ìš°ë¦¬ê°€ ì–´ë–»ê²Œ $A^{1/2}$ë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œ? ë‹µì€ ì—­ì‹œ \<Spectral Decomposition\>ì— ìˆë‹¤!!

\<Spectral Decomposition\>ì— ì˜í•´ í–‰ë ¬ $A$ëŠ” ì•„ë˜ì™€ ê°™ì´ ë¶„í•´ëœë‹¤.

$$
A = UDU^T
$$

ë§Œì•½ ì´ë•Œ $A^2$ì„ êµ¬í•œë‹¤ë©´,

$$
A^2 = A \cdot A = (UDU^T) (UDU^T) = UD^2 D^T
$$

ì¦‰, orthogonal matrix $U$ë¼ëŠ” ì¢‹ì€ ë…€ì„ì´ ìˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” í–‰ë ¬ $A$ì— ëŒ€í•œ ë©±ì—°ì‚°(power operation)ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤!!

ì´ ê°™ì€ ì•„ì´ë””ì–´ë¡œ $A^{1/2}$ë¥¼ ìœ ë„í•´ë³´ì. ê°„ë‹¨í•˜ê²Œ ì¶”ë¡ í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë˜ì§€ ì•Šì„ê¹Œ?

$$
A^{1/2} = UD^{1/2}U^T
$$

ì •ë‹µì´ë‹¤! ë§ˆì°¬ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ìŒìˆ˜ì— ëŒ€í•œ ë©±ì—°ì‚°ë„ ì‰½ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$
A^{-1} = UD^{-1}U^T
$$

### Convex Function

Positive definite matrix $A$ë¥¼ ì‚¬ìš©í•˜ë©´, \<convex function\>[^2] í•˜ë‚˜ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤. ë¨¼ì € \<convex function\>ì˜ ì •ì˜ë¶€í„° ì‚´í´ë³´ì.

<span class="statement-title">Definition.</span><br>

A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is called to be \<convex\> if

$$
f(\lambda \mathbf{x} + (1-\lambda)\mathbf{y}) \le \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y})
$$

for every $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ and $0\le \lambda \le 1$.


<div class="img-wrapper">
  <img src="https://ars.els-cdn.com/content/image/3-s2.0-B9780128008065000044-f04-23-9780128008065.jpg" width="280px">
</div>

ì‹ì„ ì˜ ì‚´í´ë³´ë©´, $\lambda \mathbf{x} + (1-\lambda)\mathbf{y}$ëŠ” $\mathbf{x}$, $\mathbf{y}$ ì‚¬ì´ì˜ ë‚´ë¶„ì ì´ë‹¤. ë˜í•œ, $\lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y})$ëŠ” ë‘ ì  $\mathbf{x}$, $\mathbf{y}$ë¥¼ ì‡ëŠ” ì§ì„  ìœ„ì˜ í•œ ì ì´ë‹¤.

ì¦‰, ë¶€ë“±ì‹ì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ë‚´ë¶„ì ì—ì„œì˜ í•¨ìˆ˜ê°’ì€ ì§ì„  ìœ„ì˜ ê°’ë³´ë‹¤ í•­ìƒ ì‘ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤!

<br/>

\<convex\>ì— ëŒ€í•œ ì •ë¦¬ë¥¼ ì‚´í´ë³´ì.

<span class="statement-title">Theorem.</span><br>

Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is twice continuously differentiable, if $f$ is convex if and only if

$$
\frac{\partial^2 f}{\partial \mathbf{x} \partial \mathbf{x}^T} \succeq 0
$$

ì¦‰, **ë‘ ë²ˆ ë¯¸ë¶„í•œ ê²ƒì´ í•­ìƒ 0ë³´ë‹¤ í¬ë‹¤ë©´**, convex functionì´ë¼ê³  ì •ì˜í•  ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ë‹¤! 2ì°¨ì›ì˜ $f(x) = ax^2 + bx + c$ì—ì„œëŠ” $f\'\'(x) = a \ge 0$ ì„ì„ ë– ì˜¬ë¦¬ë©´ ì¢€ ì™€ë‹¿ì„ ê²ƒì´ë‹¤.

<br/>

ì´ë²ˆì—ëŠ” $A \succeq 0$ ì¸ í–‰ë ¬ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ convex functionì„ ìœ ë„í•´ë³´ì.

<span class="statement-title">Example.</span><br>

A quadratic function $f(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} + \mathbf{a}^T \mathbf{x}$ is **convex** if and only if $A \succeq 0$.

ìœ„ì™€ ê°™ì´ í•¨ìˆ˜ $f(x)$ë¥¼ ì •ì˜í•˜ë©´, ë‘ ë²ˆ ë¯¸ë¶„í–ˆì„ ë•Œ,

$$
\frac{\partial^2 f}{\partial \mathbf{x} \partial \mathbf{x}^T} = A \succeq 0
$$

ê°€ ë˜ê¸° ë•Œë¬¸ì— **convex function**ì´ ëœë‹¤. Quadratic formì—ì„œ convexì¸ ì„±ì§ˆì€ ì •ë§ ì¤‘ìš”í•œë°, **Quadratic formì´ convexê°€ ë˜ì–´ì•¼ max/minì„ ë…¼í•  ìˆ˜ ìˆê¸° ë•Œë¬¸**ì´ë‹¤!

<hr/>

## Orthogonal Projection

<span class="statement-title">Definition.</span><br>

For a subspace $\mathcal{L} \subseteq \mathbb{R}^n$, the \<**orthogonal complement**\> of $\mathcal{L}$ is defined as

$$
\mathcal{L}^{\perp} = \{ \mathbf{x} \in \mathbb{R}^n : \mathbf{x}^T \mathbf{y} = 0 \quad \text{for all} \quad \mathbf{y} \in \mathcal{L} \}
$$

í‰ì†Œì— ìƒê°í•˜ë˜ orthogonalì— ëŒ€í•œ ê°œë…ì„ vector spaceì— ì ìš©í•œ ê²ƒì´ **orthogonal complement**ë‹¤.

<span class="statement-title">Theorem.</span><br>

Each $\mathbf{x} \in \mathbb{R}^n$ can be uniquely represented as

$$
\mathbf{x} = \mathbf{x}_{\mathcal{L}} + \mathbf{x}_{\mathcal{L^{\perp}}}
$$

where $$\mathbf{x}_{\mathcal{L}} \in \mathcal{L}$$ and $$\mathbf{x}_{\mathcal{L^{\perp}}} \in \mathcal{L}^{\perp}$$.

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë²¡í„° $\mathbf{x}_{\mathcal{L}}$ë¥¼ $\mathbf{x}$ì˜ $\mathcal{L}$ë¡œì˜ \<**orthogonal projection**\>ì´ë¼ê³  í•œë‹¤.

ê·¸ë¦¬ê³  ì´ \<orthogonal projection\>ì€ Linear mappingì´ë‹¤. ê·¸ë˜ì„œ í–‰ë ¬ì˜ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤!!

<div style="text-align:center; margin: 3px" markdown="1">
The map $\mathbf{x} \mapsto \mathbf{x}_{\mathcal{L}}$ is a linear mapping.
</div>

<span class="statement-title">Theorem.</span><br>

$$
\| \mathbf{x} - \mathbf{x}_{\mathcal{L}}\| \le \| \mathbf{x} - \mathbf{y} \| \quad \text{for every} \quad \mathbf{y} \in \mathcal{L}
$$

ìœ„ ë¶€ë“±ì‹ì˜ ì˜ë¯¸ëŠ” $\mathcal{L}$ ìœ„ì˜ ë²¡í„°ì™€ $\mathbf{x}$ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì´ ë•Œ, orthogonal proj. $\mathbf{x}_{\mathcal{L}}$ì´ ê°€ì¥ ì§§ì€ ê±°ë¦¬ë¥¼ ë±‰ìŒì„ ë§í•œë‹¤. ê·¸ë¦¼ìœ¼ë¡œ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/computer-science/statistical-data-mining/orthogonal-proj-1.jpg" | relative_url }}" width="230px">
</div>

<span class="statement-title">Definition.</span> idempotent or projection<br>

$A \in \mathbb{R}^{n\times n}$ is called an \<idempotent\> or \<projection\> matrix if $A^2 = A$.

<span class="statement-title">Theorem.</span><br>

T.F.A.E.

(1) $A\mathbf{x}$ is the **orthogonal projection** of $\mathbf{x}$ onto $\mathcal{C}(A)$.

ì´ ëª…ì œëŠ” $\mathbf{x}$ì— $A$ë¥¼ ê³±í•˜ëŠ” ì—°ì‚°(ë§¤í•‘) ìì²´ê°€ $\\{A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n \\}$ì¸ ì§‘í•©ì„ ìœ ë„í•˜ëŠ”ë°, ì´ ì§‘í•©ì´ ë°”ë¡œ $\mathcal{C}(A)$ì´ë‹¤.

(2) $A$ is a **projection** and $\mathcal{N}(A) \perp \mathcal{C}(A)$.

ì¦‰, for every $\mathbf{x} \in \mathcal{N}(A)$, $\mathbf{y} \in \mathcal{C}(A)$, $\mathbf{x}^T \mathbf{y} = 0$.

(3) $A$ is **symmetric** and **idempotent**.

ê·¸ë˜ì„œ ë§Œì•½ ìœ„ì˜ ëª…ì œ ì¤‘ í•˜ë‚˜ë¼ë„ ì„±ë¦½í•œë‹¤ë©´, $A$ëŠ” \<orthogonal projection matrix\> onto $\mathcal{C}(A)$ê°€ ëœë‹¤.

<br/>

<span class="statement-title">Theorem.</span><br>

Let $A \in \mathbb{R}^{n\times n}$ and symmetric. Then, T.F.A.E.

(1) $A^2 = A$

(2) All eigenvalues of $A$ are either 0 or 1.

(3) $\text{rank}(A) + \text{rank}(I_n - A) = n$

((1)$\implies$(2))ëŠ” ì‰½ê²Œ \<spectral decomposition\>ì„ í™œìš©í•˜ë©´, ì‰½ê²Œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.

<div class="proof" markdown="1">

Because $A$ is symmetric, $A = UDU^T$ by spectral theorem.

By statement (1), $A^2 = A$

$$
A^2 = (UDU^T)(UDU^T) = UD^2U^T = UDU^T
$$

ë”°ë¼ì„œ, $D^2 = D$. ì´ê²ƒì„ ë§Œì¡±í•˜ë ¤ë©´, $d_i^2 = d_i$ê°€ ë˜ì–´ì•¼ í•œë‹¤. ì´ê²ƒì€ $d_i = 0$ or $d_i = 1$ì¼ ë•Œë§Œ ê°€ëŠ¥í•˜ë‹¤. $\blacksquare$

</div>

eigenvalue $d_i$ê°€ 0 or 1ì´ë¼ëŠ” ì‚¬ì‹¤ì€ proj. $A$ê°€ $d_i = 1$ì¸ íŠ¹ì • $u_i$ ë²¡í„°ë§Œ ì‚´ë¦¬ê²Œ í•˜ëŠ” ì—°ì‚°ì„ì„ ì•Œ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

((2)$\implies$(3))ë„ ì¦ëª…í•´ë³´ì. ì´ê±´ rankê³¼ eigenvalue ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í†µí•´ ì‰½ê²Œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.

<div class="proof" markdown="1">

rankëŠ” (# of non-zero eigenvalue)ë¡œ ì •ì˜ëœë‹¤. orthognoal projì¸ $A$ëŠ” eigvenvalueê°€ 0 ë˜ëŠ” 1ì´ë¯€ë¡œ $d_i = 1$ì˜ ê°¯ìˆ˜ë¥¼ ì„¸ë©´ ëœë‹¤.

$I_n - A$ë¥¼ ì‚´í´ë³´ì. ì´ê±´ $A$ì˜ $d_i$ì˜ ê°’ì„ í† ê¸€ì‹œì¼œì¤€ë‹¤. ë”°ë¼ì„œ, $I_n - A$ì˜ rankëŠ” $A$ì˜ ê²ƒê³¼ complementí•˜ê²Œ ëœë‹¤. $\text{rank}(I_n - A) = n - \text{rank}(A)$. $\blacksquare$

</div>

<br/>

ë“œë””ì–´ ë§ˆì§€ë§‰ Theoremì´ë‹¤. í•˜ì§€ë§Œ, ì•„ë˜ì˜ ëª…ì œëŠ” ì´ \<í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹\>ì´ë¼ëŠ” ê³¼ëª©ì—ì„œ \<Regression\>ì„ ë‹¤ë£° ë•Œ ì •ë§ì •ë§ ë§ì´ ì“°ê²Œ ë˜ëŠ” ì •ë¦¬ì´ë¯€ë¡œ, ì •ë§ ì¤‘ìš”í•˜ë‹¤! ğŸ”¥

<span class="statement-title">Theorem.</span><br>

Let $X = (\mathbf{x}_1, \dots, \mathbf{x}_p)$ be an $n\times p$ matrix with $\text{rank}(X) = p$[^3] and

$$
H = X(X^TX)^{-1}X^T
$$

Then, $H$ is the **orthogonal projection** onto $C(X)$, that is

(1) $H^2 = H$

(2) $\mathcal(H) \perp \mathcal{N}(H)$

(3) $\mathcal{C}(H) = \mathcal{C}(X)$

ì´ë•Œ, $X$ë¡œë¶€í„° ìœ ë„í•œ matrix $H$ë¥¼ \<**hat matrix**\>ë¼ê³  í•œë‹¤.

<hr/>

[^1]: ì°¸ê³ ë¡œ ì˜ë²¡í„° $\mathbf{0}$ì„ ë¹¼ëŠ” ì´ìœ ëŠ” ì˜ë²¡í„°ë¥¼ ê³±í•˜ë©´ $\mathbf{x}^T A \mathbf{x} = 0$ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.

[^2]: "convex"ëŠ” ë³¼ë¡í•œ, "concave"ëŠ” ì˜¤ëª©í•œì„ ì˜ë¯¸í•œë‹¤.

[^3]: $X$ì˜ ëª¨ë“  Column $\mathbf{x}_i$ê°€ ì„œë¡œ linearly independent í•˜ë‹¤ëŠ” ë§ì´ë‹¤.