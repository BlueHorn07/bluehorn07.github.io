---
title: "Supp-1: Linear Algebra - 1"
toc: true
toc_sticky: true
categories: ["Linear Algebra", "Applied Statsitcs"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- (review) Matrix
- Vector Space
- Linearly Independent
- basis & dimension
- Column space & Row space & Null space
- Fundamental Theorem of Linear Algebra
  - Rank Theorem
- Rank
- Determinant

<hr/>

## (Reveiw) Matrix

âœ¨ (ì¤‘ìš”) ë³„ë‹¤ë¥¸ ì–¸ê¸‰ì´ ì—†ë‹¤ë©´, ëª¨ë“  ë²¡í„° "**column vector**"ë‹¤. âœ¨

A $3 \times 1$ vector $\mathbf{x}$ is

$$
\mathbf{x} = \begin{pmatrix}
  1 \\ 2 \\ 3
\end{pmatrix}
$$


<span class="half_HL">\<í–‰ë ¬; matrix\>ëŠ” "vector"ë¥¼ ì›ì†Œë¡œ ê°–ëŠ” "(column) vector"ë‹¤.</span>

A $4 \times 3$ matrix $\mathbf{X}$ is

$$
\mathbf{X} = \begin{pmatrix}
  \mathbf{x}_1^T \\ \mathbf{x}_2^T \\ \mathbf{x}_3^T \\ \mathbf{x}_4^T
\end{pmatrix} = \begin{pmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
  10 & 11 & 12
\end{pmatrix}
$$

- $X_i$: $X$ì˜ $i$ë²ˆì§¸ ì›ì†Œ = $\mathbf{x}_i$
- $X_{ij}$: $X$ì˜ $i$ë²ˆì§¸ ì›ì†Œ $\mathbf{x}_i$ì˜ $j$ë²ˆì§¸ ì›ì†Œ

í–‰ë ¬ì„ ì—´ë²¡í„°ë¡œ í•´ì„í•˜ëŠ” ì‹œì•¼ğŸ‘€ë¥¼ ì²´ë“í•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤!

### Matrix Multiplication

í–‰ë ¬ ê³±ì…ˆì˜ ì‹œì‘ì€ ì¢Œí‘œ ë³€í™˜ì´ë‹¤.

$$
\begin{pmatrix}
c_1 & c_2 & c_3 & c_4
\end{pmatrix} \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} = c_1 x + c_2 y + c_3 z + c_4 t
$$

ìœ„ì˜ ì˜ˆì‹œëŠ” ê°„ë‹¨í•œ ê±°ê³ , ì¡°ê¸ˆ ë³µì¡í•˜ê²Œ ê°€ë©´,

$$
X \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} =
\begin{pmatrix}
  \mathbf{x}_1^T \\ \mathbf{x}_2^T \\ \mathbf{x}_3^T \\ \mathbf{x}_4^T
\end{pmatrix} \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} =
\mathbf{x}_1^T \cdot \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} +
\mathbf{x}_2^T \cdot \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} +
\mathbf{x}_3^T \cdot \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix} +
\mathbf{x}_4^T \cdot \begin{pmatrix}
  x \\ y \\ z \\ t
\end{pmatrix}
$$

ì¢Œí‘œ ë³€í™˜, ë˜ëŠ” ì„ í˜• ë³€í™˜ì˜ ë„êµ¬ë¡œì˜ í–‰ë ¬ ê³±ì…ˆì€

For $A \in \mathbb{R}^{n\times p}$, $\mathbf{x} \in \mathbb{R}^{p\times 1}$

$$
A \mathbf{x} \in \mathbb{R}^{n\times 1}
$$

ì¦‰, $p$ì°¨ì› ë²¡í„°ë¥¼ ì°¨ì› $n$ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” $p$ì°¨ì› ë²¡í„°ê°€ $n$ê°œê°€ í•„ìš”í•˜ê³ , ì´ê²ƒì´ ë°”ë¡œ í–‰ë ¬ $A$ë‹¤...!

<br><span class="statement-title">Definition.</span> Design Matrix ğŸ”¥<br>

ì •ê·œ ìˆ˜ì—…ì˜ ì²«ë²ˆì§¸ ê°•ì˜ì—ì„œëŠ” [\<linear regression\>]({{"https://bluehorn07.github.io/computer_science/2021/02/26/overview-of-supervised-learning-1#regression" | relative_url}})ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ë©´ì„œ \<design matrix\> $X$ë¥¼ ì œì‹œí•œë‹¤. ì´ \<design matrix\>ëŠ” $p$-dim input features vector $n$ê°œë¥¼ ëª¨ì€ $n \times p$ ì°¨ì›ì˜ í–‰ë ¬ì´ë‹¤.

$$
X = \begin{pmatrix}
  x_1^T \\ x_2^T \\ \vdots \\ x_n^T
\end{pmatrix} \in \mathbb{R}^{n\times p}
$$

\<linear regression\>ì—ì„  coefficients ë²¡í„°ì¸ $\beta$ë¥¼ ê³±í•´ $X \beta \in \mathbf{R}^{n\times 1}$ì„ êµ¬í•œë‹¤. ê·¸ëŸ¬ë©´ $n\times 1$ì˜ ì—´ë²¡í„°ê°€ ë‚˜ì˜¤ëŠ”ë°, ë§Œì•½ $n \ge 1,000$ë¼ë©´, $X\beta$ê°€ ë„ˆë¬´ í° ì°¨ì›ì˜ ë²¡í„°ê°€ ì•„ë‹Œê°€, ë„ˆì–´ì–´ë¬´ í° ì„ í˜•ë³€í™˜ì´ ì•„ë‹Œê°€ ìƒê°í–ˆë˜ ì ì´ ìˆë‹¤. ê·¸ëŸ°ë° ê·¸ëŸ° ì˜ë¬¸ì€ ì•„ë˜ì˜ ì‹ì„ ë³´ì í•´ê²°ë˜ì—ˆë‹¤.

$$
Y = X\beta + \epsilon
$$

ì¦‰, $X\beta$ì—ì„œ $X$ëŠ” ì„ í˜•ë³€í™˜ìœ¼ë¡œ ì‚¬ìš©ëœ ê²ƒì´ ì•„ë‹ˆë‹¤. ì˜¤íˆë ¤ \<regression\>ì„ ê·¸ ìì²´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ "í–‰ë ¬"ì´ë¼ê³  ë°›ì•„ë“¤ì´ëŠ” ê²ƒì´ ì¢‹ë‹¤! ğŸ˜

<hr/>

## Linear Algebra

<br><span class="statement-title">Definition.</span> Vector Space<br>

A \<**vector space**\> is a set, consisting of vectors, equipped with **<u>scalar multiplication</u>** and **<u>vector addition</u>**.

<br><span class="statement-title">Definition.</span> Linear Function<br>

For two vector spaces $V$ and $W$, a function $f: V \longrightarrow W$ is called to be \<**linear**\> if

$$
f(c\mathbf{x} + \mathbf{y}) = cf(\mathbf{x}) + f(\mathbf{y}) \quad \text{for all} \; \mathbf{x}, \mathbf{y} \in V \; \text{and} \; c \in \mathbb{R}
$$

<br><span class="statement-title">Theorem.</span><br>

Every \<**linear function between Euclidean spaces**\> can be represented as a **<u>matrix multiplication</u>**.

$$
f(\mathbf{x}) = A \mathbf{x}
$$

<br><span class="statement-title">Note.</span><br>

$n \times p$ í¬ê¸°ì˜ í–‰ë ¬ $A = (a_{ij})$ëŠ” \<linear function from $\mathbb{R}^p$ to $\mathbb{R}^n$\>ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤!!

$$
f_A (\mathbf{x}) = A\mathbf{x}
$$

ë³´í†µ $n \times p$ í¬ê¸°ì˜ í–‰ë ¬ì—ì„œ

- $n$ = sample size
- $p$ = # of input variables

ë¡œ í•´ì„í•œë‹¤!

<br><span class="statement-title">Definition.</span> Linear Subspace<br>

$\mathcal{L} \subset \mathbb{R}^n$ is called a \<linear subspace\> of $\mathbb{R}^n$ if

$$
\text{For} \quad \mathbf{x}, \mathbf{y} \in \mathcal{L} \quad \text{and} \quad c \in \mathbf{R} \implies c \mathbf{x} + \mathbf{y} \in \mathcal{L}
$$

<br><span class="statement-title">Definition.</span> span<br>

$\mathcal{L}$ is \<spanned\> by $\{\mathbf{x}_1, \dots, \mathbf{x}_k \}$ if

$$
\mathcal{L} = \left\{ c_1 \mathbf{x}_1 + \cdots + c_k \mathbf{x}_k : c_j \in \mathbb{R} \right\}
$$

ì•ìœ¼ë¡œ ì´ëŸ° spanned subspaceë¥¼ ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•œë‹¤.

$$
\left[ \mathbf{x}_1, \dots, \mathbf{x}_k \right]
$$

<hr/>

<br><span class="statement-title">Definition.</span> Linearly Independent<br>

$\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is called to be \<linearly independent\> if

$$
c_1 \mathbf{x}_1 + \cdots + c_k \mathbf{x}_k = 0 \iff c_1 = \cdots = c_k = 0
$$

ì¦‰, Linear Comb.ê°€ 0ì´ ë˜ë ¤ë©´, Coeffi $c_i$ê°€ ëª¨ë‘ 0ì´ ë˜ì–´ì•¼ í•œë‹¤!

<br><span class="statement-title">Theorem.</span> T.F.A.E.<br>

1. $\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is linearly independent.
2. Any $\mathbf{x}_i$ cannot be represented as a linear combination of $\left\\{ \mathbf{x}_j : j \ne i \right\\}$.
  - ì¦‰, $$\displaystyle \mathbf{x}_i = \sum_{j \ne i} c_j \mathbf{x}_j$$ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ë§ì´ë‹¤.

<br><span class="statement-title">Definition.</span> basis & dimension<br>

$\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is called  a \<**basis**\> of $\mathcal{L}$ if

1. $\mathcal{L} = \left[ \mathbf{x}_1, \dots, \mathbf{x}_k \right]$
2. $\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is linearly inependent.

Also, we call $k$ as a \<**dimension**\> of $\mathcal{L}$, $\dim(\mathcal{L})$.

<br><span class="statement-title">Theorem.</span><br>

If $\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is a basis of $\mathcal{L}$ and $\mathbf{x} \in \mathcal{L}$, there exist **<u>unique</u>** $c_1, \dots, c_k \in \mathbb{R}$ s.t.

$$
\mathbf{y} = c_1 \mathbf{x}_1 + \cdots + c_k \mathbf{x}_k
$$

ì¦‰, \<**basis**\>ì— ì˜í•œ representationì€ **<u>uniqueness</u>**ë¥¼ ë³´ì¥í•œë‹¤!

<br><span class="statement-title">Definition</span> orthogonal & orthonormal<br>

A basis $\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ of $\mathcal{L}$ is called to be \<**orthogonal**\> if $\mathbf{x}_i^T \mathbf{x}_j = 0$ for $i \ne j$.

If the norm of each basis $\mathbf{x}_i$ is one, then we call them as \<**orthonormal**\>.

<br><span class="statement-title">Theorem.</span><br>

If $\left\\{\mathbf{x}_1, \dots, \mathbf{x}_k \right\\}$ is an orthonormal basis of $\mathcal{L}$ and $\mathbf{y} \in \mathcal{L}$, then

$$
\mathbf{y} = \left( \mathbf{x}_1^T \mathbf{y} \right) \mathbf{x}_1 + \cdots + \left( \mathbf{x}_k^T \mathbf{y} \right) \mathbf{x}_k
$$

ì¦‰, ë§Œì•½ \<**orthonormal basis**\>ë¼ë©´, vector $\mathbf{y}$ ë¥¼ í‘œí˜„í•˜ëŠ” <span class="half_HL">ëª¨ë“  coefficient $c_i$ë¥¼ uniquelyí•˜ê²Œ íŠ¹ì •í•  ìˆ˜ ìˆë‹¤</span>ëŠ” ë§ì´ë‹¤!

<hr/>

### Rank Theory

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Column space & Row space & Null space<br>

$$
A \in \mathbb{R}^{n \times p}
$$

$$
A\mathbf{x} =
\begin{bmatrix}
\mid & \cdots & \mid \\
\mathbf{a}_1 & \cdots & \mathbf{a}_p \\
\mid & \cdots & \mid
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\
x_p
\end{bmatrix}
= x_1 \mathbf{a}_1 + \cdots x_p \mathbf{a}_p
$$

1\. \<**Column space**\> $\mathcal{C}(A)$ëŠ” $A$ì˜ Columnìœ¼ë¡œ spaní•œ linear spaceë‹¤.

= $\left[ \mathbf{a}_1, \dots, \mathbf{a}_p \right]$

= $\left\\{ A\mathbf{x} : \mathbf{x} \in \mathbb{R}^p \right\\}$

2\. \<**Row space**\> $\mathcal{R}(A)$ëŠ” $A$ì˜ Rowë¡œ spaní•œ linear spaceë‹¤.

NOTE: $\mathcal{R}(A) = \mathcal{C}(A^T)$

3\. \<**Null space**\> $\mathcal{N}(A)$ëŠ” $A\mathbf{x} = \mathbf{0}$ì¸ ëª¨ë“  $\mathbf{x}$ë¡œ spaní•œ linear spaceë‹¤.

= $\left\\{ \mathbf{x} \in \mathbb{R}^p: A \mathbf{x} = \mathbf{0} \right\\}$

NOTE: ë§Œì•½ $A$ê°€ invertibleì´ë¼ë©´, $A\mathbf{x} = \mathbf{0}$ì„ ë§Œì¡±í•˜ëŠ” $\mathbf{x}$ëŠ” $\mathbf{0}$ ë¿ì´ê¸° ë•Œë¬¸ì—, $\mathcal{N}(A) = \mathbf{0}$ì´ ëœë‹¤.

</div>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Rank<br>

The \<**rank**\> of $A$ is the dimension of $\mathcal{C}(A)$.

</div>

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span> Fundamental Theorem of Liniear Algebra<br/>

<div class="light-margin"></div>

**Part 1**: (Rank-Nullity Theorem; Rank Theorem)

The column and row spaces of an $m \times n$ matrix $A$ both have dimension $r$, the \<**rank**\> of the matrix.

- the nullspace has dimension $n-r$
- the left nullspace has dimension $m-r$

<div class="light-margin"></div>

**Part 2**:

The \<Null space\> and \<Row space\> are \<orthogonal\>.

The \<Left Mull space\> and the \<Column space\> are also \<orthogonal\>.

<div class="light-margin"></div>

**Part 3**:

Any matrix $M$ can be written in the form

$$
U \Sigma V^T
$$

where

- $U$ is an $m\times m$ [unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix).
- $\Sigma$ is an $m\times n$ matrix with non-negative values on the diagonal.
- $V$ is an $n\times n$ unitary matrix.

</div>

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span> Rank Theorem<br>

For any $A \in \mathbb{R}^{n \times p}$,

$$
\dim (\mathcal{C}(A)) = \dim (\mathcal{R}(A))
$$

</div>

ê·¸ë˜ì„œ \<Rank Theorem\>ì— ì˜í•´ í–‰ë ¬ $A$ì˜ \<rank\>ë¥¼ $\dim (\mathcal{C}(A))$ë¡œ ì •ì˜í•˜ë“ , $\dim (\mathcal{R}(A))$ë¡œ ì •ì˜í•˜ë“  ìƒê´€ì´ ì—†ë‹¤!

ì¦ëª…ì€ ì´ê³³ì„ ì°¸ì¡°í•  ê²ƒ ğŸ‘‰ [*proof*](https://brilliant.org/wiki/rank/)

<br/>

<br><span class="statement-title">Properties.</span> Rank<br>

$$
A, B \in \mathbb{R}^{n\times p}
$$

1\. $\text{rank}(A) \le \text{rank}(n, p)$

2\. $\text{rank}(A) = \text{rank}(A^T)$ (by \<Rank Theorem\>)

3\. $\text{rank}(A) + \dim(\mathcal{N}(A)) = p$

4\. If $A \in \mathbb{R}^{n\times n}$, T.F.A.E.
- $A$ is **<u>invertible</u>**
- $\text{rank}(A) = n$
- $\mathcal{N}(A) = 0$
- $\det (A) \ne 0$

5\. $\text{rank}(AB) = \text{rank}(A)$ if $B$ is invertible

6\. $\text{rank}(AB) = \text{rank}(B)$ if $A$ is invertible

7\. $\text{rank}(AB) \le \min \\{ \text{rank}(A), \text{rank}(B)\\}$

8\. $\text{rank}(AA^T) = \text{rank}(A^TA) = \text{rank}(A)$

<br/>

<hr/>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Determinant<br>

The \<**determinant**\> of a matrix $A \in \mathbb{R}^{n \times n}$ is

$$
\sum_{\pi} \text{sgn} (\pi) \cdot \left( a_{1\pi(1)} \cdots a_{n\pi(n)}\right)
$$

where the summation is taken over all permuations of $\\{ 1, \dots, n\\}$.

</div>

\<Determinant\>ì— ëŒ€í•œ ì •ë§ Abstract í•œ ì •ì˜ë‹¤. $3\times3$ ì— ëŒ€í•´ì„  ì•„ë˜ì™€ ê°™ì´ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/computer-science/statistical-data-mining/determinant-3x3-1.png" | relative_url }}" width="320px">
</div>

<br>

<span class="statement-title">Properties.</span> Determinant<br>

ì´ì–´ì§€ëŠ” ì„±ì§ˆë“¤ì€ \<Determinant\>ì— ëŒ€í•œ ì„±ì§ˆì„ ì •ë§ Abtract í•œ ìš©ì–´ë¡œ ê¸°ìˆ í•œ ê²ƒì´ë‹¤.

$$
A, B \in \mathbb{R}^{n\times n}
$$

- Multilinear (= linear for each column & row)

$$
\det (\mathbf{a}_1, \dots, c\mathbf{a}_i + \mathbf{d}, \dots, \mathbf{a}_n) = c \det (\mathbf{a}_1, \dots, \mathbf{a}_i, \dots, \mathbf{a}_n) + \det (\mathbf{a}_1, \dots, \mathbf{d}, \dots, \mathbf{a}_n)
$$

- Alternating

$$
\det (\mathbf{a}_1, \dots, \mathbf{a}_i, \dots, \mathbf{a}_j, \dots, \mathbf{a}_n) = -\det (\mathbf{a}_1, \dots, \mathbf{a}_j, \dots, \mathbf{a}_i, \dots, \mathbf{a}_n)
$$

- Normalized

$$
\det (\mathbf{I}_n) = 1
$$

ì´ì–´ì§€ëŠ” ì„±ì§ˆë“¤ì€ \<Dterminant\>ì—ì„œ ì•„ì£¼ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„±ì§ˆë“¤ì´ë‹¤!

- $\det(c A) = c^n \det (A)$
- $\det (A) = \det (A^T)$
- $\det(AB) = \det(A)\det(B)$
- $\det(A^{-1}) = 1/\det(A)$
- If $A$ is triangular, then $\displaystyle \det A = \prod^n_{i=1} a_{ii}$
- For eigenvalues $\lambda_i$ of $A$, $\displaystyle \det (A) = \prod^n_{i=1} \lambda_i$

<hr/>

ì´ì–´ì§€ëŠ” ë‚´ìš©ì—ì„  \<Eigen Value\>ì™€ \<Eigen Vector\>, \<Matrix Derivative\>, \<Spectral Decomposition\> ë“± ë” ì§€ë„ ë§ì€ ë‚´ìš©ë“¤ì´ íŠ€ì–´ë‚˜ì˜¨ë‹¤ ğŸ¤©

ğŸ‘‰ [Supp-1: Linear Algebra - 2]({{"2021/03/08/supp-1-linear-algebra-2" | relative_url}})