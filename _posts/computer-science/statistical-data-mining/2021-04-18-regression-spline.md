---
title: "Regression Spline"
layout: post
use_math: true
tags: ["Statistical Data Mining"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- Basis Expansion
- [Polynomial Regression](#polynomial-regression)
  - [Local Polynomial Regression](#local-polynomial-regression)
- [Regression Spline](#regression-spine)
  - spline basis function
- [Natural Cubic Spline](#natural-cubic-spline)
  - power basis functions
- [Smoothing Splines](#smoothing-splines)
  - knot selection

<hr/>

ì´ì „ ì±•í„°ì—ì„œëŠ” parameterë¥¼ ì¶”ë¡ í•´ linear regreeion & linear classificationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ë‹¤.

$$
f(x) = E[Y \mid X = x] = \beta^T x
$$

ì´ë²ˆ ì±•í„°ì—ì„œëŠ” **non-linear model**ì— ëŒ€í•´ ì‚´í´ë³¼ ì˜ˆì •ì´ë©°, \<Basis Expansion\>ê³¼ \<Kernel Method\>ì„ ì¤‘ì‹¬ìœ¼ë¡œ non-linear modelì„ ì‚´í´ë³¸ë‹¤!

<hr/>

### Basis Expansion

<span class="statement-title">Definition.</span> Basis expansion<br>

For $m=1, \dots, M$, let $h_m: \mathbb{R}^p \rightarrow \mathbb{R}$ be the $m$-th **transformation** of $X$.

Then, we model as 

$$
f(X) = \sum^M_{m=1} \beta_m h_m (X)
$$

with a **<u>linear basis expansion</u>** in $X$.

<br/>

<span class="statement-title">Example.</span><br>

1\. 1-dimentional projection

$$
h_m(X) = X_m \quad \text{for} \quad m=1, \dots, p
$$

2\. Covariatic transform <small>(ì •ì‹ ëª…ì¹­ì€ ì•„ë‹ˆê³ , Cov ê°™ì€ ëŠë‚Œì´ë¼ ë³¸ì¸ì€ ì´ë ‡ê²Œ ë¶€ë¥¸ë‹¤.)</small>

$$
h_m (X) = X^2_j \quad \text{or} \quad h_m(X) = X_j X_k
$$

3\. log transform or root transform

$$
h_m(X) = \log (X_j) \quad \text{of} \quad h_m (X) = \sqrt{X_j}
$$

4\. range transform

$$
h_m(X) = I(L_m \le X_k < U_m)
$$

íŠ¹ì • ë²”ìœ„ì— ë”°ë¼ $X_k$ë¥¼ ì œë‹¨í•˜ëŠ” transformì´ë‹¤.

<br/>

5\. **Splines** ğŸ”¥

ê³§ ë§Œë‚  ì˜ˆì •!

<br/>

6\. Wavelet bases

ì •ë§ ì¤‘ìš”í•˜ê²Œ ì“°ì´ëŠ” ë°©ë²•ì´ì§€ë§Œ, ì •ê·œ ìˆ˜ì—…ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šì•˜ë‹¤.

<hr/>

### Polynomial Regression

<div class="statement">

"Every smooth function can be approximated by polynomials!" <br/>

<small style="color: grey">-- Weierstrass's Approximation Theorem</small>

</div>

For $p$-dimensional $X$, the $m$-th order polynomial is given as

$$
f(X) = \beta_0 + \left(\sum^p_{j=1} \beta_j X_j\right) + \left(\sum^{p^2}_{j,k} \beta_{jk} X_j X_k\right) + \cdots + \left(\sum^{p^m}_{j_1, \dots, j_m} \beta_{j_1, \dots, j_m} X_{j_1} \cdots X_{jm} \right)
$$

- As $p$ increases, the # of parameters grows exponentially.
- In general, it is difficult to estimate $p$-dimentional regression function for large $p$.

ê·¸ë¦¬ê³  ê·¸ëŸ° $m$-th order polynomial ë°©ì‹ì€ \<Multi-collinearity\>ì— ëŒ€í•œ ë¬¸ì œë„ ê°€ì§€ê³  ìˆë‹¤. ğŸ‘‡

ë…¼ì˜ì˜ í¸ì˜ë¥¼ ìœ„í•´ $p=1$ë¼ê³  í•˜ì. ê·¸ë¦¬ê³ , $X_i$ë¥¼ ì•„ë˜ì™€ ê°™ì´ ë””ìì¸ í•˜ì.

Let $X_1 \sim \text{Unif}(0, 1)$, and $X_m = X_1^m$ for $m \le 4$.

ì´ë•Œ, $X_1, \dots, X_4$ì˜ Corrë¥¼ êµ¬í•´ë³´ë©´,

$$
\text{Corr}(X_1, \dots, X_4) = \begin{bmatrix}
    1.00 & 0.97 & 0.92 & 0.86 \\
    0.97 & 1.00 & 0.98 & 0.95 \\
    0.92 & 0.98 & 1.00 & 0.99 \\
    0.86 & 0.95 & 0.99 & 1.00
\end{bmatrix}
$$

ì™€ ê°™ì€ë°, Corrì˜ ê°’ì„ ì‚´í´ë³´ë©´, ëª¨ë‘ 1ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§„ë‹¤. ì´ê²ƒì€ ê° <span class="half_HL">ë…ë¦½ë³€ìˆ˜ ì‚¬ì´ì— ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤</span>ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì´ê²ƒì„ \<**Multi-collinearity**; ë‹¤ì¤‘ê³µì„ ì„±\>ì´ë¼ê³  í•œë‹¤.

- High order polynomials are often **unstable** at the boundary.

â†’ bad!!! ğŸ˜¥

<br/>

\<Polynomial Regression\>ì—ì„œì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë¥¼ localë¡œ ë¶„í• í•´ ê·¼ì‚¬í•˜ëŠ” \<Local Polynomial Regression\> ë°©ì‹ì´ ì œì•ˆ ë˜ì—ˆë‹¤.

<hr/>

### Local Polynomial Regression

<div class="statement">

"Every smooth function can be locally approximated by low dimentional polynomials!" <br/>

<small style="color: grey">-- Talyor's Theorem</small>

</div>

Supp. that $p=1$. Divide domain interval $(a, b)$ into $K+1$ sub-intervals $(a=\xi_0, \xi_1], (\xi_1, \xi_2], \dots, (\xi_k , b=\xi_{k+1})$.

Then, a local polynomial regression model is given as

$$
f(X) = \sum^{K+1}_{i=1} f_{i}(X) \cdot I(\xi_{i-1} < X \le \xi_i)
$$

where $f_i$ are polynomials and $\xi_i$ are the **<u>knots</u>**.

ì•ì—ì„œ ì‚´í´ë³¸ \<Polynomial Regression\>ê³¼ ë¹„êµí–ˆì„ ë•Œ, <span class="half_HL">ë²”ìœ„ ì „ì²´ë¥¼ ë„ë©”ì¸ìœ¼ë¡œ ê°–ëŠ” polynomialì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ domain intervalì„ ë¶„í• í•´ polynomial fittingì„ í•œë‹¤</span>ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.

<br/>

<span class="statement-title">Example.</span> Constant & Linear<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-1.jpg" | relative_url }}" width="450px">
</div>

$f_i$ë¥¼ constant function, linear functionìœ¼ë¡œ ëª¨ë¸ë§ í–ˆì„ ë•Œì˜ ê²°ê³¼ì´ë‹¤. ê·¸ë¦¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´ <span class="half_HL">knots ì£¼ë³€ì—ì„œ continuous í•˜ì§€ ì•Šë‹¤</span>. ì´ê±¸ non-continuous í˜„ìƒì€ orderê°€ ì»¤ì ¸ë„ ì—¬ì „íˆ ë°œìƒí•œë‹¤. ğŸ˜¥

\<Regression Spline\>ì€ \<Local Polynomial Regression\>ì— "continuous & continuous derivativeì— ëŒ€í•œ ì œì•½"ì„ ì£¼ì–´ non-continuous ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤!

<hr/>

### Regression Spine

<span class="statement-title">Definition.</span> Regression Polynomial Spline<br>

Given a fixed knot sequence $\xi_1, \dots, \xi_K$,, a function defined on an open interval $(a, b)$ is called a \<**<u>regression (polynomial) spline of order $M$</u>**\> if it is a piecewise polynomial of order $M$ on each of the intervals.

$$
(a=\xi_0, \xi_1], (\xi_1, \xi_2], \dots, (\xi_k , b=\xi_{k+1})
$$

and <span class="half_HL">has continuous derivatives up to order $(M-1)$</span>.

ì•ì—ì„œ ì‚´í´ë³¸ \<Local Polynomial Regression\>ì˜ not-continuous ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ \<Regression Spline\>ì—ì„œëŠ” piecewise polynomialì´ $(M-1)$ì°¨ ì—°ì†ì¸ ë„í•¨ìˆ˜ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì´ ì¶”ê°€ëœ ê²ƒì´ë‹¤!!

<br/>
<hr/>

<span class="statement-title">Example.</span> 3rd-order polynomial ($M=3$)<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-2.jpg" | relative_url }}" width="500px">
</div>

ì™¼ìª½ì€ ì•ì—ì„œ ì‚´í´ë³¸ \<Local polynomial regression\>ì˜ ëª¨ë¸ì´ë‹¤. knotì—ì„œ not-continuousí•˜ë‹¤.

ì˜¤ë¥¸ìª½ì€ <span class="half_HL">knotì—ì„œ continuousí•´ì•¼ í•œë‹¤</span>ëŠ” ì œì•½ì„ ì¤€ ëª¨ë¸ì´ë‹¤.

ì´ëŠ” $x \mapsto a_3 x^3 + a_x x^3 + a_1 x + a_0$ì¸ ê° $f_i\left((\xi_{i-1}, \xi_i]\right)$ì— ëŒ€í•´ 

$$
f_i(\xi_i) = f_{i+1}(\xi_i)
$$

ë¼ëŠ” ì¡°ê±´ì´ ì¶”ê°€ëœ ê²ƒì´ë‹¤. ì´ëŠ” ê³§, $f_i(\xi_i) = f_{i+1}(\xi_i)$ì´ê¸° ë•Œë¬¸ì—, $f_{i+1}$ì—ì„œ $b_4, b_3, b_2$ì— ëŒ€í•œ ê°’ì´ ì •í•´ì¡Œë‹¤ë©´, ìë™ìœ¼ë¡œ $b_0$ì˜ ê°’ì´ ê²°ì •ëœë‹¤.

$$
a_3 x^3 + a_2 x^2 + a_1 x + a_0 = b_3 x^3 + b_2 x^2 + b_1 x + \cancelto{\text{calculated by contraint}}{b_0}
$$

ë”°ë¼ì„œ, estimate í•´ì•¼ í•˜ëŠ” Coefficientsì˜ ê°¯ìˆ˜ëŠ”

$$
(M+1)(K+1) - (K-1)
$$

(ex: If $M=3$ and $K=2$, then we need to estimate $4 + 4 - 1$ coefficients.)

<br/>
<hr/>

<span class="statement-title">Example.</span> 3rd-order polynomial with 1st & 2nd derivative<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-3.jpg" | relative_url }}" width="500px">
</div>

ì´ë²ˆì—ëŠ” "knotì—ì„œ continuous" ì¡°ê±´ê³¼ í•¨ê»˜ "<span class="half_HL">knotì—ì„œ 1st/2nd derivatives continuous</span>" ì¡°ê±´ì´ ì¶”ê°€ë˜ì—ˆë‹¤.

ì´ë¥¼ ë§Œì¡±í•˜ë ¤ë©´, $f_i' (\xi_i) = f_{i+1}'(\xi_i)$ë¥¼ ë§Œì¡±í•´ì•¼ í•˜ë©°, ì´ê²ƒì€ ê³§

$$
3 a_3 x^2 + 2 a_2 x + a_1 = 3 b_3 x^2 + 2 b_2 x + \cancelto{\text{calculated by contraint}}{b_1}
$$

ì´ë¯€ë¡œ contraintë¥¼ í†µí•´ $b_1$ì˜ ê°’ì„ ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ ëœë‹¤. 2nd derivative continuousì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ëœë‹¤.

ë”°ë¼ì„œ, estimate í•´ì•¼ í•˜ëŠ” Coefficientsì˜ ê°¯ìˆ˜ëŠ”

$$
(M+1)(K+1) - MK = M + K + 1
$$

<br/>
<hr/>

#### Spline basis function

<span class="statement-title">Notation.</span><br>

$$
x_{+} = \begin{cases}
    x & \text{if} \quad x > 0 \\
    0 & \text{else}
\end{cases}
$$

ë˜ëŠ”

$$
(x-\xi)_{+}^M = \begin{cases}
    (x-\xi)^M & \text{if} \quad x > \xi \\
    0 & \text{else}
\end{cases}
$$

<span class="statement-title">Example.</span><br>

ë§Œì•½ $M=1$(linear) and $K=2$ë¼ë©´, **spline basis function**ì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/spline-basis-function-1.jpg" | relative_url }}" width="500px">
</div>

<br/>

ë§Œì•½ $M=3$(cubic) and $K=2$ë¼ë©´, **spline basis function**ì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/spline-basis-function-2.jpg" | relative_url }}" width="500px">
</div>

<hr/>

### Natural Cubic Spline

ì™„ë²½í•  ê²ƒ ê°™ì€ \<Regression Spline\> ë°©ì‹ë„ ì‘ì€ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤. ë°”ë¡œ ì–‘ë boundaryì—ì„œ regressionì´ ì˜ ì•ˆ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ \<Natural cubic spline\>ì€ <span class="half_HL">ì–‘ëì—ì„œ linearë¡œ ëª¨ë¸ë§ í•œë‹¤<span>.

<span class="statement-title">Definition.</span> Natrual Cubic Spline<br>

A cubic spline is called a \<**natural cubic spline**\>, if it is **<u>linear</u>** beyond the boundary knots $\xi_1$ and $\xi_K$.

<div class="img-wrapper">
  <img src="http://www.stanford.edu/class/stats202/figs/Chapter7/7.7.png" width="500px">
    <p>
  (ì¶œì²˜: <a href="http://web.stanford.edu/class/stats202/notes/Nonlinear/Splines.html">Stanford - STAT202</a>)
  </p>
</div>

ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë†’ì€ ì°¨ì›ì˜ $M$ìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ë¡ \<Regression Spline\>ì€ boundaryì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. \<Natural Cubic Spline\>ì€ ì–‘ëì—ì„œ linearë¡œ ê·¼ì‚¬í•¨ìœ¼ë¡œì¨ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤!

\<Natural Cubic Spline\>ì—ì„œ estimate í•´ì•¼ í•˜ëŠ” Coefficient ìˆ˜ëŠ” $M=3$ì´ë¯€ë¡œ

$$
(M + K + 1) - (2 \times 2) = (3 + K + 1) - 4 = K
$$

<hr/>

### Smoothing Splines

<span class="half_HL">\<knot slection\></span>ì€ Spline Methodì˜ ì£¼ëœ ì´ìŠˆì´ë‹¤. \<smoothing spline\>ì€ ì´ ë¬¸ì œë¥¼ ì•„ë˜ì™€ ê°™ì´ í•´ê²°í•œë‹¤!

Consider $\hat{f} = \underset{f\in\mathcal{F}}{\text{argmin}} \; \text{RSS}_{\lambda}(f)$, where

$$
\text{RSS}_{\lambda} \; (f) = \left(\sum^n_{i=1} \left\{ y_i - f(x_i)\right\}^2\right) + \lambda \int \left\{ f''(t) \right\}^2 \; dt
$$

and $\lambda$ is a fixed smoothing parameter.

ìœ„ì˜ ì‹ì„ ì˜ ì‚´í´ë³´ë©´, RSS ì‹ì— íŒ¨ë„í‹° í…€ì´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë•Œ, íŒ¨ë„í‹° í…€ì—ì„œëŠ” $f'\'(t)$ë¥¼ ì ë¶„í•˜ëŠ”ë°, ì´ê²ƒì€ í•¨ìˆ˜ $f(t)$ì— ëŒ€í•œ \<ê³¡ë¥ ; curverture\>ë¥¼ ì˜ë¯¸í•œë‹¤. $(f'\'(t))^2$ëŠ” ê³¡ë¥ ì˜ ì ˆëŒ“ê°’ì´ë©°, ë”°ë¼ì„œ  íŒ¨ë„í‹° í…€ì€ <span class="half_HL">í•¨ìˆ˜ $f(t)$ê°€ ì–¼ë§ˆë‚˜ flunctuationì´ ì‹¬í•œì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í…€</span>ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

ìœ„ì˜ ìµœì í™” ë¬¸ì œì˜ solutionì¸ $\hat{f}$ë¥¼ \<**smoothing spline estimator**\>ë¼ê³  í•œë‹¤!

1\. If $\lambda = 0$,<br/>
then $\hat{f}$ can be any function that interpolates the data.

2\. If $\lambda = \infty$, <br/>
then $\hat{f}$ is the **least square estimator**. (ì™œëƒí•˜ë©´, $f'\'(t)=0$ì´ ë˜ë ¤ë©´, $f(t)$ê°€ linear functionì´ì–´ì•¼ í•œë‹¤!)

3\. (general case) small $\lambda$ <br/>
model complexity $\uparrow$ / bias $\downarrow$ / variance $\uparrow$<br/>
(= flunctuationì´ ì‹¬í•´ì§!)

4\. (general case) large $\lambda$ <br/>
model complexity $\downarrow$ / bias $\uparrow$ / variance $\downarrow$

<div class="statement" markdown="1">

ë§Œì•½ $\mathcal{F}$ê°€ íŠ¹ì • \<Sobolev space\>ì— ì†í•˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ë¼ë©´, smooth splineì´ ê³§ natrual cubic splineì´ ëœë‹¤ê³  í•œë‹¤.<br/>
<small>ESL, Exercise 5.7</small>

</div>

<br/>

ì•ì—ì„œ ì‚´í´ë³¸ Smoothing Splineì— ëŒ€í•œ ì‹ì€ **infinite-dimensional problem**ì´ì—ˆë‹¤ ì´ê²ƒì„ **linear fit** ë°©ì‹ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ë„ ìˆëŠ”ë°, ê·¸ ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
f(\theta; x) = \sum^n_{j=1} \theta_j N_j (x)
$$

where $N_j$ are basis functions of natrual cubic splines.

Then,

$$
\text{RSS}_{\lambda} \; (\theta) = (\mathbf{y} - \mathbf{N}\theta)^T (\mathbf{y} - \mathbf{N}\theta) + \lambda \cdot \theta^T \mathbf{\Omega} \theta
$$

where $$\mathbf{N}_{ij} = N_j(x_i)$$ and $\displaystyle\mathbf{\Omega}_{ij} = \int {N_i}'\' (t) {N_j}'\'(t) \; dt$.

ì‹ì´ ì¡°ê¸ˆ ë³µì¡í•´ë³´ì´ì§€ë§Œ, ìœ„ì—ì„œ ì œì‹œí•œ $f(\theta; x)$ë¥¼ smoothing splineì˜ ì‹ì— ë§ê²Œ ê¸°ìˆ í•œ ê²ƒì¼ ë¿ì´ë‹¤!

ìœ„ì˜ $\text{RSS}_{\lambda}(\theta)$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ê·¸ëƒ¥ $\theta$ë¡œ ë¯¸ë¶„í•´ì„œ ìµœì†Œê°€ ë˜ëŠ” $\theta$ë¥¼ êµ¬í•´ì£¼ë©´ ëœë‹¤.

$$
\hat{f}(x) = \sum^n_{j=1} \hat{\theta}_j N_j(x)
$$

where

$$
\hat{\theta} = (\mathbf{N}^T \mathbf{N} + \lambda \mathbf{\Omega})^{-1}\mathbf{N}^T \mathbf{y}
$$

ë‚´ë¶€ì— regularization í…€ìœ¼ë¡œ ì¸í•´ $\lambda \mathbf{\Omega}$ê°€ ìƒê²¼ë‹¤!

<br/>

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” spline methodì˜ ë‚¨ì€ ë‚´ìš©ì„ ì¢€ë” ì‚´í´ë³¸ë‹¤. ğŸ¤©

ğŸ‘‰ [Spline Method (2)]({{"/2021/04/19/splines-method-2.html" | relative_url}})