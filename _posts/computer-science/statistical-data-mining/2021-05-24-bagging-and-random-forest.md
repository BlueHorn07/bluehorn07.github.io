---
title: "Bagging & Random Forest"
layout: post
use_math: true
tags: ["Statistical Data Mining"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- [Bagging](#bagging)
  - [Bootstrap Sampling](#bootstrap-sampling)
  - [Variance Reduction for Bagging](#variance-reduction-for-bagging)
- [Random Forest](#random-forest)
- [Out of Bag Error](#out-of-bag-error)

<hr/>

\<Bagging\>, \<Boosting\>, \<Random Forest\>ë¥¼ ì•„ìš°ë¥´ëŠ” ê³µí†µì ì€ ëª¨ë‘ \<**ensemble model**\>ì´ë¼ëŠ” ì ì´ë‹¤. \<**ensemble model**\>ì€ ë§ì€ weak learnerë¥¼ ê²°í•©í•´ strong learnerë¥¼ ë””ìì¸ í•˜ëŠ” ì ‘ê·¼ë²•ì´ë‹¤. ë§ì€ ê²½ìš°ì— \<**ensemble model**\>ê°€ single modelë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ ğŸ˜

<hr/>

### Bagging

#### Bootstrap Sampling

\<Bagging\>ì„ ì‚´í´ë³´ë ¤ë©´, ë¨¼ì € \<bootstrap sampling\>ì— ëŒ€í•´ ì´í•´í•´ì•¼ í•œë‹¤.

ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´, ê¸°ì¡´ì˜ ìƒ˜í”Œ $X = \\{ (x_i, y_i) \\}_i$ì—ì„œ <span class="half_HL">"sampling with replacement"</span>ë¡œ ë½‘ì€ ìƒ˜í”Œì„ ë§í•œë‹¤. \<Bagging\>ì—ì„œëŠ” ì´ \<bootstrap sample\>ì„ $Z$ë¡œ í‘œê¸°í•œë‹¤.

$Z^{(b)}$ë¥¼ $b$ë²ˆì§¸ \<bootstrap sample\>ì´ë¼ê³  í•œë‹¤ë©´, \<Bagging\>ì€ ê° boostrap sampleì— ëŒ€í•´ estiamtor $\hat{f}^{(b)}(\, \cdot \, ; \, Z^{(b)})$ë¥¼ êµ¬í•œë‹¤.

#### Bagging

\<Bagging\>ì€ "Bootstrap Aggregation"ì˜ í•©ì„±ì–´ì´ë‹¤. ëŒ€ì¶© \<bootstrap\>ì„ ì¢…í•©(aggregate) í–ˆë‹¤ëŠ” ë§ì´ë‹¤.

Bagging estimatorëŠ” ì•„ë˜ì˜ ì‹ìœ¼ë¡œ predictionì„ ì§„í–‰í•œë‹¤.

$$
\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum^B_{b=1} \hat{f}^{(b)}(x)
$$

ì¦‰, bootstrap sampleì—ì„œ ë””ìì¸í•œ estiamtorì˜ í‰ê· ìœ¼ë¡œ prediction í•œë‹¤ëŠ” ë§ì´ë‹¤. ì‹¬í”Œí•˜ë‹¤!

<br>

<span class="statement-title">Model.</span> Bagging for classification<br>

\<Bagging\>ìœ¼ë¡œ classificationì„ ìˆ˜í–‰í•  ë•ŒëŠ” ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì´ ìˆë‹¤: "consensus votes" & "averaging the probability"

"consensus votes"ëŠ” $\hat{f}^{(b)}(x)$ì—ì„œ "num. of 1"ê³¼ "num. of 0"ì˜ ìˆ˜ë¥¼ ë¹„êµí•´ ë§ì€ ê²ƒì„ ì·¨í•˜ëŠ” ì ‘ê·¼ì´ë‹¤.

"averaging the probability"ëŠ” ê°œë³„ estimatorê°€ í™•ë¥ ì„ predictí•˜ë©°, ì „ì²´ bagging estimatorì˜ ê²°ê³¼ì— í‰ê· ì„ ë‚´ëŠ” ë°©ë²•ì´ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/bagging-1.png" | relative_url }}" width="400px">
</div>

#### Variance Reduction for Bagging

Let $f_A (x) = E_{\mathbf{Z}}\hat{f}(x)$ be the aggregated estimator. ì´ $f_A (x)$ëŠ” ì‹¤ì œ estimatorê°€ ì•„ë‹ˆë¼ ë…¼ì˜ì˜ í¸ì˜ë¥¼ ìœ„í•´ ì¡´ì¬ë¥¼ ê°€ì •í•œ ë…€ì„ì´ë‹¤!

ìš°ë¦¬ê°€ í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” bagging estiamtorì¸ $\hat{f}_{\text{bag}}$ì™€ $f_A$ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ errorë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•˜ì.

$$
e = E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - \hat{f}(X) \right]^2 \right]
$$

$$
e_A = E_{Y, X} \left[ Y - f_A (X) \right]^2
$$

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

With the squared error loss, $f_A$ always has a smaller risk than $\hat{f}$.

$$
e \ge e_A
$$

If each single classifier is **unstable** â€“ that is, it has **high variance**, the aggregated classifier $f_A$ has a **smaller variance** than a single original classifier.

</div>

ì¦‰, ìš°ë¦¬ê°€ ì œëŒ€ë¡œ aggregate í–ˆë‹¤ë©´, í•­ìƒ ê·¸ error $e_A$ëŠ” í•­ìƒ ë‹¨ì¼ estiatmorì˜ error $e$ ë³´ë‹¤ ì‘ë‹¤ëŠ” ì •ë¦¬ì´ë‹¤.


<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
e
&= E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - \hat{f}(X) \right]^2 \right] \\
&= E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - f_A(X) + f_A(X) - \hat{f}(X) \right]^2 \right] \\
&= E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - f_A(X) \right]^2 + E_{Y, X} \left[ f_A(X) - \hat{f}(X) \right]^2 + 2 E_{Y, X} \left[ (Y - f_A(X)) (f_A(X) - \hat{f}(X)) \right] \right]
\end{aligned}
$$

ì´ë•Œ, $2 E_\mathbf{Z} \left[ E_{Y, X} \left[ (Y - f_A(X)) (f_A(X) - \hat{f}(X)) \right] \right]$ëŠ”

$$
\begin{aligned}
& 2 E_\mathbf{Z} \left[ E_{Y, X} \left[ (Y - f_A(X)) (f_A(X) - \hat{f}(X)) \right] \right] \\
&= 2 E_{Y, X} \left[ (Y - f_A(X)) \cdot E_\mathbf{Z} \left[ (f_A(X) - \hat{f}(X)) \right] \right] \\
&= 2 E_{Y, X} \left[ (Y - f_A(X)) \cdot (f_A(X) - E_\mathbf{Z} \hat{f}(X)) \right] \\
&= 2 E_{Y, X} \left[ (Y - f_A(X)) \cdot \cancel{(f_A(X) - f_A(X))} \right] \\
&= 0
\end{aligned}
$$

ë”°ë¼ì„œ,

$$
\begin{aligned}
e
&= E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - \hat{f}(X) \right]^2 \right] \\
&= E_\mathbf{Z} \left[ E_{Y, X} \left[ Y - f_A(X) \right]^2 + E_{Y, X} \left[ f_A(X) - \hat{f}(X) \right]^2 \right] \\
&= E_{Y, X} \left[ Y - f_A(X) \right]^2 + E_\mathbf{Z} \left[ E_{X} \left[ f_A(X) - \hat{f}(X) \right]^2 \right] \\
&\ge E_{Y, X} \left[ Y - f_A(X) \right]^2 = e_A
\end{aligned}
$$

</div>

ì´ë•Œ, \<Bagging\>ì— ì˜í•´ ê°œì„ ëœ ì •ë„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
E_\mathbf{Z} \left[ E_{X} \left[ f_A(X) - \hat{f}(X) \right]^2 \right] 
= \text{Var}\left( \hat{f}(X) - f_A(X) \right)  
$$

<br/>

<span class="statement-title">Remark.</span><br>

- <span class="half_HL">Increasing $B$ does not cause an overfitting</span>
  - $B$ê°€ ì»¤ì§ˆ ìˆ˜ë¡ í‰ê· ì— ê°€ê¹Œì›Œì§.
  - ë°˜ë©´ì— Boostingì€ $B$ê°€ ì»¤ì§ˆìˆ˜ë¡ overfitting ë¨.
- ë§Œì•½ individual treeê°€ í¬ë‹¤ë©´, ê°œë³„ treeê°€ overfitting í•  ê°€ëŠ¥ì„±ì€ ìˆìŒ.
- \<Bagging\>ì€ high-variance & low-bais ëª¨ë¸ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„.
- In practice, <span class="half_HL">\<Boosting\> is better than \<Bagging\> in many examples</span>.
  - \<Bagging\>ì„ ë³€í˜•í•œ ëª¨ë¸ì¸ \<**Random Forest**\>ë¥¼ ì‚¬ìš©í•˜ë©´, \<Boosting\>ê³¼ ë¹„ìŠ·í•œ ì •ë„ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•¨!


<hr/>

### Random Forest

\<Bagging\>ì—ì„œ ê°œë³„ DTëŠ” ëª¨ë‘ ë™ì¼í•œ ë¶„í¬ë¥¼ ê°–ëŠ”ë‹¤. ê·¸ë˜ì„œ <span class="half_HL">"bagged estimator"ëŠ” ê°œë³„ bootstrap treeì™€ ë™ì¼í•œ biasë¥¼ ê°–ëŠ”ë‹¤</span>.

ì´ë•Œ, \<Bagging\> ëª¨ë¸ì˜ "variance"ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ <span class="half_HL">ê°œë³„ bootstrap tree ì‚¬ì´ì˜ correlationì„ ì¤„ì—¬ ëª¨ë¸ì„ ê°œì„ </span>í•  ìˆ˜ ìˆë‹¤!!! ğŸ˜

<div class="proof" markdown="1">

The 'average' of $B$ iid random variables with variance $\sigma^2$ has variance $\sigma^2/B$. (ë‹¹ì—°!)

ì´ë²ˆì—ëŠ” iid sampleì´ ì•„ë‹Œ ê²½ìš°ë¥¼ ì‚´í´ë³´ì! $\text{Var}(X_i) = \sigma^2$ì´ê³ , pairwise correlationì€ $\text{Corr}(X_i, X_j) = \rho$ì¸ ê²½ìš°ë‹¤!

The 'average' of $B$ identically distributed random variables with variance $\sigma^2$ and pairwise correlation $\rho$ has variance

$$
\rho \sigma^2 + \frac{1-\rho}{B} \sigma^2
$$

ì´ë•Œ, ì´ê²ƒê³¼ iidì¸ ê²½ìš°ì—ì„œì˜ variance $\sigma^2/B$ë¥¼ ë¹„êµí•´ë³´ì.

$$
\text{Var} \left( \frac{X_1 + \cdots X_B }{B} \right) = \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 \quad \text{vs.} \quad \frac{\sigma^2}{B}
$$

ë§Œì•½ correlation $\rho$ê°€ 0ì´ë¼ë©´, iidì¸ ê²½ìš°ì™€ ê°™ë‹¤.

$$
0 \sigma^2 + \frac{1-0}{B} \sigma^2 = \frac{\sigma^2}{B}
$$

ë§Œì•½ correlation $\rho$ê°€ 1ì¸ ê²½ìš°, ì¦‰ $X = X_i$ë¼ë©´, 

$$
\rho \sigma^2 + \frac{1-1}{B} \sigma^2 = \sigma^2 > \frac{\sigma^2}{B}
$$

ë”°ë¼ì„œ, sampleì´ correlate ë˜ì–´ ìˆë‹¤ë©´, varianceëŠ” ë” ì»¤ì§„ë‹¤.

$$
\rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 \ge \frac{\sigma^2}{B}
$$

</div>

\<Bagging\>ì€ bootstrap sampleë¡œ DTë¥¼ êµ¬ì„±í•œë‹¤. ì´ë•Œ, bootstrap sampleì€ sampling with replacementì´ê¸° ë•Œë¬¸ì— ê°œë³„ DTë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©í•œ inputì˜ ì¼ë¶€ëŠ” í•­ìƒ ì–´ëŠ ì •ë„ ê²¹ì¹  ê²ƒì´ë‹¤. ë”°ë¼ì„œ, <span class="half_HL">\<Bagging\>ì˜ ê°œë³„ DTëŠ” ì„œë¡œ positively correlated ë˜ì–´ ìˆë‹¤!</span> RFëŠ” ì´ëŸ° correlationì„ ì¤„ì—¬ Baggingì„ ê°œì„ í•œë‹¤!!

Before each split in bagging DT, <span class="half_HL">RF selects $m \le p$ of input variables at random as cadidates for splitting</span>. ì¦‰, ë³€ìˆ˜ ì¤‘ ì¼ë¶€ë§Œ ì“°ê¸° ë•Œë¬¸ì—, ê°œë³„ DTë¼ë¦¬ ë³€ìˆ˜ê°€ ê²¹ì¹  í™•ë¥ ì´ ì¤„ì–´ë“ ë‹¤!

RFì˜ ê²½ìš°, ì¼ë¶€ì˜ ë³€ìˆ˜ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— \<bagging\>ë³´ë‹¤ ë” ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì¸ë‹¤!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/random-forest-1.png" | relative_url }}" width="400px">
</div>

<hr/>

### Out of Bag Error

ì´ë²ˆì—ëŠ” \<Bagging\>ê³¼ \<Random Forest\>ì—ì„œ cross-validationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì¸ \<OOB error; Out of Bag error\>ì— ëŒ€í•´ ì†Œê°œí•œë‹¤.

\<Bootstrap sample\>ì˜ ê²½ìš°, sampling with replacementì´ê¸° ë•Œë¬¸ì— 1,000ê°œì˜ ì „ì²´ ìƒ˜í”Œ ì¤‘ì— 800ë§Œ ì“°ì´ê³ , ë‚¨ì€ 200ê°œê°€ ì•ˆ ì“°ì´ê²Œ ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ Bagging, RFì—ì„œëŠ” ì´ëŸ° "out of bag"í•œ ìƒ˜í”Œë“¤ì„ ê°€ì§€ê³  validation errorë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤!!

ê·¸ë˜ì„œ Bagging, RFëŠ” <span class="half_HL">\<OOB error\>ê°€ stabilizeí•˜ëŠ” ìˆœê°„ì— trainingì„ ì¢…ë£Œ</span>í•œë‹¤!


