---
title: "AdaBoost"
layout: post
use_math: true
tags: [applied_statistics]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

\<Boosting\>ì— ëŒ€í•œ ê°œìš”ëŠ” ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ˜

ğŸ‘‰ [Boosting]({{"/2021/05/09/boosting.html" | relative_url}})

<br>

<span class="statement-title">TOC.</span><br>

- AdaBoost
  - Algorithm
- Boosting as Additive Expansion
  - Forward Stagewise Update ğŸ”¥
    - with Squared Error Loss
    - with Exponential Loss ğŸ”¥

<hr/>

### AdaBoost

\<AdaBoost\>ë€ "Adaptive Boosting"ì˜ ì•½ìë¡œ \<Boosting\> ì•Œê³ ë¦¬ì¦˜ì„ ì²˜ìŒìœ¼ë¡œ ê³ ì•ˆí•œ Schapire & Freundì— ì˜í•´ 1996ë…„ ê°œë°œëœ ìµœì´ˆì˜ Boosting ëª¨ë¸ì´ë‹¤.

<br/>

ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê¸° ì „ì— ë¨¼ì € ì…‹ì—…ì„ í™•ì¸í•˜ì.

<div class="light-margin" markdown="1">

**[Data]**

$\\{ (x_1, y_1), \dots, (x_n, y_n) \\}$

- p-dimensional input: $x_i \in \mathcal{X}$
- Binary output: $y_i \in \\{ -1, 1 \\}$ <small>// 1, -1ë¡œ ì¸ì½”ë”©</small>

</div>

<div class="light-margin" markdown="1">

**[Weak Classifier]**

$m$-th Weak Classifier $G_m : \mathcal{X} \rightarrow \\{-1, 1 \\}$

</div>

<div class="light-margin" markdown="1">

**[Prediction]**

predictions are based on a <u>weighted</u> majority vote!

$$
G(x) = \text{sign} \left( \sum^M_{m=1} \alpha_m G_m (x) \right)
$$

where $\alpha_m$ is weight of weak classifier.

</div>

<div class="math-statement" markdown="1">

<span class="statement-title">Algorithm.</span> AdaBoost<br>

**Initialize** the weights for data points<br/>
$w_i = 1 / n$ for $i=1, \dots, n$.

<span color="grey">// training!</span><br/>
**For** $m=1$ **to** $M$<br/>
&emsp;&emsp; Obtain $G_m(x)$ based on the training data using weights $w_i$<br/>
&emsp;&emsp; Compute error of $m$-th classifier.

$$
\text{err}_m = \frac{\displaystyle\sum^n_{i=1} w_i \cdot I(y_i \ne G_m(x_i))}{\displaystyle\sum^n_{i=1} w_i}
$$

&emsp;&emsp; Compute weights for $m$-th classifier

$$
\alpha_m = \log \left( \frac{1 - \text{err}_m}{\text{err}_m}\right)
$$

&emsp;&emsp; // ì •ë¶„ë¥˜ $w_i$ê°€ ê·¸ëŒ€ë¡œ, ì˜¤ë¶„ë¥˜ $w_i$ê°€ ì»¤ì§ â†’ normalize â†’ ì •ë¶„ë¥˜ $w_i$ â–¼, ì˜¤ë¶„ë¥˜ $w_i$ â–²

&emsp;&emsp; Update weights for data points

$$
w_i \leftarrow w_i \cdot \exp \left[ \alpha_m \cdot I(y_i \ne G_m(x_i)) \right] \quad \text{for} \quad i=1, \dots, n
$$

Output $\displaystyle G(x) = \text{sign} \left[ \sum^M_{m=1} \alpha_m G_m(x) \right]$

</div>

<hr/>

#### Boosting as Additive Expansion

\<AdaBoost\> ì•Œê³ ë¦¬ì¦˜ì˜ ê²°ê³¼ì¸ $G(x)$

$$
G(x) = \text{sign} \left[ \sum^M_{m=1} \alpha_m G_m(x) \right]
$$

ë¥¼ **additive/basis expansion** ëª¨ë¸ë¡œë„ ì´í•´í•  ìˆ˜ ìˆë‹¤.

<div class="statement" markdown="1">

<span class="statement-title">Algorithm.</span> general additive function expansion<br>

$$
f(x) = \sum^M_{m=1} \beta_m \cdot h(x; \theta_m)
$$

</div>

ì¦‰, $\alpha_m$ë¥¼ basis coefficient $\beta_m$ë¡œ, weak classifier $G_m(x)$ë¥¼ basis function $h(x; \theta_m)$ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ë‹¤!

ë³¸ë˜ additive modelì„ ERM(Empirical Risk Minimization)ìœ¼ë¡œ fitting ì‹œí‚¤ëŠ” ê²ƒì€ hardí•œ ë¬¸ì œì´ë‹¤.

$$
\min_{\left\{\beta_m, \theta_m\right\}^M_{m=1}} \; \sum^n_{i=1} L \left( y_i, \sum^M_{m=1} \beta_m \cdot h(x_i; \theta_m) \right)
$$

ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œëŠ” computationally infeasible í•˜ë‹¤ê³  ë³´ë©´ ëœë‹¤.

ì´ëŸ° ê³„ì‚°ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œëŠ”

- Update the expansion sequentially
- Consider only a single basis function at each step

ê°€ ìˆëŠ”ë°, ì´ëŸ° ì ‘ê·¼ì„ í”íˆ \<**Forward Stagewise Update**\>ë¼ê³  í•œë‹¤!

ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ë³´ìë©´,

1. $\beta_0$ë¥¼ fitting
2. $\beta_0$ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , $\beta_1$ì„ fitting
3. $\beta_0, \beta_1$ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , $\beta_2$ë¥¼ fitting
4. ...

<div class="math-statement" markdown="1">

<span class="statement-title">Algorithm.</span> Forward Stagewise Update ğŸ”¥<br>

Supp. that the current expansion is

$$
f_{m-1} (x) = \sum^{m-1}_{j=1} \beta_j \cdot h(x; \theta_j)
$$

then, the next update can be achieved by solving

$$
(\beta_m, \theta_m) = \underset{(\beta, \theta)}{\text{argmin}} \sum^n_{i=1} L \left( y_i, f_{m-1}(x_i) + \beta \cdot h(x_i; \theta) \right)
$$

ì¦‰, \<forward stagewise update\>ëŠ” ê¸°ì¡´ì˜ solutionì„ ìˆ˜ì •í•˜ì§€ ì•Šìœ¼ë©´ì„œ ìƒˆë¡œìš´ basis functionì„ ì¶”ê°€í•œë‹¤.

</div>

<div class="math-statement" markdown="1">

<span class="statement-title">Example.</span> Forward Stagewise Update with Squared Error Loss<br>

Consider the squared error loss:

$$
L(y, f(x)) = (y - f(x))^2
$$

then, the loss in forward stagewise is

$$
L(y_i, f_{m-1} (x_i) + \beta \cdot h(x_i; \theta)) = \left( y_i -  f_{m-1} (x_i) - \beta \cdot h(x_i; \theta) \right)
$$

ì´ë•Œ, current residualì¸ $(y_i - f_{m-1}(x_i))$ëŠ” ìƒìˆ˜ê°’ìœ¼ë¡œ $\beta, \theta$ë¥¼ ì°¾ëŠ”ë°ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” $\beta, \theta$ì— ì˜¨ì „íˆ ì§‘ì¤‘í•˜ë©´ ëœë‹¤!

</div>

<div class="math-statement" markdown="1">

<span class="statement-title">Example.</span> Forward Stagewise Update with Exponential Loss ğŸ”¥<br>

Consider the exponential error loss:

$$
L(y, f(x)) = \exp (-y \cdot f(x))
$$

<div class="light-margin" markdown="1">

ì´ë•Œ! ë†€ëê²Œë„ \<AdaBoost\>ëŠ” <span class="half_HL">Forward Stagewise Updates for the ERM with Exponential Loss</span>ì™€ ë™ì¹˜ë‹¤!! ğŸ˜²

ì´ ëª…ì œëŠ” \<Boosting\>ì— ëŒ€í•œ ì™„ì „íˆ ìƒˆë¡œìš´ ë°©ì‹ì˜ ì‹œì•¼ë¥¼ ì œì‹œí•œë‹¤!

</div>

Note that the $m$-th step of the forward stagewise udpates solves

$$
\begin{aligned}
(\beta_m , G_m)
&= \underset{(\beta, G)}{\text{argmin}} \sum^n_{i=1} \exp \left[ -y \cdot \left( f_{m-1}(x_i) + \beta \cdot G(x_i) \right)\right] \\
&= \underset{(\beta, G)}{\text{argmin}} \sum^n_{i=1} w_i^{(m)} \cdot \exp \left[ - \, y_i \cdot \beta  \cdot G(x_i) \right] \\
& \quad \text{where} \quad w_i^{(m)} = \exp \left[ -y \cdot f_{m-1}(x_i) \right]
\end{aligned}
$$

It can be shown that

$$
G_m = \underset{G}{\text{argmin}} \sum^n_{i=1} w_i^{(m)} \cdot I(y_i \ne G(x_i))
$$

$$
\beta_m = \frac{1}{2} \log \left( \frac{1-\text{err}_m}{\text{err}_m}\right)
$$

where

$$
\text{err}_m = \frac{\displaystyle\sum^n_{i=1} w_i^{(m)} \cdot I(y_i \ne G_m(x_i))}{\displaystyle\sum^n_{i=1} w_i^{(m)}}
$$

Then, the update is

$$
f_(x) = f_{m-1}(x) + \beta_m \cdot G_m (x)
$$

and leads the weights for the new iterations to be

$$
\begin{aligned}
w_i^{(m+1)}
&= w_i^{(m)} \cdot \exp \left[ - \, y_i \cdot \beta_m \cdot G_m (x_i) \right] \\
&= w_i^{(m)} \cdot \exp \left[ \alpha_m \cdot I(y_i \ne G_m(x_i)) \right] \cdot \exp \left[ - \beta_m \right] 
\end{aligned}
$$

where $\alpha_m = 2 \beta_m$ appears in the AdaBoost algorithm.

</div>

<div class="statement" markdown="1">

<big>Q. Why Exponential Loss?</big>

Population minimizer of the exponential loss:

$$
\begin{aligned}
f^{*} (x) 
&= \underset{f(x)}{\text{argmin}} \; E_{Y \mid x} \left[ L(Y, f(x)) \right] \\
&= \underset{f(x)}{\text{argmin}} \; E_{Y \mid x} \left[ \exp \left( -Y \cdot f(x) \right) \right] \\
&= \frac{1}{2} \log \frac{P(Y = 1 \mid x)}{P(Y = -1 \mid x)}
\end{aligned}
$$

equivalently,

$$
P(Y = 1 \mid x) = \frac{\exp \left( f^* (x) \right)}{\exp \left( -f^* (x) \right) + \exp \left( f^* (x) \right)}
$$

<details class="math-statement" markdown="1">

<summary>Derivation</summary>

It is true that $P(Y=1 \mid x) = 1 - P(Y=-1 \mid x)$.

$$
\begin{aligned}
f^*(x) 
&= \frac{1}{2} \log \frac{P(Y = 1 \mid x)}{P(Y = -1 \mid x)} \\
\exp \left( 2 \cdot f^* (x) \right) 
&= \frac{P(Y = 1 \mid x)}{P(Y = -1 \mid x)} \\
&= \frac{P(Y = 1 \mid x)}{1 - P(Y=1 \mid x)} \\
\exp \left( 2 \cdot f^* (x) \right) \cdot (1 - P(Y = 1 \mid x))
&= P(Y = 1 \mid x) \\
\exp \left( 2 \cdot f^* (x) \right)
&= P(Y = 1 \mid x) + \exp \left( 2 \cdot f^* (x) \right) \cdot P(Y = 1 \mid x) \\
&= P(Y = 1 \mid x) \cdot \left( 1 - \exp \left( 2 \cdot f^* (x) \right) \right) \\
\frac{\exp \left( 2 \cdot f^* (x) \right)}{1 - \exp \left( 2 \cdot f^* (x) \right)}
&= P(Y = 1 \mid x) \\
\frac{\exp \left( f^* (x) \right)}{\exp \left( -f^* (x) \right) + \exp \left( f^* (x) \right)}
&=
\end{aligned}
$$

$\blacksquare$

</details>




</div>


<hr/>

\<AdaBoost\>ì— ëŒ€í•´ì„œëŠ” <span class="half_HL">\<AdaBoost\>ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì‚¬ì‹¤ "Forward Stagewise Updates for the ERM with Exponential Loss"ë¼ëŠ” ê²ƒ</span>ì„ ê¼­ ê¸°ì–µí•´ì•¼ í•œë‹¤!! ğŸ˜¤

ì´í›„ì— ë‚˜ì˜¨ GBM ê³„ì—´ì´ ë” ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ ìš”ì¦˜ì€ ì˜ ì“°ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤ ğŸ˜¥