---
title: "Clustering"
layout: post
use_math: true
tags: [applied_statistics]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- [K-means Clustering](#k-means-clustering)
  - K-means Algorithm
- [Hierarchical Clustering](#hierarchical-clustering)
- [Spectral Clustering](#spectral-clustering)
- [Model-based Clustering]({{"/2021/06/02/model-based-clustering.html" | relative_url}}) (ë‹¤ìŒí¸)

<hr/>

<div class="statement" markdown="1">

"Clustering refers to a braod set of techniques for partitioning a dataset into homogeneous subgroups called clusters."

</div>

âœ¨ How can we define the "homogeneity"?

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ìš°ë¦¬ëŠ” "clustering"ì„ ìˆ˜í–‰í•˜ëŠ” 3ê°€ì§€ ë°©ì‹ì— ëŒ€í•´ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

1. Distance-based clustering
2. Hierarchical clustering
3. Model-based clustering

<hr/>

### K-means clustering

Algorithm for partitiong a dataset into $k$ subgroups.

ì´ë•Œ, number of cluster $K$ëŠ” ë¯¸ë¦¬ ì •í•´ì ¸ì•¼ í•¨! (HOW?)

<br/>

\<K-means\>ì—ì„œ ìš°ë¦¬ëŠ” \<within-cluster variation measure\>ë¥¼ ì •ì˜í•´ ì‚¬ìš©í•œë‹¤!!

<div class="definition" markdown="1">

For a set $C \subset \\{ 1, 2, \dots, n\\}$, the within-cluster variation measure $W(C)$ is

$$
W(C) = \frac{1}{\left| C \right|} \sum_{\begin{aligned}
  i, j &\in C \\
  i &< j
\end{aligned}} \left\| x_i - x_j \right\|_2^2
$$

â€» small value of $W(C)$ may represent a good cluster.

</div>

ì¦‰, \<within-cluster variation\>ì€ <span class="half_HL">cluster ë‚´ì—ì„œì˜ varianceë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ</span>ì¸ ê²ƒì´ë‹¤!

<br/>

ìš°ë¦¬ê°€ ë°ì´í„°ë¥¼ $C_1, \dots, C_K$ì˜ ì§‘í•©ìœ¼ë¡œ ë¶„í• í•œë‹¤ë©´, ì´ê²ƒì€ ì•„ë˜ì˜ ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒê³¼ ê°™ë‹¤.

$$
\underset{C_1, \dots, C_K}{\text{minimize}} \left\{ \sum_{k=1}^K W(C_k) \right\}
$$

<span class="statement-title">Algorithm.</span> Brute Force<br>

ë‚˜ì´ë¸Œí•˜ê²Œ ì ‘ê·¼í•´ë³´ìë©´, $n$ê°œ ë°ì´í„°ë¥¼ $K$ê°œ ë¶€ë¶„ì§‘í•©ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê°€ì§“ìˆ˜ëŠ” $K^n$ì´ë‹¤. ì¦‰, ë‚˜ì´ë¸Œí•œ ì ‘ê·¼ì€ exponential complexityê°€ ê±¸ë¦¬ëŠ” ì•„ì£¼ ì–´ë ¤ìš´ ë¬¸ì œë‹¤ ã… ã… 

ê·¸ë˜ì„œ Brute Forceì™€ ê°™ì€ ë°©ì‹ì´ ì•„ë‹ˆë¼, ìœ„ ë¬¸ì œì˜ sub-optimal solutionì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ \<K-means clustering\>ì„ ìˆ˜í–‰í•œë‹¤!

<div class="math-statement" markdown="1">

<span class="statement-title">Algorithm.</span> K-means Algorithm<br>

1\. Randomly partition $\\{1, \dots, n \\}$ into $K$ clusters.

2\. Repeat the following until converges.<br/>
&emsp;&emsp; - For each cluster, compute the **cluster centroid**.<br/>
&emsp;&emsp; - **Assign** each obs to the cluster whose centroid is the closest.

</div>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/k-means-algorithm-1.png" | relative_url }}" width="400px">
  <p>figure from ISLR</p>
  <p>ê·¸ë¦¼ì„ ë³´ë©´, 2ë²ˆë§Œì— ìˆ˜ë ´í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤!!</p>
</div>

âœ¨ NOTE: \<K-means algorithm\> is guaranteed to decrease $W(C)$ becuase

$$
W(C_k) = \frac{1}{\left| C \right|} \sum_{\begin{aligned}
  i, j &\in C \\
  i &< j
\end{aligned}} \left\| x_i - x_j \right\|_2^2 = \sum_{i \in C_k} \left\| x_i - \bar{x}_k \right\|_2^2
$$

where $\bar{x}_k$ is mean of cluster $C_k$.

<details class="proof" markdown="1">
<summary>ì‹ ìœ ë„</summary>
ì¶”í›„ì— ì—…ë°ì´íŠ¸!
</details>

\<K-means algorithm\>ì€ global optimumì´ ì•„ë‹ˆë¼ local optimumì„ ì°¾ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.

\<K-means algorithm\>ì˜ ê²°ê³¼ëŠ” initial clusterì— ë”°ë¼ ë°”ë€Œê¸° ë•Œë¬¸ì—, ì„œë¡œ ë‹¤ë¥¸ ëª‡ê°€ì§€ initial clusterë¡œ ì—¬ë ¤ë²ˆ ëŒë ¤ë³´ëŠ” ê²ƒì´ ê¶Œì¥ëœë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/k-means-algorithm-2.png" | relative_url }}" width="400px">
  <p>figure from ISLR</p>
  <p>ê·¸ë¦¼ì„ ë³´ë©´, í•˜ë‚˜ì˜ í˜•íƒœë¡œ clusterê°€ ê²°ì •ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. <br/>
  ìœ„ ê·¸ë¦¼ì—ì„œëŠ” 2, 3, 4, 5ë²ˆì§¸ì˜ ê²½ìš°ë¡œ clustering ë˜ëŠ” ê²ƒì´ ìš°ì„¸í•˜ë‹¤.</p>
</div>

<hr/>

### Hierarchical Clustering

\<Hierarchical Clustering\>ì—ì„œëŠ” "dendrogram"ì´ë¼ëŠ” ê²ƒì„ í†µí•´ clusterë¥¼ ë‚˜ëˆ„ê³ , ë˜ ê·¸ ê³„ì¸µì„ ë¶„ì„í•œë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/hierarchical-clustering-1.png" | relative_url }}" width="400px">
  <p>figure from ISLR</p>
</div>

ìœ„ ê·¸ë¦¼ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–»ê²Œ \<Hierarchical Clustering\>ì„ ìˆ˜í–‰í•˜ê³ , "dendrogram"ì„ ë§Œë“œëŠ”ì§€ ì‚´í´ë³´ì.

ë¨¼ì €, ìœ„ì˜ 2ì°¨ì› ë°ì´í„° ë¶„í¬ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë‘ ì ì„ ì°¾ëŠ”ë‹¤. ì—¬ê¸°ì„œëŠ” (5, 7)ì´ë‹¤. ë‘ ì ì„ ì™¼ìª½ì˜ dendrogramì— í‘œì‹œí•˜ê³ , ë‘ ì ì„ í•˜ë‚˜ì˜ clusterë¡œ ë¬¶ëŠ”ë‹¤. cluster í•˜ë‚˜ê°€ ì¶”ê°€ëœ ë°ì´í„° ë¶„í¬ì—ì„œ ë˜ ê°€ì¥ ê°€ê¹Œìš´ ë‘ ì ì„ ì°¾ëŠ”ë‹¤. ì—¬ê¸°ì„œëŠ” (1, 6)ì´ë‹¤. ì´ ë‘ ì ì„ ì™¼ìª½ì˜ dendrogramì— í‘œì‹œí•˜ê³ , ë˜ í•˜ë‚˜ì˜ clusterë¡œ ë¬¶ëŠ”ë‹¤. ë˜ ì§„í–‰í•˜ë©´, ì´ë²ˆì—ëŠ” ((5, 7), 8)ì´ ê°€ì¥ ê°€ê¹Œìš´ ìŒì´ë‹¤. ì´ê²ƒì„ ì™¼ìª½ì˜ dendrogramì— í‘œì‹œí•œë‹¤. ... (continue) ...

ë°©ë²•ì€ ì •ë§ ê°„ë‹¨í•˜ë‹¤!! í•˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ëª‡ê°€ì§€ ì´ìŠˆê°€ ìˆë‹¤. ê·¸ê²ƒì€ ë°”ë¡œ "cluster-point ê±°ë¦¬ë¥¼ ì–´ë–»ê²Œ ì¸¡ì •í•  ê²ƒì¸ê°€"ë‹¤!

<br/>

ë‘ 'ì 'ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•˜ê²Œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.

ê·¸ëŸ¬ë‚˜ cluster-point, cluster-clusterì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ê°€ëŠ¥í•˜ë‹¤.

- Complete linkage: maximal intercluster dissimilarity
- Single linkage: minimal intercluster dissimilarity
- Average linkage: mean intercluster dissimilarity
- Centroid linkage: dissimilarity between the centroids

ë³´í†µ "complete linkage"ì™€ "average linkage"ë¥¼ ì‚¬ìš©í•œë‹¤. "centroid linkage"ì˜ ê²½ìš°, ìˆœì„œê°€ ì—­ì „ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì—, ê¶Œì¥í•˜ì§„ ì•ŠëŠ”ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/hierarchical-clustering-2.png" | relative_url }}" width="150px">
  <p>Cluster ì‚¬ì´ ê±°ë¦¬ëŠ” non-decreasing í•´ì•¼ í•˜ëŠ”ë°, ì´ ê²½ìš°ëŠ” 8L ë‹¤ìŒì— 7Lì´ ë˜ì–´ inversionì´ ìƒê²¼ë‹¤.</p>
</div>

<hr/>

### Spectral Clustering

ğŸ’¥ ì •ë§ íŠ¹ì´í•œ Clustering ë°©ì‹ì´ë‹¤. ë‚´ìš©ì´ ì¡°ê¸ˆ ì–´ë ¤ìš°ë‹ˆ ì§‘ì¤‘í•˜ì!

<div class="definition" markdown="1">

Traditional clustering (ex. K-means) does not work well when clusters are non-convex. \<**Spectral Clustering**\> is designed for these situations.

</div>

ì¼ë‹¨ ë°”ë¹ ì„œ íŒ¨-ìŠ¤

<hr/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ì‚´í´ë³¸ ë°©ì‹ì€ ëª¨ë‘ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ë“¤ì´ë‹¤. ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” ëª¨ë¸(model)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” \<Model-based clustering\>ì„ ì‚´í´ë³¸ë‹¤. \<Mixture Model\>, \<EM-Algorithm\> ë“± ë¬´ì‹œë¬´ì‹œí•œ ê²ƒë“¤ì´ ì™•ì°½ ë‚˜ì˜¨ë‹¤;;

ğŸ‘‰ [Model-based Clustering]({{"/2021/06/02/model-based-clustering.html" | relative_url}})

