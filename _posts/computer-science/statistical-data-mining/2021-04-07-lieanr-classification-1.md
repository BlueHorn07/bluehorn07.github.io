---
title: "Linear Classification - 1"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- Classification by Linear Regression
  - Binray Classification
  - Multi-class Classification
- Probabilistic Model
- Linear Discriminant Analysis
- Quadratic Discriminant Analysis

<hr/>

## Classficiation by Linear Regression

### Binray Classification

Assume that $\mathcal{Y}= \\{ 0, 1 \\}$, and consider the \<linear regression\> model

$$
Y = X^T \beta + \epsilon
$$

For some estimator $\hat{\beta}$, and given $X=x$, predictor $Y$ do something like this

$$
\hat{Y} = \begin{cases}
  \quad 1 & \text{if } x^{T}\hat{\beta} > 0.5 \\
  \quad 0 & \text{if } x^{T}\hat{\beta} \le 0.5
\end{cases}
$$

ì´ë•Œ, $x^T \hat{\beta}$ëŠ” $Y=1$ì— ëŒ€í•œ í™•ë¥ ì„ ë±‰ëŠ” í•¨ìˆ˜ì˜ ì¼ì¢…ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

$$
\hat{f}(x) = \text{Pr}(Y = 1 \mid X = x) = E(Y \mid X = x)
$$

ì´ë ¨ í˜•íƒœì˜ ê° Classì— ëŒ€í•œ posterior probabilityë¥¼ ê³„ì‚°í•´ Classifyë¥¼ ì§„í–‰í•˜ëŠ” ëª¨ë¸ì„ \<**Bayes Classifier**\>ë¼ê³  í†µì¹­í•œë‹¤.

### Multi-class Classification

Assume that $\mathcal{Y}=\\{1, \dots, K\\}$, and let $\mathbf{Y}$ be the $n \times K$ binray matrix where the $(i, k)$ element is $1$ if $y_i = k$ and $0$ otherwise.

Let

$$
\hat{\mathbf{B}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{Y}
$$

(deriven from RSS estimator!)

$$
\hat{\mathbf{Y}} = \mathbf{X} \hat{\mathbf{B}}
$$

ì´ë ‡ê²Œ ë‘”ë‹¤ë©´ inferenceì—ì„œëŠ”, For $X=x$, one may predict $Y$ as

$$
\hat{Y} = \underset{k\in\mathcal{Y}}{\text{argmax}} \; {\hat{f}_k (x)}
$$

where $(\hat{f}_1 (x), \dots, \hat{f}_K(x)) = x^T \hat{\mathbf{B}}$.

ì´ë•Œ, ê° $\hat{f}_i (x)$ëŠ” binray classificationì˜ $\hat{f}(x)$ì™€ ë¹„ìŠ·í•˜ê²Œ ì•„ë˜ì˜ ì¡°ê±´ë¶€ í™•ë¥  ë˜ëŠ” poster-priorityë¥¼ ì˜ˆì¸¡í•˜ëŠ” estimatorì˜ ì—­í• ì„ í•œë‹¤.

$$
\hat{f}_k (x) = \text{Pr}(Y=k \mid X=x)
$$

<br/>

<span class="statement-title">Problem.</span><br>


(ì¶”í›„ì— ê¸°ìˆ )

<hr/>

## Probabilistic Model

ìœ„ì˜ ë‘ ê²½ìš°ì—ì„œ ì‚´í´ë´¤ë“¯, Linear Regressionì— ì˜í•œ ì ‘ê·¼ì€ ê²°êµ­ ì•„ë˜ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ estimate í•˜ëŠ” ì ‘ê·¼ì´ì—ˆë‹¤.

$$
\text{Pr}(Y=k \mid X=x)
$$

ì´ë•Œ, ì´ ì¡°ê±´ë¶€ í™•ë¥ ì€ posterior probabilityë¡œ \<ë² ì´ì¦ˆ ì •ë¦¬\>ì— ì˜í•´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\text{Pr}(Y=k \mid X=x) = \frac{\pi_k \cdot f_k(x)}{\displaystyle \sum^K_{i=1} \pi_i \cdot f_i (x)}
$$

ì´ë•Œ, ìš°ë³€ì˜ ê° í•­ëª©ë“¤ì„ ì‚´í´ë³´ë©´

- $\pi_k$: the \<**class probability**\> of class $k$; prior probability

$$
\pi_k = P(y=k)
$$

- $f_k (x)$: the \<**class-conditional density**\> of $X$ in class $Y=k$

$$
f_k (x) = P(X=x \mid Y=k)
$$

ë§Œì•½ ìš°ë¦¬ê°€ $\pi_k$, $f_k (x)$ë¥¼ ëª¨ë‘ ì•Œê³  ìˆë‹¤ë©´, ìš°ë¦¬ëŠ” $X=x$ê°€ ìˆì„ ë•Œ, ì–´ë–¤ $Y=k$ê°€ ì„ íƒë˜ì–´ì•¼ í•˜ëŠ”ì§€ estimate í•  ìˆ˜ ìˆë‹¤. ë‹¨, $\pi_k$, $f_k(x)$ëŠ” ëª¨ë‘ ìš°ë¦¬ê°€ ì¶”ì •í•´ì•¼ í•˜ëŠ” ê°’ì´ë©°, ì´ê²ƒì„ ì¶”ì •í•˜ê¸° ìœ„í•´ ì–´ë–»ê²Œ ì ‘ê·¼í•˜ê³  ì–´ë–¤ ê°€ì •ì„ ì“°ëŠ”ì§€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ë‹¤.

## Linear Discriminant Analysis

LDAëŠ” $f_k$ê°€ ì•„ë˜ì™€ ê°™ì€ ì •ê·œ ë¶„í¬ $N(\mu_k, \Sigma_k)$ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤. [^1]

$$
f_k(x) = \frac{1}{\left| 2\pi \Sigma_k \right|^{1/2}} \exp \left[ - \frac{(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)}{2}\right]
$$

ì´ë•Œ, ë§Œì•½ ê° $\Sigma_k$ì— ëŒ€í•´

$$
\Sigma_k = \Sigma \quad \text{for all} \; k
$$

ë¼ëŠ” ì•„ì£¼ì•„ì£¼ íŠ¹ë³„í•œ ê°€ì •ì´ ì¶”ê°€ëœ ê²ƒì´ \<**LDA; Linear Discriminant Analysis**\>ë‹¤! ğŸ˜

<br/>

ìœ„ì™€ ê°™ì´ ì„¤ì •í–ˆì„ ë•Œì˜ \<decision boundary\>ë¥¼ ìƒê°í•´ë³´ì.

$$
P(Y=k \mid X=x) = P(Y=l \mid X=x)
$$

boundaryì— ëŒ€í•œ ìœ„ì˜ ì‹ì„ ì˜ ì •ë¦¬í•´ë³´ë©´,

$$
\begin{aligned}
\log \frac{P(Y=k \mid X=x)}{P(Y=l \mid X=x)} &= \log \frac{\pi_k \cdot f_k(x)}{\pi_l \cdot f_l(x)} = \log \frac{\pi_k}{\pi_l} + \log \frac{f_k(x)}{f_l(x)} \\
&= \log \frac{\pi_k}{\pi_l} + \log \left( \frac{\exp\left( -\frac{(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)}{2}\right)}{\exp\left( -\frac{(x-\mu_l)^T\Sigma^{-1}(x-\mu_l)}{2}\right)}\right) \\
&= \log \frac{\pi_k}{\pi_l} + \log \left( \exp \left( \frac{-(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)+(x-\mu_l)^T\Sigma^{-1}(x-\mu_l)}{2}\right) \right) \\
&= \log \frac{\pi_k}{\pi_l} + \left( \frac{-(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)+(x-\mu_l)^T\Sigma^{-1}(x-\mu_l)}{2}\right) \\
&= \left(\log \frac{\pi_k}{\pi_l} -\frac{1}{2} (\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l)\right) + x^T \Sigma^{-1} (\mu_k - \mu_l) \\
&= a + b^T x = 0
\end{aligned}
$$

ì¼ë¶€ ìŠ¤í…ì€ ê³¼ì •ì„ ìƒëµí•˜ê³  ê²°ê³¼ë§Œ ë°”ë¡œ ì ì—ˆëŠ”ë°, ìì„¸í•œ ê³¼ì •ì€ ì•„ë˜ì˜ í¼ì³ë³´ê¸°ì— ê¸°ìˆ í•´ë‘ê² ë‹¤.

<details class="math-statement" markdown="1">
<summary>í¼ì³ë³´ê¸°</summary>

(ì¶”í›„ ê¸°ìˆ )

</details>

ìœ„ì˜ ìŠ¤í…ì˜ ë§ˆì§€ë§‰ì— $a+b^Tx = 0$ë¼ëŠ” í‰ë©´ì‹ì´ ìœ ë„ ë˜ì—ˆëŠ”ë°, ê²°êµ­ ì´ $a+b^T x = 0$ì´ë¼ëŠ” hyper-plainì´ \<decision boundary\>ê°€ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤!!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/LDA-1.jpg" | relative_url }}" width="380px">
</div>

<br/>

<span class="statement-title">Classification.</span><br>

ì´ì œ LDAë¥¼ ì´ìš©í•´ classificationì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•˜ëŠ”ì§€ ë…¼í•´ë³´ì.

ë¨¼ì € $P(Y = k \mid X=x)$ì—ì„œ ë¶„ëª¨ê°€ ëª¨ë‘ ë™ì¼í•œ ê°’ì„ ê°€ì§ˆ í…Œë‹ˆ, ë¶„ìì¸ $\pi_k \cdot f_k(x)$ë¥¼ maximizeí•˜ë©´ ëœë‹¤! $f_k(x)$ì— ì§€ìˆ˜ì‹ì´ ìˆìœ¼ë‹ˆ ê³„ì‚°ì˜ í¸ì˜ë¥¼ ìœ„í•´ $\log$ë¥¼ ì”Œì›Œì£¼ì.

$$
\delta_k (x) = \log P(Y = k \mid X=x) = \log \pi_k + x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k
$$

ì´ë•Œ, $\log P(Y = k \mid X=x)$ë¥¼ \<**linear discriminant function**\> $\delta_k (x)$ë¼ê³  ë¶€ë¥¸ë‹¤. ìš°ë¦¬ëŠ” $\\{\delta_k(x)\\}_{k \in \mathcal{Y}}$ì— ëŒ€í•´ $\text{argmax}$ë¥¼ ì·¨í•´

$$
\hat{y} = \underset{k \in \mathcal{Y}}{\text{argmax}} \; \delta_k (x)
$$

ê°€ì¥ í° **discriminant function** $\delta_k(x)$ í•¨ìˆ˜ê°’ì„ ê°–ëŠ” í´ë˜ìŠ¤ë¡œ classificationì„ ì§„í–‰í•œë‹¤!

<br/>

<span class="statement-title">Parameter Estimation.</span><br>

ì•„ì§ ìš°ë¦¬ëŠ” $\pi_k$ì™€ $f_k(x)$ë¥¼ ì •í™•íˆ ì •ì˜í•˜ì§€ëŠ” ì•Šì•˜ë‹¤. $f_k(x)$ê°€ normal dist. $N(\mu_k, \Sigma)$ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •ì€ í–ˆì§€ë§Œ, $\mu_k$, $\Sigma$ì— ëŒ€í•œ ëª…í™•íˆ ì •ì˜í•˜ì§€ëŠ” ì•Šì•˜ì—ˆë‹¤. ì´ë²ˆ íŒŒíŠ¸ì—ì„œëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë“¤ì„ ì–´ë–»ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³¸ë‹¤.

Let $\displaystyle n_k = \sum^n_{i=1} I(y_i=k)$, then

$$
\hat{\pi}_k = \frac{n_k}{n} \quad \text{(sample proportion)}
$$

$$
\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i = k} x_i \quad \text{(sample mean)}
$$

$$
\hat{\Sigma} = \frac{1}{N-k} \sum^K_{k=1} \sum_{i: y_i=k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k)^T \quad \text{(pooled sample cov-var matrix)}
$$

<details class="math-statement" markdown="1">
<summary>í¼ì³ë³´ê¸°</summary>

1\. $\mu$ë¥¼ í™•ì‹¤íˆ ì•Œ ë•Œ

$$
\hat{\Sigma} = \frac{1}{N} \sum^K_{k=1} \sum_{i: y_i=k} (x_i - \mu_k) (x_i - \mu_k)^T
$$


2\. $\mu$ë¥¼ ëª¨ë¥¼ ë•Œ

$$
\hat{\Sigma} = \frac{1}{N-k} \sum^K_{k=1} \sum_{i: y_i=k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k)^T
$$

</details>

## Quadratic Discriminant Analysis

LDAì—ì„œëŠ” ê° class $k$ì˜ Cov-Var matrix $\Sigma_k$ê°€ ëª¨ë‘ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í–ˆë‹¤. ê·¸ëŸ°ë° ë§Œì•½ ì´ ê°€ì •ì„ ì œê±°í•œë‹¤ë©´, ìš°ë¦¬ëŠ” \<**QDA; Quadratic Discriminant Analysis**\>ë¥¼ í•˜ê²Œ ëœë‹¤.

$$
\delta_k (x) = \log \pi_k - \frac{1}{2} \log \left| \Sigma_k \right| - \frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)
$$

LDAì—ì„œì˜ $\delta_k(x)$ì™€ ë¹„êµí•´ë©´, QDAì˜ ê²½ìš°, cancel outì´ ëœ ë˜ê¸° ë•Œë¬¸ì— "2ì°¨ì‹"ì´ ë‚¨ê²Œ ëœë‹¤!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/QDA-1.jpg" | relative_url }}" width="400px">
  <p>
  (ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” QDAë¥¼, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” $X_i$ì— ëŒ€í•œ 2ì°¨ì‹($X_i^2, X_iX_j$)ì„ ë„£ê³  LDAë¥¼ ëŒë¦° ê²°ê³¼ë‹¤.)
  </p>
</div>

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” \<Logistic Regression\>ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤. ì¢€ë” ìµìˆ™í•œ ìš©ì–´ë¥¼ ì“°ìë©´, \<**MLE; Maximum Likelihood Estimation**\>ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤ëŠ” ë§ì´ë‹¤!

ğŸ‘‰ [Linear Classification - 2]({{"/2021/04/07/lieanr-classification-2.html" | relative_url}})



<hr/>

[^1]: ê°•ì¡°í•˜ì§€ë§Œ, ë°˜ë“œì‹œ ì´ë ‡ê²Œ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” ê±´ ì•„ë‹ˆë‹¤. $f_k(X)$ì˜ ë¶„í¬ê°€ $N(\mu_k, \Sigma_k)$ë¥¼ ë”°ë¥¸ë‹¤ê³  'ê°€ì •'í–ˆì„ ë¿ì´ë‹¤! ì‹¤ì œ $f_k(X)$ì˜ ë¶„í¬ëŠ” ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ í¬ë‹¤!