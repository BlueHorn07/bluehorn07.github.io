---
title: "Linear Regression - 1-1"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>



<hr/>

<span class="statement-title">Goal.</span><br>

Regressionì˜ ëª©í‘œëŠ” ì•„ë˜ì™€ ê°™ì€ \<regression function\>ì„ ì¶”ì •í•˜ëŠ” ê²ƒì— ìˆë‹¤.

$$
f(x) = E[Y \mid X = x]
$$

ìœ„ì˜ ê´€ê³„ì‹ì€ ì•„ë˜ì˜ ì‹ê³¼ ë™ì¹˜ë‹¤. ì¦‰, ìœ„ì˜ í•¨ìˆ˜ $f(x)$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‚˜ ì•„ë˜ì˜ $f(x)$ë¥¼ ì˜ ì°¾ìœ¼ë©´ \<regression\>ì˜ ëª©í‘œë¥¼ ì„±ì·¨í•œ ê²ƒìœ¼ë¡œ ë³¸ë‹¤.

$$
Y = f(x) + \epsilon, \quad E[\epsilon \mid X] = 0
$$

\<linear regression\>ì„ ë‹¬ì„±í•˜ê³  ì‹¶ë‹¤ë©´, \<regression function\> $f(x)$ë¥¼ ì°¾ê¸° ìœ„í•´ $X$, $Y$ì˜ ê´€ê³„ì‹ì„ ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ë§í•œë‹¤.

$$
\hat{Y} = \hat{\beta_0} + \sum^p_{j=1} \hat{\beta}_j X_j
$$

í‘œê¸°ì˜ í¸ì˜ë¥¼ ìœ„í•´ \<intercept\> ë˜ëŠ” \<bias\> í…€ì„ í¬í•¨í•´ ì•„ë˜ì™€ ê°™ì´ ê¸°ìˆ í•˜ê¸°ë¡œ í•œë‹¤.

$$
\hat{Y} = \sum^p_{j=0} \hat{\beta}_j X_j = X^T \hat{\beta}
$$

<hr/>

### Least Squared Estimator

\<Linear regression\>ì˜ í•´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ **RSS**ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
\text{RSS}(\beta) &= \sum^n_{i=1} \left( y_i - x_i^T \beta\right)^2 \\
    &= (\mathbf{y} - \mathbf{X}\beta)^T (\mathbf{y} - \mathbf{X}\beta)
\end{aligned}
$$

where $\mathbf{y} = (y_1, \dots, y_n)^T$ (response vector) and $\mathbf{X} = (\mathbf{x}_1, \dots, \mathbf{x}_p)$ (design matrix)

**RSS**ì— ëŒ€í•œ ì‹ì„ $\beta$ì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´ solutionì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ì •ë§ ë¯¸ë¶„ë§Œ ì˜ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ ìœ ë„ ê³¼ì •ì€ ì—¬ê¸°ì„œëŠ” ìƒëµí•œë‹¤.

$$
\hat{\beta} = \underset{\beta \in \mathbb{R}^p}{\text{argmin}} \; \text{RSS}(\beta) = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

ì´ê²ƒì„ ì•ì—ì„œ ì–¸ê¸‰í•œ $\hat{Y} = X^T \hat{\beta}$ì— ëŒ€ì…í•´ì£¼ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\hat{Y} = X^T \hat{\beta} = \mathbf{X}^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \left( \mathbf{X}^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \right) \mathbf{y} = \mathbf{H} \mathbf{y}
$$

ì´ë•Œì˜ $\mathbf{H}$ë¥¼ \<hat matrix\>ë¼ê³  ë¶€ë¥¸ë‹¤.

<hr/>

#### Design Matrix

\<design matrix\> $\mathbf{X}$ì—ëŠ” ë‘ ê°€ì§€ íƒ€ì…ì´ ìˆë‹¤.

(1) \<**Random Design**\>: $x_i$'s are regarded as i.i.d. realization

(2) \<**Fixed Design**\>: $x_i$'s are fixed (non-random)

ë‘ ê°œë…ì´ \<regression estimation\>ì—ëŠ” í° ì°¨ì´ê°€ ì—†ë‹¤ê³  í•œë‹¤. ìš°ë¦¬ëŠ” ì•ìœ¼ë¡œë„ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œ $\mathbf{X}$ë¥¼ \<fixed design\>ìœ¼ë¡œ ì·¨ê¸‰í•  ê²ƒì´ë‹¤.

<hr/>

ì•ì—ì„œ RSS ë°©ì‹ì„ ì‚¬ìš©í•´ $\hat{\beta}$ë¥¼ êµ¬í–ˆë‹¤. ì´ë•Œ, ì´ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë…¼í•˜ê¸° ìœ„í•´ \<prediction error\>ë¥¼ êµ¬í•´ì•¼ í•œë‹¤. ì´ë•Œ í•„ìš”í•œ ê°œë…ì´ \<**bias**\>ì™€ \<**variance**\>ì´ë‹¤. ì´ ë‘ ê°œë…ì— ë¬´ì—‡ì¸ì§€ëŠ” ë³„ë„ì˜ í¬ìŠ¤íŠ¸ì— ì •ë¦¬í•´ë‘ì—ˆë‹¤. ë§Œì•½ biasë„ ì‘ê³  varianceë„ ì‘ë‹¤ë©´, ìš°ë¦¬ëŠ” ê·¸ ëª¨ë¸ì´ ì¢‹ë‹¤ê³  í‰ê°€í•œë‹¤.<br/>
ğŸ‘‰ [bias & variance]({{"/2021/03/21/overview-of-supervised-learning-2.html#bias-variance-decomposition" | relative_url}})

$$
\text{Err}(x_0) = \sigma^2 + \left\{ \text{Bias}(\hat{f}(x_0)) \right\}^2 + \text{Var}(\hat{f}(x_0))
$$

$Y = X^T \beta + \epsilon$ë¼ê³  ê°€ì •í•˜ì.

ë§Œì•½, $\text{Var}(Y) = \text{Var}(\epsilon) = \sigma^2$ë¼ë©´,

$$
\begin{aligned}
\text{Var}(\hat{\beta}) &= \text{Var}\left( (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} \right) \\
&= \left((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \right) \text{Var}(\mathbf{y}) \left((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \right)^T \quad (\because \text{Var}(A\mathbf{x}) = A \text{Var}(\mathbf{x})A^T) \\
&= (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \cdot \text{Var}(\mathbf{y}) \cdot X (X^TX)^{-1} \\
&= (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \cdot \textcolor{red}{\sigma^2 I_n} \cdot X (X^TX)^{-1} \\
&= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T X (X^TX)^{-1} \\
&= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{aligned}
$$

ìœ„ì˜ ì‹ì—ì„œ $X^TX$ë¥¼ \<gram matrix\>ë¼ê³  í•œë‹¤.

ì´ë²ˆì—ëŠ” biasë¥¼ ì‚´í´ë³´ì. $\hat{\beta}$ì˜ í‰ê· ì¸ $E[\hat{\beta}]$ë¥¼ êµ¬í•´ë³´ì.

ë§Œì•½, $E[Y] = X^T \beta$ë¼ë©´,

$$
\begin{aligned}
E[\hat{\beta}] &= E\left[ (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{y} \right] = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T E [\mathbf{y}] \\
&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T (X \beta) = \beta
\end{aligned}
$$

<details class="math-statement" markdown="1">
<summary>$E[\mathbf{y}]$ ìœ ë„</summary>

$\mathbf{y} = (y_1, \dots, y_n)^T$ì— ëŒ€í•´ $E[\mathbf{y}]$ëŠ”

$$
E[\mathbf{y}] = \begin{pmatrix}
    E[y_1] \\
    \vdots \\
    E[y_n]
\end{pmatrix} = \begin{pmatrix}
    x_1^T \beta \\
    \vdots \\
    x_n^T \beta
\end{pmatrix} = \mathbf{X} \beta
$$

</details>

$E[\hat{\beta}] = \beta$ì´ê¸° ë•Œë¬¸ì— ***unbiased estimator***ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì˜ ì˜ë¯¸ëŠ” ì´ estimatorì˜ ì„±ëŠ¥ì´ í‰ê· ì ì¸ ê´€ì ì—ì„œëŠ” ì •ë§ ì˜ ì¶”ì •í•œë‹¤ëŠ” ë§ì´ë‹¤.

ì¢…í•©í•˜ë©´, LS estimatorëŠ” biasì˜ ê²½ìš° unbiasedì˜€ë‹¤. í•˜ì§€ë§Œ, varianceì˜ ê²½ìš° í–‰ë ¬ì˜ í˜•íƒœë¡œ ë‚˜ì™”ë‹¤. ì „ì²´ì˜ ê´€ì ì—ì„œ ë´¤ì„ ë•Œ, LS estimatorëŠ” ë¶„ì‚°ì´ í° í¸ì´ê¸° ë•Œë¬¸ì— ì•„ì£¼ ì¢‹ì€ estimatorëŠ” ì•„ë‹ˆë¼ê³  í•œë‹¤.

ì´ë²ˆì—ëŠ” estimatorì—ì„œ ì˜¤ì°¨ì— ëŒ€í•œ varianceì¸ $\sigma^2$ë„ ì¶”ì •í•´ë³´ì.

$$
\hat{\sigma} = \frac{1}{n} \sum^n_{i=1} (y_i - \hat{y_i})^2 = \frac{1}{n} \sum^n_{i=1} (y_i - x_i \hat{\beta})^2
$$

ê·¸ëŸ°ë° ì—¬ê¸°ì—ì„œ $n$ì´ ì•„ë‹ˆë¼ $n-p$ë¡œ ë‚˜ë‘ë„ë¡ í•œë‹¤.

$$
\hat{\sigma} = \frac{1}{n-p} \sum^n_{i=1} (y_i - x_i \hat{\beta})^2 = \frac{1}{n-p} \| \mathbf{y} - \hat{\mathbf{y}} \|
$$

ì´ë•Œ, $(n-p)$ëŠ” \<ììœ ë„\>ë¥¼ ì˜ë¯¸í•˜ëŠ”ë°, ì´ ë¶€ë¶„ì€ ì†”ì§íˆ ì•„ì§ ì˜ ëª¨ë¥´ëŠ” ë¶€ë¶„ì´ë¼ ìì„¸í•œ ì„¤ëª…ì€ ìƒëµí•œë‹¤.

ì¼ë‹¨ ë§Œì•½ ì €ë ‡ê²Œ $\sigma^2$ë¥¼ ì¶”ì •í•œë‹¤ë©´, ì´ê²ƒì´ ***unbiased estimaor*** ì„ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

$$
E[\hat{\sigma^2}] = \sigma^2
$$