---
title: "KNN & kernel method"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

ì´ í¬ìŠ¤íŠ¸ëŠ” [Regression Spline]({{"/2021/04/18/regression-spline.html" | relative_url}})ê³¼ [Spline Method (2)]({{"/2021/04/19/splines-method-2.html" | relative_url}})ì´ì–´ì§€ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤ ğŸ˜Š

<br><span class="statement-title">TOC.</span><br>

- [KNN; K-Nearest Neighbor method](#knn-k-nearest-neighbor-method)
- [kernel method](#kernel-method)
  - [Nadaraya-Watson Estimator](#nadaraya-watson-estimator)
- [Local Linear Regression with kernel method](#local-linear-regression-with-kernel-method)
- [Local Likelihood Appraoch for logistic regression](#local-likelihood-appraoch-for-logistic-regression)
- [Kernel method with $p>1$](#kernel-method-with-p--1)

<hr/>

### KNN; K-Nearest Neighbor method

Regression function $f(x) = E(Y \mid X=x)$ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ê·¼ì¤‘ í•˜ë‚˜ëŠ” ë°”ë¡œ $x$ì— ê°€ì¥ ê°€ê¹Œìš´ $k$ê°œì˜ ì ìœ¼ë¡œ í‰ê· ì„ ë‚´ëŠ” \<KNN\>ì´ë‹¤.

$$
\hat{f}(x) = \text{Avg} \, (y_i \, \mid \, x_i \in N_k(x))
$$

where $N_k(x)$ is the set of $k$ points nearest to $x$.

ì´ë ‡ê²Œ KNNì„ ì“¸ë•Œ ê¹”ë¦° ê°€ì •ì€ <span class="half_HL">"ì¶”ì •í•˜ë ¤ëŠ” í•¨ìˆ˜ $f(x)$ê°€ smooth í•˜ë‹¤"</span>ì´ë‹¤.

KNNì˜ parameterì¸ $k$ëŠ” model complexityë¥¼ ì»¨íŠ¸ë¡¤ í•œë‹¤.

- small $k$: model complexity â–², bias â–¼, variance â–² <small>// make ziggle model</small>
- Large $k$: model complexity â–¼, bias â–², variance â–¼ <small>// make smooth model</small>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/knn-1.png" | relative_url }}" width="500px">
</div>

[Problem ğŸ˜±] KNN $\hat{f}(x)$ is <span style="color:red">**not smooth**</span> and <span style="color:red">**not continuous**</span>!!

<br/>

KNNì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë„ì…í•˜ëŠ” ê²ƒì´ ë°”ë¡œ \<kernel method\>ë‹¤!

<hr/>

### Kernel Method

KNNì˜ ì‹ì„ ë‹¤ì‹œ ê¸°ìˆ í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\hat{f}(x) = \frac{\displaystyle \sum^n_{i=1} \, K_k (x, x_i) \cdot y_i}{\displaystyle \sum^n_{i=1} \, K_k (x, x_i) }
$$

where $K_k (x, x_i) = I(x_i \in N_k (x))$.

\<kernel method\>ì˜ ë©”ì¸ ì•„ì´ë””ì–´ëŠ” <span class="half_HL">KNNì—ì„œ $I(x_i \in N_k(x))$ë¥¼ ë‹¤ë¥¸ smooth function $K_{\lambda}(x, x')$ë¡œ ëŒ€ì²´í•œë‹¤</span>ëŠ” ê²ƒì´ë‹¤!! ì´ë•Œì˜ ê·¸ smooth function $K_{\lambda}(x, x')$ë¥¼ \<**kernel function**\>ì´ë¼ê³  í•œë‹¤!!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/kernel-method-1.png" | relative_url }}" width="550px">
</div>

ìœ„ì˜ ê·¸ë¦¼ì€ \<kernel function\> ì¤‘ í•˜ë‚˜ì¸ \<Epanechnikov kernel\>ì„ ì‹œê°í™”í•œ ê²ƒì´ë‹¤.

ì¦‰, \<kernel method\>ëŠ” equally weighting í•˜ëŠ” KNNê³¼ ë‹¬ë¦¬, kernelì— ë”°ë¼ ê°€ê¹Œì´ ìˆëŠ” pointì—ëŠ” ë” í° weightì„ ë¶€ì—¬í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.

<div class="light-margin" markdown="1">

Q. \<kernel method\>ë¥¼ ì ìš©í•œ ê·¸ë¦¼ì—ì„œ bounary ìª½ì„ ë³´ë©´, estimated curveê°€ ì•½ê°„ ì˜¬ë¼ê°€ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì€ ì™œ ë°œìƒí•˜ëŠ” ê²ƒì´ë©°, ì´ ë¬¸ì œëŠ” datasetì— ìƒê´€ì—†ì´ ì¼ì–´ë‚˜ëŠ” \<kernel method\>ì˜ ë³¸ì§ˆì ì¸ ë¬¸ì œì¸ê°€?

A. ì•„ë¬´ë¦¬ \<kernel method\>ë¼ê³  í•´ë„ boundary problemì—ì„œ ììœ ë¡œìš¸ ìˆœ ì—†ë‹¤. ì´ê²ƒì€ \<kernel method\>ì˜ ë³¸ì§ˆì ì¸ ë¬¸ì œë‹¤.

// ì¶”ê°€ë¡œ í•¨ìˆ˜ì˜ ê°€ìš´ë° ë¶€ë¶„ì—ì„œë„ ì¶”ì •ì´ ì •í™•í•˜ì§€ ì•Šì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤ ğŸ˜¥

</div>

<hr/>

#### Nadaraya-Watson Estimator

For a given kernel $K_\lambda$, \<Nadaraya-Watson Estimator\> is

$$
\hat{f}(x) = \frac{\displaystyle \sum^n_{i=1} K_{\lambda} (x, x_i) \cdot y_i}{\displaystyle\sum^n_{i=1} K_{\lambda} (x, x_i)}
$$

ì´ \<NW Estimator\>ì— ì‚¬ìš©ë˜ëŠ” ì»¤ë„ ì¤‘ ìœ ëª…í•œ ê²ƒì€ ëŒ€ë¶€ë¶„ ì•„ë˜ì˜ í˜•íƒœë¥¼ ì§€ë‹Œë‹¤.

$$
K_{\lambda} (x, x') = D \left( \frac{\left| x - x' \right|}{\lambda} \right)
$$

where $D$ is a non-negative decreasing on $[0, \infty)$.

$D$ëŠ” ëŒ€ì¶© ì¢…ëª¨ì–‘ì˜ ìš°í•¨ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. \<Epanechnikov kernel\>ê°€ ëŒ€í‘œì ì¸ ì˜ˆì‹œë‹¤.

<br>

<span class="statement-title">Examples.</span><br>

- Epanechnikov kernel

$$
D(t) = \begin{cases}
  \dfrac{3}{4} (1-t^2) & \text{if } \left| t \right| \le 1 \\
  \qquad 0 & \text{else}
\end{cases}
$$

- Tri-cube kernel

$$
D(t) = \begin{cases}
  \left( 1 - \left| t \right|^3 \right)^3 & \text{if } \left| t \right| \le 1 \\
  \qquad 0 & \text{else}
\end{cases}
$$

- Gaussian kernel

$$
D(t) = \dfrac{1}{\sqrt{2\pi}} \exp \left( - t^2 / 2\right)
$$

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/kernel-functions-1.jpg" | relative_url }}" width="550px">
</div>

<br/>

$\lambda$ëŠ” ì¼ì¢…ì˜ tunning parameterë¡œ, scaleì„ ì¡°ì •í•˜ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤; bandwidth (smoothing) parameter.

- small $\lambda$: model complexity â–², bias â–¼, variance â–² <small>// make ziggle model</small>
- Large $\lambda$: model complexity â–¼, bias â–², variance â–¼ <small>// make smooth model</small>

// KNNì˜ $k$ì²˜ëŸ¼ model complexityë¥¼ ê²°ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤!!

ğŸ’¥ Problem: \<Nadaraya-Watson Estimator\> has large bias around the boundary ğŸ˜¥

<hr/>

#### Local Linear Regression with kernel method

kernel methodë¥¼ ì“°ëŠ” \<Nadaraya-Watson Estimator\>ì—ì„  "curvurseë¥¼ ì œëŒ€ë¡œ catchí•˜ì§€ ëª» í•œë‹¤", "boundaryì—ì„œ biasê°€ í¬ë‹¤" ë“±ì˜ ë¬¸ì œê°€ ìˆì—ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì‚´í´ë³´ì!

ì§€ê¸ˆê¹Œì§€ì˜ \<kernel method\>ëŠ” ì‚¬ì‹¤ <span class="half_HL">weighted local constant fitting</span>ì„ í•œ ê²ƒì´ë‹¤. ì¦‰, "local constant fitting"ì„ í•˜ê¸´ í–ˆëŠ”ë°, kernel functionìœ¼ë¡œ weightingì„ í–ˆë‹¤ëŠ” ë§ì´ë‹¤. ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” constant fitting ëŒ€ì‹ ì— <span style="color:red">linear fitting</span> í•˜ëŠ” ë°©ë²•ì„ ë„ì…í•˜ê²Œ ëœë‹¤!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/weighted-local-linear-regression-1.jpg" | relative_url }}" width="500px">
</div>

ğŸ‘€ \<local linear regression\>ì´ \<NW estimator\> ë³´ë‹¤ boundaryì—ì„œ í›¨ì”¬ biasê°€ ì¤„ì–´ë“  ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤!! ğŸ‘€

Find a local linear estimator $\hat{f}(x) = \alpha(x) + \beta(x)\cdot x$ where

$$
\underset{\alpha(x), \, \beta(x)}{\text{argmin}} \; \sum^n_{i=1} \; K_{\lambda} (x, x_i) \, \left[ y_i - \alpha(x) - \beta(x) \cdot x_i \right]^2
$$

ìœ„ì˜ ì‹ì€ local êµ¬ê°„ ì¤‘ í•˜ë‚˜ì˜ êµ¬ê°„ì— ëŒ€í•œ estimated function $\hat{f}(x)$ì„ ê¸°ìˆ í•œ ê²ƒì´ë‹¤. ì´ê²ƒì„ ì „ì²´ êµ¬ê°„ìœ¼ë¡œ í™•ì¥í•´ì„œ í–‰ë ¬ì˜ í˜•íƒœë¡œ ê¸°ìˆ í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

Let $\mathbf{W}(x) = \text{diag} (K_\lambda (x, x_i))$, and $\mathbf{B}$ be $n\times 2$ with $i$-th row $b(x_i)^T$, where $b(x) = (1, x)^T$.

Then,

$$
\hat{f}(x) = b(x)^T \cdot \left( \mathbf{B}^T \mathbf{W}(x) \mathbf{B} \right)^{-1} \cdot \mathbf{B}^T \mathbf{W}(x) \mathbf{y}
$$

ì‹ì´ ë§ì´ ë³µì¡í•˜ë‹¤. ì¡°ê¸ˆ ê°„ë‹¨í•˜ê²Œ ë³´ë ¤ë©´, weight ë¶€ë¶„ì¸ $\mathbf{W}(x)$ë¥¼ ë¹¼ê³  ì‹ì„ ë¨¼ì € ì´í•´í•˜ëŠ”ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤.

<br/>

ì´ \<local linear regression\>ì˜ ì•„ì´ë””ì–´ë¥¼ í™•ì¥ì— \<local <u>polynomial</u> regression\>ìœ¼ë¡œ í™•ì¥í•´ë³¼ ìˆ˜ë„ ìˆë‹¤!!

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/weighted-local-quadratic-regression-1.jpg" | relative_url }}" width="500px">
</div>

ğŸ‘€ \<local quadratic regression\>ì´ \<local linear regression\>ë³´ë‹¤ curvertureê°€ ìˆëŠ” ë¶€ë¶„ì„ ë” ì˜ fitting í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ğŸ‘€

<div class="notice" markdown="1">

Q. ì§€ê¸ˆê¹Œì§€ì˜ ì‚´í´ë³¸ ê²ƒì„ ì¢…í•©í•´ë³´ë©´, \<local constant fit\> ë³´ë‹¨ \<local linear fit\>ì´ ì¢‹ì•˜ë‹¤. ë˜, \<local quadratic fit\>ì´ ë” ì¢‹ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´, polynomial fittingì˜ ì°¨ìˆ˜ê°€ ë†’ì„ ìˆ˜ë¡ ì¢‹ì€ ê²ƒ ì¼ê¹Œ?

A. NO!!!

ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ë‹¤!!! ì•„ë˜ì˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/bias-variance-tradeoff-1.jpg" | relative_url }}" width="450px">
</div>

ì´ ê²½ìš°, \<constant fit\>ì´ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. ë˜, boundaryì—ì„œ \<quadratic fit\>ì€ í° varianceì„ ë³´ì´ëŠ” ê±¸ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <small>// ìœ„ ê·¸ë¦¼ì—ì„  biasëŠ” ë˜‘ê°™ì€ë° varianceëŠ” quadraticì´ ê°€ì¥ ì»¸ë‹¤.</small>

ì´ê²ƒì€ ê²°êµ­ \<bias-variance tradeoff\>ë¡œ higher polynomialì€ biasë¥¼ ì˜ ë§ì¶œ ìˆ˜ ìˆì–´ë„ varianceê°€ ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.

ë”°ë¼ì„œ, higher polynomialë¡œ fitting í•˜ëŠ” ì „ëµì´ í•­ìƒ ìš°ì„¸í•˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ëŠ” ê²ƒì´ë‹¤.

ğŸ’¥ ë‹¹ì—°í•˜ê²Œë„ datasetì˜ true relationì˜ curvurtureê°€ ì‘ì„ ìˆ˜ë¡ linear fitì´ ì¼ë°˜ì ìœ¼ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.

</div>

<hr/>

### Local Likelihood Appraoch for logistic regression

ì´ë²ˆì—ëŠ” \<spline method\>ì—ì„œ ì‚´í´ë´¤ë˜ [\<non-parametric (binary) logistic regression\>]({{"/2021/04/19/splines-method-2.html#multi-dimensional-splines" | relative_url}}) ëª¨ë¸ì— \<kernel method\>ë¥¼ ì ìš©í•´ë³´ì.

\<logistic regreeion\>ì—ì„œëŠ” $f(x) = x^T \beta(x)$ë¥¼ êµ¬í–ˆì—ˆë‹¤. ì—¬ê¸°ì— \<kernel method\>ë¥¼ ì¶”ê°€í•˜ë©´, ì•„ë˜ì˜ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ $\hat{\beta}(x)$ë¥¼ êµ¬í•˜ëŠ” ë¬¸ì œê°€ ëœë‹¤.

$$
\hat{\beta}(x) = \underset{\beta}{\text{argmax}} \; \sum^n_{i=1} \; K_{\lambda}(x, x_i) \cdot \ell (y_i, x_i^T\beta)
$$

where $\ell(y, f(x)) = y \cdot f(x) - \log (1 + e^{f(x)})$.

<hr/>

### Kernel method with $p > 1$

ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ \<kernel method\>ëŠ” input variableì˜ ì°¨ì› $p$ê°€ 1ì¸ ê²½ìš° ì˜€ë‹¤. ì´ë²ˆì—ëŠ” $p$ê°€ 1 ì´ìƒì¸ ê²½ìš°, \<kernel method\>ì— ëŒ€í•œ ì‹ì„ ì‚´í´ë³´ê² ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ $p$ê°€ ì»¤ì§€ë©´, "neighborhood"ì˜ ê°œë…ì´ ì˜ ë™ì‘í•˜ì§€ ì•Šê²Œ ëœë‹¤. (curse of dimensionality) ê·¸ë˜ì„œ $p > 1$ì—ì„œëŠ” "neighbor"ë¥¼ ë‹¨ìˆœíˆ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¨ <span class="half_HL">ì–´ë–¤ structureë¥¼ ê°€ì •í•˜ê³  ì ‘ê·¼</span>í•œë‹¤.

$$
K_\lambda (x, x') = D \left( \frac{(x-x')^T A (x - x')}{\lambda} \right)
$$

ì´ëŸ° "structured assumption"ì˜ ì˜ˆì‹œë¡œëŠ”

- $f(X) = \alpha + \beta^T X$; linear structure
- $f$ is a super-smooth function
- $f$ can be represented by few basis functions
- Additive model: $f(X_1, \dots, X_p) = \alpha + f_1(X_1) + \cdots + f_p(X_p)$
- ...

ì´ ì¤‘ì—ì„œ ë§ˆì§€ë§‰ ì ‘ê·¼ì¸ \<Additive Model\>ì´ ë°”ë¡œ ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë‹¤ë£° ë‚´ìš©ì´ë©°, $p$ ì°¨ì› í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ì–´ë ¤ìš´ ë¬¸ì œë¥¼ 1ì°¨ì› í•¨ìˆ˜ $p$ê°œë¥¼ ì¶”ì •í•˜ëŠ” ë¬¸ì œë¡œ ê°€ì •í•´ í•´ê²°í•˜ëŠ” ì ‘ê·¼ë²•ì´ë‹¤.

ğŸ‘‰ [Additive Model]({{"/2021/05/17/additive-model.html" | relative_url}}) ğŸ”¥