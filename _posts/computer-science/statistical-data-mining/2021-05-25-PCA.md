---
title: "PCA"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---

2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

### Introduction to PCA

\<PCA; Principal Component Analysis\>ëŠ” **ì°¨ì›ì¶•ì†Œ**(dimensionality reduction)ì— ì£¼ìš”í•œ ê¸°ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í…Œí¬ë‹‰ì´ë‹¤.

<div class="statement" markdown="1" align="center">

âœ¨ Goal âœ¨<br/>
To find low-dimensional representation of input variables which <span class="half_HL">contains most information in data</span>

</div>

<div class="img-wrapper">
  <img src="https://i.stack.imgur.com/Q7HIP.gif" width="450px">
  <p markdown="1">Image from ['stackoverflow'](https://stats.stackexchange.com/a/140579)</p>
</div>

ì´ë•Œ, "contains most information"ì´ë€ ë¬´ìŠ¨ ì˜ë¯¸ì¼ê¹Œ? \<PCA\>ì—ì„œëŠ” ì´ê²ƒì„ <span class="half_HL">ë°ì´í„°ì˜ "ë¶„ì‚°(variance)"ì„ ìµœëŒ€í•œ ë³´ì¡´</span>í•˜ëŠ” ê²ƒì´ë¼ê³  ë§í•œë‹¤! ì´ë ‡ê²Œ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ë³´ì¡´í•˜ëŠ” ë°©í–¥ì„ \<**principal component direction**\>ë¼ê³  í•˜ë©°, ìš°ë¦¬ê°€ \<PCA\>ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì°¾ì•„ì•¼ í•˜ëŠ” ëŒ€ìƒì´ë‹¤! ğŸ˜

<hr/>

#### First Principal Component

For a design matrix $X = \left[ X_1, X_2, \dots, X_p \right]$, the first principal component $Z_1$ is the noramlized linear combination:

$$
Z_1 = \phi_{11} X_1 + \cdots + \phi_{1p}X_p
$$

that has the largest variance.

by normalization, $\sum^p_{j=1} \phi_{1j}^2 = 1$; normalizationì„ í–ˆê¸° ë•Œë¬¸ì—, principal componentì˜ directionì´ uniqueí•˜ê²Œ ì •ì˜ëœë‹¤.

The coefficient vector $\phi_1 = (\phi_{11}, \dots, \phi_{1p})^T$ is called the "**loading vector of the first principal component**".

<br/>

Q. Given a $n\times p$ design matrix $X$, how can we estimate $\phi_1$??

<div class="math-statement" markdown="1">

Let $X = (X_1, \dots, X_p)^T$, and w.o.l.g. let's assume that $E[X] = 0$ by standardization and $\text{Var}(X) = \Sigma$.

Then, for $Z_1 = \phi_{11}X_1 + \cdots + \phi_{1p}X_p$, we want to maximuze $\text{Var}(Z_1)$!

$$
\begin{aligned}
\text{maximize} \; \text{Var}(Z_1)
&= \underset{\phi_{11}, \dots, \phi_{1p}}{\text{maximize}} \; \text{Var}(\phi_{11}X_1 + \cdots + \phi_{1p}X_p) \\
&= \underset{\phi_{11}, \dots, \phi_{1p}}{\text{maximize}} \; \text{Var}(\phi_1^T X) \\
&= \underset{\phi_{11}, \dots, \phi_{1p}}{\text{maximize}} \; \phi_1^T \text{Var}(X) \phi_1 \\
&= \underset{\phi_{11}, \dots, \phi_{1p}}{\text{maximize}} \; \phi_1^T \Sigma \phi_1
\quad \text{s.t.} \quad \phi_{11}^2 + \cdots \phi_{1p}^2 = 1
\end{aligned}
$$

ì´ë•Œ, covariance matrix $\Sigma$ëŠ” symmetric matrixì´ê¸° ë•Œë¬¸ì— \<spectral decomposition\>ì´ ê°€ëŠ¥í•˜ë‹¤!

$$
\Sigma = UDU^T
$$

ë˜, covariance matrix $\Sigma$ëŠ” positive definite matrixì´ë‹¤.

$$
x^T \Sigma x \succ 0
$$

ì´ì— ë”°ë¼ ëª¨ë“  egiven valueê°€ postiiveë‹¤.

$$
\Sigma = UDU^T = d_1 u_1 u_1^T + \cdots + d_p u_p u_p^T
$$

w.o.l.g. $0 \le d_1 \le \cdots \le d_p$.

ì´ì œ covariance matrix $\Sigma$ì— ëŒ€í•´ì„  ì¶©ë¶„íˆ ì‚´í´ë´¤ê³ , ì´ $\Sigma$ë¡œ $\phi$ë¥¼ í‘œí˜„í•´ë³´ì. ìš°ë¦¬ëŠ” $\Sigma$ì˜ Orthogomal matrix $U$ë¥¼ ì‚¬ìš©í•´ ì•„ë˜ì™€ ê°™ì´ $\phi$ë¥¼ $U$-basis expansion í•  ìˆ˜ ìˆë‹¤.

$$
\phi = (u_1^T\phi) u_1 + \cdots (u_p^T \phi) u_p
= U \begin{pmatrix}
  u_1^T \phi \\
  \vdots \\
  u_p^T \phi
\end{pmatrix}
$$

ì´ë•Œ, $\phi^T \phi = 1$ì´ê¸° ë•Œë¬¸ì— $(u_1^T\phi)^2 + \cdots (u_p^T \phi)^2 = 1$ì´ë‹¤.

$$
\begin{aligned}
\phi^T \Sigma \phi
&= (u_1^T \phi, \, \dots, \, u_p^T \phi) \cancel{U^T} \cdot \cancel{U}D\cancel{U^T} \cdot \cancel{U} \begin{pmatrix}
  u_1^T \phi \\
  \vdots \\
  u_p^T \phi
\end{pmatrix} \\
&= (u_1^T \phi, \, \dots, \, u_p^T \phi) D \begin{pmatrix}
  u_1^T \phi \\
  \vdots \\
  u_p^T \phi
\end{pmatrix} \\
&= d_1 (u_1^T \phi)^2 + \cdots + d_p (u_p^T \phi)^2
\end{aligned}
$$

ì¦‰, maximum varianceë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” $\phi^T\Sigma \phi$ë¥¼ ìµœëŒ€ë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤!

ì´ë•Œ, covariance matrix $\Sigma$ì˜ eigen value $d_i$ëŠ” í†µì œí•  ìˆ˜ ìˆëŠ” ê°’ì´ ì•„ë‹ˆë‹¤! ëŒ€ì‹  <span class="half_HL">ê°€ì¥ í° eigen valueì´ $d_p$ì— ê³±í•´ì§€ëŠ” $(u_p^T \phi)^2$ì˜ ê°’ì€ ì¡°ì •í•  ìˆ˜ ìˆë‹¤</span>!!

ì´ë•Œ, $(u_p^T \phi)^2$ì˜ ê°’ì´ ê°€ì¥ ì»¤ì§€ëŠ” ê²½ìš°ëŠ” ë°”ë¡œ $\phi$ê°€ $u_p$ê°€ ë  ë•Œì´ë‹¤!! ($u_p^T u_p = 1$)

ì¦‰, the constraint maximization problem for $\phi_1$ì˜ solutionì€ <span class="half_HL">$u_p$, the eigen vector associated with the largest eigen value $d_p$ì´ë‹¤</span>!

</div>

ì•ì—ì„œ ì‚´í´ë³¸ ê³¼ì •ì„ ë‹¤ì‹œ formalí•˜ê²Œ ê¸°ìˆ í•´ë³´ì!

<br><span class="statement-title">Definition.</span> First Principal Component<br>

Since we are only considering variance, we may assume that each column of $X$ is centered. <small>(ì»¬ëŸ¼ í‰ê· ì´ 0)</small>

Let define $z_{i1}$ to be

$$
z_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{1p}x_{ip} = (X\phi_1)_i
$$

The each $z_{i1}$ are called "**the scores of the first principal component**".

Then, we can find an estimator $\hat{\phi}_1$ of $\phi_1$ by solving the following constraint maximizing problem.

$$
\underset{\phi_1}{\text{maximize}} \left\{ \frac{1}{n} \sum^n_{i=1} z_{i1}^2 \right\} \quad \text{subject to} \; \sum^p_{j=1} \phi_{1j}^2 = 1
$$

Equivalently, $\phi_1$ can be obtained by

$$
\underset{\phi_1}{\text{maximize}} \; \phi_1^T \hat{\Sigma} \phi_1 \quad \text{subject to} \; \sum^p_{j=1} \phi_{1j}^2 = 1
$$

where $\hat{\Sigma} = X^T X / n$, the sample covariance matrix.

Therefore, <span class="half_HL">$\hat{\phi_1}$ is the eigen vector of $\hat{\Sigma}$ associated with the largest eigen value!</span>

<hr/>

#### Second and more Principal Components

The \<**second principal component**\> $Z_2$ is <span class="half_HL">the normalized linear combination of $X_1, \dots, X_p$ that has maximal variance</span> out of all normalized linear combination that <span class="half_HL">are uncorrelated with $Z_1$</span>.

ì´ë•Œ, \<second principal component\>ëŠ” $Z_1$ì„ êµ¬í•  ë•Œì²˜ëŸ¼ ì•„ì£¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëƒ¥ second largest eigen valueì— ì—°ê´€ëœ eigen vectorê°€ $Z_2$ê°€ ëœë‹¤!! <small>(ì›ë˜ eigen vectorë¼ë¦¬ëŠ” ì„œë¡œ orthogonalí•˜ë‹¤.)</small>

$k$-th principal componentì— ëŒ€í•´ì„œë„ ê·¸ëƒ¥ $k$ë²ˆì§¸ eigen valueì˜ eigen vectorë¥¼ ì“°ë©´ ëœë‹¤!!

<hr/>

### PCA Metric

\<PCA\>ì—ì„œëŠ” \<PVE; Proportion of Variance Explained\> of the m-th principal componentë¼ëŠ” metricë¥¼ ì •ì˜í•´ ì‚¬ìš©í•œë‹¤.

$$
\frac{\sum^n_{i=1} z_{im}^2}{\sum^p_{j=1} \sum^n_{i=1} x_{ij}^2} = \frac{\sum^n_{i=1} z_{im}^2 / n}{\sum^p_{j=1} \sum^n_{i=1} x_{ij}^2 / n} = \frac{\text{Var}(Z_m)}{\sum^p_{j=1} \text{Var}(X_j)}
$$

ì´ë•Œ, \<PVE\> ê°’ì„ ë³´ê³ , ëª‡ê°œì˜ principal componentë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ê²°ì •í•œë‹¤!

<div class="img-wrapper">
  <img src="{{ "/images/computer-science/statistical-data-mining/PCA-2.png" | relative_url }}" width="400px">
</div>

<hr/>

### PCA: Another Interpretation

<hr/>

### Sparse Principal Components

<hr/>

#### references

- ['ratsgo'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/)