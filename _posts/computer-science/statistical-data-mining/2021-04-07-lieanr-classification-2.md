---
title: "Linear Classification - 2"
layout: post
tags: [applied_statistics]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- Binary Logistic Regression
  - MLE; Maximum Likelihood Estimation
- Multi-class Logistaic Regression

<hr/>

## Binary Logistic Regression

LDAì™€ QDAì—ì„œëŠ” $f_k(x)$, ì¦‰ "the conditional density of $X$ given $Y=k$"ë¥¼ ëª¨ë¸ë§í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ, \<**Logistic Regression**\> ëª¨ë¸ì€ Regression output $x^T \beta$ë¥¼ \<Logistic Function\> $\dfrac{e^x}{1 + e^x}$ì— ì ìš©í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤!! í•œë²ˆ ì‚´í´ë³´ì. ğŸ¤©

<span class="statement-title">Definition</span><br>

Assume that $\mathcal{Y} = \\{0, 1\\}$. Then the \<**logistic regression model**\> assumes

$$
P(Y=1 \mid X=x) = \frac{\exp (x^T \beta)}{1 + \exp (x^T \beta)}
$$

<br/>

<big><b>Q1. ì™œ Logistic "<u>Regression</u>" ì¸ê°€?</b></big>

í‰ë²”í•œ Linear Regression Modelì—ì„œ $P(Y = 1 \mid X = x)$ëŠ”

$$
P(Y = 1 \mid X=x) = x^T \beta
$$

ì˜ ê²°ê³¼ë¥¼ ë±‰ëŠ”ë‹¤. í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ëª¨ë¸ë§í•  ê²½ìš°, $x^T \beta$ì˜ ê°’ì´ í™•ë¥ ì˜ ì •ì˜ì¸ $[0, 1]$ ê°’ì„ ê°–ëŠ”ë‹¤ëŠ” ì¡°ê±´ì„ ì˜ ë§Œì¡±í•˜ì§€ ëª» í–ˆë‹¤. "The linear regression model vilostes that $x^T \beta \in [0, 1]$"

ê·¸ë˜ì„œ ì´ $[0, 1]$ ì‚¬ì´ì— ë“¤ì–´ì˜¨ë‹¤ëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ $x^T \beta$ì˜ ê°’ì— Transformationì„ ì ìš©í•œë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ê°€ ì´ë²ˆì— ì‚¬ìš©í•˜ëŠ” \<**logistic function**\>, ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ \<**sigmoid function**\>ì¸ ê²ƒì´ë‹¤.

$$
\text{sigmoid}(x) = \frac{e^x}{1+e^x}
$$

ì‚¬ì‹¤ \<sigmoid function\> ì™¸ì—ë„ \<Gaussian cdf\>ë‚˜ \<Gompertz function\>ì„ ì¨ë„ ëœë‹¤ê³  í•˜ë©°, ì´ ê²½ìš° ì¢€ë” íŠ¹ë³„í•œ ìƒí™©, ì˜ˆë¥¼ ë“¤ë©´ 'ë³´í—˜' ë“±ì˜ ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  í•œë‹¤.

<br/>

<big><b>Q2. ì™œ "<u>Logistic</u>" Regressionì¸ê°€?</b></big>

Linear Regressionê³¼ LDA/QDA ëª¨ë‘ classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì ì ˆí•œ **decision boundary**ë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤. 

$$
\left\{ x : \log \frac{P(Y=1\mid X=x)}{P(Y=0 \mid X = x)} \right\}
$$

ì´ë•Œ, íŠ¹ë³„í•˜ê²Œë„ **decision boundary**ê°€ "linear"í•œ ìƒí™©ì´ë¼ë©´, ì•„ë˜ì˜ ì‹ì„ ë§Œì¡±í•˜ë©° classificationì„ ìœ„í•œ **hyper-plain**ì´ ì •ì˜ëœë‹¤.

$$
\log \frac{P(Y=1 \mid X=x)}{P(Y=0 \mid X = x)} = x^T \beta
$$

ê·¸ë¦¬ê³  ìœ„ì˜ ì‹ì—ì„œ ë¡œê·¸ë¥¼ í’€ê³ , í™•ë¥ ì˜ ì„±ì§ˆì„ ì˜ ì´ìš©í•˜ë©´ ì•„ë˜ì˜ ì‹ì´ ìœ ë„ëœë‹¤.

$$
P(Y=1 \mid X= x) = \frac{\exp (x^T \beta)}{1 + \exp (x^T \beta)}
$$

ë†€ëê²Œë„ ì´ë•Œ ìœ ë„ëœ ì‹ì´ ë°”ë¡œ \<**Logistic Function**\>ì¸ ê²ƒì´ë‹¤!! ğŸ¤©

<hr/>

### MLE; Maximum Likelihood Estimation

\<**Logistic Regression**\> ëª¨ë¸ë„ ê²°êµ­ì€ Regressionì„ ìœ„í•´ $\beta$ ê°’ì„ ì¶”ì •í•´ì•¼ í•œë‹¤. ì´ê²ƒì€ \<MLE; Maximum Likelihood Estimation\>ì„ í†µí•´ ì¶”ì •í•œë‹¤. ê·¸ ê³¼ì •ì„ ì‚´í´ë³´ì.

ë¨¼ì € \<**Likelihood function**\> $L(\beta)$ë¥¼ ì •ì˜í•˜ì.

$$
L(\beta) = \prod^n_{i=1} P(Y = y_i \mid X = x_i)
$$

$L(\beta)$ê°€ ì™œ ì´ë ‡ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ ì‚´í´ë³´ì. 

<div class="img-wrapper">
  <img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic2.png" width="600px">
  <p>
  (ì¶œì²˜: <a href="https://angeloyeo.github.io/2020/07/17/MLE.html">ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸</a>)
  </p>
</div>

ìš°ì„ , $L(\beta)$ì—ì„œ $X=x_i$ì—ì„œì˜ í™•ë¥ ì„ ëª¨ë‘ ê³±í•˜ê³  ìˆë‹¤. ì´ë ‡ê²Œ ê³±í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” MLEì˜ ê°€ì •ì¸ <span class="half_HL">"ê° $x_i$ê°€ ëª¨ë‘ i.i.d.í•˜ë‹¤"</span>ì— ê¸°ë°˜í•œë‹¤. ë…ë¦½ì¸ ì‚¬ê±´ë“¤ì´ ë™ì‹œì— ë°œìƒí•˜ëŠ” ê²ƒì´ë¯€ë¡œ "í™•ë¥ ì˜ ê³±ë²•ì¹™"ì— ì˜í•´ ìœ„ì™€ ê°™ì´ $\prod$ ë¥¼ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.

ìœ„ì˜ $L(\beta)$ì˜ ì‹ì— \<logistic function\>ì„ ë„£ì–´ì„œ ì¡°ê¸ˆ í’€ì–´ì„œ ì¨ë³´ì.

$$
\begin{aligned}
L(\beta) &= \prod^n_{i=1} \left( \frac{\exp (x_i^T \beta)}{1 + \exp (x_i^T \beta)}\right)^{y_i} \left( \frac{1}{1 + \exp (x_i^T \beta)}\right)^{1-y_i} \\
&= \prod^n_{i=1} \left( \frac{\exp (y_i \cdot x_i^T \beta)}{1 + \exp( x_i^T \beta)}\right)
\end{aligned}
$$

ë”± ë´ë„ ì‹ì´ ì¢€ ë³µì¡í•˜ë‹¤. ê·¸ë˜ì„œ ê³„ì‚°ì˜ í¸ì˜ë¥¼ ìœ„í•´ $L(\beta)$ì— $\log$ë¥¼ ì·¨í•´ \<Log-Likelihood\> $\ell(\beta)$ë¥¼ ì‚¬ìš©í•˜ì!

$$
\begin{aligned}
\ell (\beta) &= \log \left( \prod^n_{i=1} \left( \frac{\exp (y_i \cdot x_i^T \beta)}{1 + \exp( x_i^T \beta)}\right) \right) \\
&= \sum^n_{i=1} \; \log \left( \frac{\exp (y_i \cdot x_i^T \beta)}{1 + \exp( x_i^T \beta)}\right) \\
&= \sum^n_{i=1} \; \left(  y_i \cdot x_i^T \beta - \log \left( 1 + \exp(x_i^T \beta)\right) \right)
\end{aligned}
$$

Productionìœ¼ë¡œ êµ¬ì„±ëœ ê¸°ì¡´ì˜ ì‹ì„ Summationìœ¼ë¡œ ë³€í™˜í–ˆê¸°ì— ì´ì „ë³´ë‹¤ëŠ” ë¶„ì„í•˜ê¸° í›¨ì”¬ ì‰¬ì›Œì¡Œì§€ë§Œ, ì—¬ì „íˆ $\ell (\beta)$ë¥¼ Maximizationí•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•˜ì§€ ì•Šë‹¤.

ê·¸ëŸ¬ë‚˜ $\ell (\beta)$ê°€ \<**concave function**\>ì´ë¼ëŠ” ì [^1]ì€ ìš°ë¦¬ê°€ <span class="half_HL">nemerical methodë¥¼ ì‚¬ìš©í•´ Maximizationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ</span>[^2]ì„ ë§í•œë‹¤!! ğŸ¤© ê·¸ë˜ì„œ \<**Newton-Raphson method**\> ë“±ì˜ Nemerical Approximationì˜ ë°©ë²•ì„ ì‚¬ìš©í•´ MLEë¥¼ ìˆ˜í–‰í•œ ëœë‹¤.

<hr/>

<span class="statement-title">Regularization.</span><br>

(ì•„ì§ ì˜ ì™€ë‹¿ì§ˆ ì•Šë„¤;; ê°•ì˜ í•œë²ˆ ë” ë“£ê³ , ì´ ë¶€ë¶„ êµì¬ ì½ê³  ì±„ìš¸ ê²ƒ)

<hr/>

### LDA vs. Logistic Regression

<table style="text-align: center; table-layout:fixed">
<th>LDA</th>
<th>Logistic Regression</th>
<tr>
    <td colspan="2">linear decision boundary</td>
</tr>
<tr>
    <td>Normal ë¶„í¬ ê°€ì • æœ‰</td>
    <td>Normal ë¶„í¬ ê°€ì • ç„¡</td>
</tr>
<tr>
    <td>joint distributionì¸ $P(Y, X)$ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê°’ì´ í•„ìš”</td>
    <td>$P(Y=1 \mid X = x)$ì— ëŒ€í•œ ê°’ë§Œ ìˆìœ¼ë©´ ì¶©ë¶„</td>
</tr>
<tr>
    <td>Logisticì— ë¹„í•´ ë” ë§ì€ 'ê°€ì •'ì´ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— Logisticê³¼ ë¹„êµí•´ applicabilityê°€ ë–¨ì–´ì§.</td>
    <td>LDAì™€ ë¹„êµí•´ categorical inputì„ ì“°ê¸° ì‰¬ì›€</td>
</tr>
</table>

<hr/>

## Multi-class Logistic Regression

Let $\mathcal{Y} = \\{ 1, \dots, K \\}$, and assume that

$$
P(Y = k \mid X = x) \propto \exp (x^T \beta_k)
$$

ì´ê²ƒì€ ê³§,

$$
P(Y = k \mid X = x) = \frac{\exp(x^T \beta_k)}{\displaystyle \sum^K_{i=1} \exp (x^T \beta_i)}
$$



## references

- ['ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸'ë‹˜ì˜ í¬ìŠ¤íŠ¸](https://angeloyeo.github.io/2020/07/17/MLE.html)

<hr/>

[^1]: $\ell (\beta)$ê°€ ì™œ concave functionì´ ë˜ëŠ”ì§€ëŠ” ë” ì¡°ì‚¬ë¥¼ í•œ í›„ì— ì¶”í›„ì— ê¸°ìˆ í•˜ê² ã„·.

[^2]: ê·¸ëŸ¬ë‚˜ ì–´ë–¤ íŠ¹ë³„í•œ ì¼€ì´ìŠ¤ì—ì„œëŠ” MLEì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤ê³  í•œë‹¤. ì´ê²ƒ ì—­ì‹œ ë” ì¡°ì‚¬ë¥¼ í•œ í›„ì— ì¶”í›„ì— ê¸°ìˆ í•˜ê² ë‹¤.