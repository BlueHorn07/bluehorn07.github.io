---
title: "Feature Selection Techniques"
toc: true
toc_sticky: true
categories: ["Applied Statsitcs"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

ìš°ë¦¬ëŠ” Featureì˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ \<Curse of Dimensionality\>ë¼ëŠ” ë¬¸ì œë¥¼ ê°–ê²Œ ëœë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” ë§ì€ ê¸°ë²•ë“¤ì´ ì œì‹œë˜ì—ˆê³ , ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ê·¸ ê¸°ë²• ì¤‘ ì „ì²´ featureì—ì„œ ëª‡ê°œë¥¼ ì„ íƒí•´ ì‚¬ìš©í•˜ëŠ” \<Feature Selection\>ì˜ ê¸°ë²•ë“¤ì— ëŒ€í•´ ì†Œê°œí•  ì˜ˆì •ì´ë‹¤ ğŸ˜

<span class="statement-title">TOC.</span><br>

- [Best Subset Selection](#best-subset-selection)
- [Forward Stepwise Selection](#forward-stepwise-selection)
- [Backward Stepwise Selection](#backward-stepwise-selection)
- [Mallow's $C_p$](#mallows-c_p)
- [AIC & BIC](#aic--bic)
- [Instability of Variable Selection](#instability-of-variable-selection)

<hr/>

# Best Subset Selection

For given $k \le p$, choose $k$ input variables. This makes $\binom{n}{k}$ number of models. Find a model with which <span class="half_HL">the residual mean square error is minimized</span> among all models having $k$ input variables. Denote the model as $M_k$.

Select the optimal model among $M_0, M_1, \dots, M_p$. For $M_p$, this means we use all the input variables.

ğŸ’¥ ì´ë•Œ, ì–´ë–¤ ëª¨ë¸ì´ ì¢‹ì€ì§€ëŠ” Trainin Errê°€ ì•„ë‹ˆë¼ Test Errë¥¼ ë´ì•¼ í•œë‹¤!

<div class="img-wrapper" style="display:flex; justify-content:center; align-items:center;">

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/best-subset-selection-1.png" | relative_url }}" width="450px">
</div>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/best-subset-selection-2.png" | relative_url }}" width="320px">
  <p>
    Prostate cancer dataset
  </p>
</div>

</div>

\<Best subset Selection\>ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´, ë” ì ì€ featureë¥¼ ì‚¬ìš©í–ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  weightëŠ” ë¹„ìŠ·í•˜ê²Œ ë‚˜ì™”ê³ , ë˜ ì˜ˆì¸¡ ì„±ëŠ¥ ì—­ì‹œ ì „ì²´ featureë¥¼ ì“°ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ì™”ë‹¤.

ë‹¹ì—°í•˜ê²Œë„ ë” ì ì€ featureë¥¼ ì¼ìœ¼ë‹ˆ ê³„ì‚° ì¸¡ë©´ì—ì„œë„ ì´ë“! ğŸ˜

<br/>

However, if $p$ is large, say $\ge 40$, it becomes computationally infeasible.

$$
\binom{p}{k} = O(p^k)
$$

ì´ëŸ° ê³„ì‚°ìƒì˜ ì´ìŠˆ ë•Œë¬¸ì— \<Best Subset Selection\> ëŒ€ì‹  \<Forward & Backward Selection\> ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤.

<hr/>

# Forward Stepwise Selection

Start with the model $M_0$ containing the intercept only.

Construct a sequence of models $M_1, \dots, M_p$ by sequentially adding the predictor that most improves the fit .

Choose the best model among $M_0, \dots, M_p$.

ì¦‰, input variableì„ í•˜ë‚˜ ì¶”ê°€í•  ë•Œ, Test Errê°€ ê°€ì¥ í¬ê²Œ ë‚®ì•„ì§€ëŠ” ë…€ì„ì„ í•˜ë‚˜ ë„£ê² ë‹¤ëŠ” ë§ì´ë‹¤!

<hr/>

# Backward Stepwise Selection

Start with the full model $M_p$.

Construct a sequence of models $M_{p-1}, \dots, M_0$ by sequentially deleting the predictor that has the least impact on the fit.

Choose the best model among $M_p, \dots, M_0$.

ì¦‰, input variableì„ í•˜ë‚˜ ì œê±°í•  ë•Œ, Test Errê°€ ì œì¼ ì ê²Œ ë‚˜ì˜¤ëŠ” ë…€ì„ì„ ëº€ë‹¤ëŠ” ë§ì´ë‹¤!

<hr/>

# Mallow's $C_p$

\<Mallow's $C_p$\> ì´í•˜ $C_p$ëŠ” ì–´ë–¤ í†µê³„ ëª¨ë¸ $M$ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ë‹¤.

$$
C_p (M) = \frac{1}{n} \cdot \left( \sum^n_{i=1} (y_i - \hat{y})^2 + 2d \hat{\sigma}^2 \right)
$$

ì¦‰, $C_p$ëŠ” Test Errì™€ í•¨ê»˜ ëª¨ë¸ í”¼ì²˜ ìˆ˜ $d$ë¥¼ ê³ ë ¤í•œë‹¤ëŠ” ë§ì´ë‹¤!

ê·¸ë˜ì„œ ëª¨ë¸ì„ ì„ íƒí•  ë•Œ, <span style="color: red">$C_p$ê°€ ê°€ì¥ ë‚®ì€ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ëœë‹¤.</span>

<hr/>

# AIC & BIC

\<**AIC**; Akaike Information Criterion\>ê³¼ \<**BIC**; Bayesian Information Criterion\>ì€ ì¢€ë” generalí•œ model selection ì§€í‘œì´ë‹¤.

\<AIC\> & \<BIC\>ëŠ” \<MLE\> ê¸°ë²•ê³¼ë„ ê´€ë ¨ë˜ì–´ ìˆë‹¤.

$$
\text{AIC}(M) = - \frac{2}{n} \cdot \text{loglik} + \frac{2d}{n}
$$

ì´ë•Œ, $\text{loglik}$ëŠ” "log-likelihood"ì˜ ì•½ìë¡œ,\<AIC\>ê°€ MLEì— ì˜í•´ ìµœëŒ€í™”ëœ "log-likelihood" í…€ì— ë³€ìˆ˜ì˜ ê°¯ìˆ˜ $d$ì— ë”°ë¥¸ íŒ¨ë„í‹°ë¥¼ ì¶”ê°€í•œ ì§€í‘œë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

í”¼ì²˜ë¥¼ ë§ì´ ì“°ëŠ” ëª¨ë¸ì´ë¼ë©´($d$ê°€ í°) RSSëŠ” ì‘ì•„ì§€ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ AICì™€ BICëŠ” í”¼ì²˜ ìˆ˜ $d$ë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— <span style="color: red">AIC/BICê°€ ê°€ì¥ ì‘ì€ ëª¨ë¸ì„ ì“´ë‹¤</span>ëŠ” ê²ƒì€ ìš°ë„(likelihood)ë¥¼ ê°€ì¥ í¬ê²Œ í•˜ëŠ”(explainable) ë™ì‹œì— í”¼ì²˜ ê°¯ìˆ˜ëŠ” ê°€ì¥ ì ì€(parsimonious) ìµœì ì˜ ëª¨ë¸ì„ ì“´ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

$$
\text{BIC}(M) = - \frac{2}{n} \cdot \text{loglik} + \frac{d \cdot \log(n)}{n}
$$

\<AIC\>ì˜ ê²½ìš° \<Mallow's $C_p$\>ì™€ ë™ì¹˜ë¼ê³  í•˜ë©°, <span style="color: red">\<BIC\>ê°€ \<AIC\>ë³´ë‹¤ ë” ì‹¬í”Œí•œ ëª¨ë¸ì„ ì„ í˜¸í•œë‹¤</span>ê³  í•œë‹¤.

<hr/>

# Instability of Variable Selection

<div class="statement" markdown="1">

"Variable selection methods are known to be unstable."

\- Breiman, L. (1996)

</div>

"Unstable" means that small change of data results in large change of the estimator.

This is because variable selection uses hard decision rule (hard survivie or die rule).

\<Variable Selection\>ì˜ 'instability' ë¬¸ì œì— ëŒ€í•œ ëŒ€ì•ˆìœ¼ë¡œ ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ì†Œê°œí•  \<Shrinkage method\>ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

<hr/>

ğŸ‘‰ [Shrinkage Method]({{"/2021/05/16/shrinkage-method.html" | relative_url}})
