---
title: "Back-propagation"
toc: true
toc_sticky: true
categories: ["Artificial Intelligence"]
---


2020-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'ì¸ê³µì§€ëŠ¥' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<hr/>

### Backpropagation

\<Back-propagation\>ì€ \<GD\>ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Lossì˜ í¸ë¯¸ë¶„ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

$$
\nabla_\mathbf{w} \text{TrainLoss}(\mathbf{w})
$$

\<Back-propagation\> ì•Œê³ ë¦¬ì¦˜ì€ \<Neural Network\>ë¥¼ ì¼ì¢…ì˜ "**Computational Graph**"ë¡œ ëª¨ë¸ë§ í•˜ì—¬, Gradientë¥¼ êµ¬í•œë‹¤. GradientëŠ” \<**Chain Rule**\>ì— ì˜í•´ ì•ˆìª½ì˜ ì—°ì‚°ê¹Œì§€ ì°¨ë¡€ëŒ€ë¡œ ì „íŒŒëœë‹¤.

\<Back-propagation\>ì€ ë³µì¡í•œ í˜•íƒœì˜ Loss í•¨ìˆ˜ì¼ì§€ë¼ë„ ê·¸ê²ƒì„ ì‘ì€ ìš”ì†Œë¶€í„° ì°¨ë¡€ë¡œ \<Chain Rule\>ì— ë”°ë¼ Gradientì„ êµ¬í•˜ê¸° ë•Œë¬¸ì— ë³µì¡í•œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì‹ì„ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³ ë„ ì‰½ê²Œ Gradientë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤! ğŸ˜

<hr/>

### Function as boxes

ë¨¼ì € ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ \<Computational Graph\>ë¶€í„° ì‚´í´ë³´ì.

<div class="notice" markdown="1">

<span class="statement-title">Remark.</span> basic building blocks<br>

<div class="img-wrapper">
  <img src="{{ "/images/artificial-intelligence/back-propagation-1.png" | relative_url }}" width="500px">
</div>

1\. Addition $f(a, b) = a+b$

$$
\frac{\partial f(a, b)}{\partial a} = 1 \quad, \quad \frac{\partial f(a, b)}{\partial b} = 1
$$

<div class="light-margin"></div>

2\. Multiplication $f(a, b) = ab$

$$
\frac{\partial f(a, b)}{\partial a} = b \quad , \quad \frac{\partial f(a, b)}{\partial b} = a
$$

$a$, $b$ì˜ ìˆœì„œê°€ ë°”ë€Œì—ˆë‹¤!!

<div class="light-margin"></div>

3\. max function $f(a, b) = \max(a, b)$

$$
\frac{\partial f(a, b)}{\partial a} = I[a > b] \quad , \quad \frac{\partial f(a, b)}{\partial b} = I[a < b]
$$

ë‹¹ì—°íˆ $a > b$ì¼ ë•Œ, $f(a, b)$ê°€ $a$ì— ëŒ€í•œ identity func.ì´ ë˜ë¯€ë¡œ, ìœ„ì˜ ê²°ê³¼ëŠ” ë‹¹ì—°í•˜ë‹¤!

<div class="light-margin"></div>

4\. sigmoid function $f(a) = \sigma(a)$

$$
\frac{\partial f(a)}{\partial a} = \sigma(a) (1 - \sigma(a))
$$

</div>

<div class="notice" markdown="1">

<span class="statement-title">Remark.</span> generalized build block<br>

<div class="img-wrapper">
  <img src="{{ "/images/artificial-intelligence/back-propagation-3.png" | relative_url }}" width="180px">
</div>

\<Computational Graph\>ì˜ building blockì˜ í˜•íƒœë¥¼ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

</div>

<div class="notice" markdown="1">

<span class="statement-title">Remark.</span> composing functions<br>

<div style="display:flex; justify-content:center;">
  <div style="float:left; width:25%;">
    <img src="{{ "/images/artificial-intelligence/back-propagation-2.png" | relative_url }}">
  </div>
  <div style="margin-left: 20px" markdown="1">

  í•©ì„± í•¨ìˆ˜ì˜ ê²½ìš°ë„ \<back-propagation\>ìœ¼ë¡œ ì‰½ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‹¤!

  ì²« blockë¶€í„° ì‚´í´ë³´ìë©´, $\text{mid}$ë¥¼ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ ë³€ìˆ˜ë¡œ ì·¨ê¸‰í•˜ë©´, ì†ì‰½ê²Œ gradientë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

  $$
  \frac{\partial \text{out}}{\partial \text{mid}}
  $$

  ì—¬ê¸°ì—ì„œ ê·¸ ë‹¤ìŒ blockì„ ì‚´í´ë³´ë©´, ì´ê²ƒë„ ê·¸ëƒ¥ í¸ë¯¸ë¶„ì„ ìˆ˜í–‰í•´ì£¼ë©´ ë˜ëŠ” ê²ƒì´ë‹¤.

  $$
  \frac{\partial \text{mid}}{\partial \text{in}}
  $$

  ë‘ gradient ê°’ì„ ë‹¤ êµ¬í–ˆë‹¤ë©´, êµ¬í•œ gradientë¥¼ ëª¨ë‘ ê³±í•´ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” gradientë¥¼ êµ¬í•œë‹¤! ğŸ˜

  $$
  \frac{\text{out}}{\text{in}} = \frac{\partial \text{out}}{\partial \text{mid}} \cdot \frac{\partial \text{mid}}{\partial \text{in}}
  $$

  </div>
</div>

</div>

<div class="notice" markdown="1">

<span class="statement-title">Remark.</span> Branching Block<br>

<div class="img-wrapper">
  <img src="{{ "/images/artificial-intelligence/back-propagation-5.png" | relative_url }}" width="240px">
</div>

í•¨ìˆ˜ê°’ì´ ë‘ êµ°ë°ë¡œ ë¶„ê¸°í•˜ëŠ” ê²½ìš°ì˜ gradientëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$
\frac{\partial\text{out}}{\partial \text{mid}}
= \frac{\partial\text{out1}}{\partial \text{mid}} + \frac{\partial\text{out2}}{\partial \text{mid}}
$$

ì¦‰, ë¶„ê¸°í•˜ëŠ” ê²½ìš°ëŠ” gradient ê°’ì„ ë”í•´ì£¼ë©´ ëœë‹¤!

</div>

<hr/>

<div class="example" markdown="1">

<span class="statement-title">Example.</span> Binary Classification with Hinge Loss<br>

$$
\text{Loss}(x, y, \mathbf{w})
= \max \left\{\, 0, \; 1 - (\mathbf{w} \cdot \phi(x)) y \, \right\}
$$

âœ¨ Goal: $\dfrac{\partial \text{Loss}}{\partial \mathbf{w}}$

ì œì¼ ë¨¼ì € í•  ì¼ì€ \<Computational Graph\>ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ë‹¤ìŒì—ëŠ” Graphì˜ edgeì— ê·œì¹™ì— ë”°ë¼ ê³„ì‚°í•œ gradient ê°’ì„ í‘œì‹œí•œë‹¤. ë§ˆì§€ë§‰ì—ëŠ” gradientë¥¼ êµ¬í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒ, ì—¬ê¸°ì„œëŠ” $\mathbf{w}$ê¹Œì§€ Graphë¥¼ ë”°ë¼ê°€ë©° edgeì— í‘œì‹œëœ gradientë¥¼ ëª¨ë‘ ê³±í•´ì¤€ë‹¤! ğŸ˜

<div class="img-wrapper">
  <img src="{{ "/images/artificial-intelligence/back-propagation-4.png" | relative_url }}" width="200px">
</div>

ë”°ë¼ì„œ gradientëŠ”

$$
\frac{\partial \text{Loss}}{\partial \mathbf{w}}
= - 1 \cdot I(\text{margin} < 1) \cdot \phi(x) \cdot y
$$

</div>

