---
title: "Momemtum Generating Function"
toc: true
toc_sticky: true
categories: ["Probability"]
---

â€œí™•ë¥ ê³¼ í†µê³„(MATH230)â€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Probability and Statistics](https://bluehorn07.github.io/categories/probability-and-statistics)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ²
{: .notice--info}

<br><span class="statement-title">TOC.</span><br>

- [Moment Generating Function](#moment-generating-function)
  - [MGF Examples](#mgf-examples)
  - [Uniqueness Theorem for MGF](#uniqueness-theorem-for-mgf)
  - [MGF with Independence](#mgf-with-independence)

<hr/>

## Moment Generating Function

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> moment<br>

We call $E[X^k]$ as the \<$k$-th moment\> of $X$.

</div>

<span class="statement-title">Remark.</span><br>

1\. Moments can be infinite.

2\. Moments may not be defined.

í•˜ì§€ë§Œ, ì •ê·œ ìˆ˜ì—…ì—ì„œëŠ” ìœ„ì˜ ì‚¬ë¡€ë“¤ì„ ë‹¤ë£¨ì§€ëŠ” ì•ŠëŠ”ë‹¤!

<br/>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> MGF; Moment Generating Function<br>

The \<moment generating function\> of $X$ is given by

$$
M_X(t) := E[e^{tX}]
$$

</div>

Q. Why is this called the "moment" generating function?

A. Because it generates moments!

Note that

$$
\frac{d}{dt} e^{tX} = X e^{tX}
$$

and then,

$$
\begin{aligned}
\frac{d}{dt} M_X(t) &= \frac{d}{dt} E[e^{tX}] \\
        &= E \left[ \frac{d}{dt} e^{tX}\right] \\
        &= E \left[ X e^{tX}\right]
\end{aligned}
$$

ìœ„ì˜ ì‹ $\dfrac{d}{dt} M_X(t)$ì—ì„œ ìš°ë¦¬ê°€ $t=0$ì„ ëŒ€ì…í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ì†ì‰½ê²Œ momentë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤!

$$
\left. \frac{d}{dt} M_X(t) \right|_{t=0} = \left. E \left[ X e^{tX}\right] \right|_{t=0} = E[X]
$$

ë§ˆì°¬ê°€ì§€ë¡œ

$$
\frac{d^k}{dt^k} e^{tX} = \frac{d^{(k-1)}}{dt^{(k-1)}} X e^{tX} = X \frac{d^{(k-1)}}{dt^{(k-1)}} e^{tX} = \cdots = X^k e^{tX}
$$

ì¸ ì‚¬ì‹¤ì„ ì´ìš©í•˜ë©´, $k$-th moment $E[X^k]$ë„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤!!

$$
\left. \frac{d^k}{dt^k} M_X(t) \right|_{t=0} = \left. E \left[ X^k e^{tX}\right] \right|_{t=0} = E[X^k]
$$

<hr/>

### MGF Examples

<span class="statement-title">Example.</span><br>

Let $X \sim \text{BIN}(n, p)$, then find its MGF.

$$
\begin{aligned}
M_X(t) &= E[e^{tX}] \\
    &= \sum_k e^{tk} f(k) \\
    &= \sum^n_{k=0} e^{tk} \cdot \binom{n}{k} p^k q^{n-k} \\
    &= \sum^n_{k=0} \binom{n}{k} \cdot \left( p \cdot e^t \right)^k q^{n-k} \\
    &= \left( p \cdot e^t + q \right)^n
\end{aligned}
$$

ìœ„ì˜ MGFë¥¼ í†µí•´ ì§ì ‘ $E[X]$ë¥¼ êµ¬í•´ë³´ë©´,

$$
\left. \frac{d}{dt} M_X(t) \right|_{t=0} = \left. n \cdot p \cdot \left( p \cdot e^t + q \right)^{n-1} \right|_{t=0} = np
$$

<br/>

<span class="statement-title">Example.</span><br>

Let $X \sim \text{Poi}(\lambda)$, find its MGF.

$$
\begin{aligned}
M_X(t) &= E[e^{tX}] \\
    &= \sum^{\infty}_{k=0} e^{tk} \cdot e^{-\lambda} \frac{\lambda^{k}}{k!} \\
    &= e^{-\lambda} \cdot \sum^{\infty}_{k=0} \frac{\left(e^t \lambda \right)^k}{k!} \\
    &= e^{-\lambda} \cdot \exp \left( \lambda e^t\right) = \exp (\lambda(e^t - 1))
\end{aligned}
$$

ì´ê²Œ ìœ„ì˜ MGFë¥¼ ì´ìš©í•´ $E[X]$ë¥¼ êµ¬í•´ë³´ì!

$$
\begin{aligned}
\left. \frac{d}{dt} \exp (\lambda(e^t - 1)) \right|_{t=0} &= \left. \left( \frac{d}{dt} \lambda(e^t - 1) \right) \cdot \exp (\lambda(e^t - 1)) \right|_{t=0} \\
&= \left. \lambda \cdot \exp (\lambda(e^t - 1)) \right|_{t=0} \\
&= \lambda \cdot \exp (\lambda(1 - 1)) = \lambda \cdot 1 \\
&= \lambda
\end{aligned}
$$

<span class="statement-title">Example.</span><br>

Let $X \sim \text{NegBIN}(k, p)$, find its MGF.

$$
\begin{aligned}
M_X(t) &= \sum^{\infty}_{x=k} e^{tx} \cdot \binom{x-1}{k-1} p^{k} q^{x-k} \\
    &= \left(\frac{p}{q}\right)^k \cdot \sum^{\infty}_{x=k} \binom{x-1}{k-1} \left( e^t q \right)^{x} \\
\end{aligned}
$$

ìœ„ì˜ ì‹ì—ì„œ $y = x-k$ë¥¼ ëŒ€ì…í•˜ì.

$$
\begin{aligned}
\left(\frac{p}{q}\right)^k \cdot \sum^{\infty}_{x=k} \binom{x-1}{k-1} \left( e^t q \right)^{x} &=
\left(\frac{p}{q}\right)^k \cdot \sum^{\infty}_{y=0} \binom{y+k-1}{k-1} \left( e^t q \right)^{y+k} \\
&= \left(\frac{p}{q}\right)^k \cdot \left( e^t q \right)^k \sum^{\infty}_{y=0} \binom{y+k-1}{k-1} \left( e^t q \right)^y \\
&= (p \cdot e^t)^k \cdot \sum^{\infty}_{y=0} \binom{y+k-1}{k-1} \left( e^t q \right)^y
\end{aligned}
$$

ì´ë•Œ, $\displaystyle \binom{y+k-1}{k-1}$ì— ëŒ€í•´ ì•„ë˜ì˜ ì‹ì´ ì„±ë¦½í•œë‹¤.

$$
\binom{y+k-1}{k-1} = \left( -1 \right)^y \cdot \binom{-k}{y}
$$

ìœ„ì˜ ì‹ì„ ëŒ€ì…í•˜ë©´,

$$
\begin{aligned}
(p \cdot e^t)^k \cdot \sum^{\infty}_{y=0} \binom{y+k-1}{k-1} \left( e^t q \right)^y
&= (p \cdot e^t)^k \cdot \sum^{\infty}_{y=0}  \left( -1 \right)^y \cdot \binom{-k}{y} \left( e^t q \right)^y \\
&= (p \cdot e^t)^k \cdot \left( 1 - q\cdot e^t \right)^{-k} \\
&= \left( \frac{p \cdot e^t}{1 - q\cdot e^t} \right)^k \qquad \text{for} \quad 1 - q \cdot e^{t} > 0
\end{aligned}
$$

ë§Œì•½ $k=1$ì´ë¼ë©´, RV $X$ê°€ Geometric Distributionì„ ë”°ë¥´ê²Œ ë˜ë¯€ë¡œ, Geoì˜ MGFëŠ” ì•„ë˜ì™€ ê°™ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.

$$
M_X (t) = \frac{p \cdot e^t}{1 - q\cdot e^t} \qquad \text{for} \quad 1 - q \cdot e^{t} > 0
$$

<span class="statement-title">Example.</span><br>

Let $X \sim \text{Gamma}(\alpha, \beta)$, find its MGF.

ìš°ë¦¬ëŠ” ë…¼ì˜ì˜ í¸ì˜ë¥¼ ìœ„í•´ $Y \sim \text{Gamma}(\alpha, 1)$ë¥¼ ë¨¼ì € ì‚´í´ë³¼ ê²ƒì´ë‹¤. <small>($Y$ì— ëŒ€í•´ $\beta Y = X$ì´ê¸° ë•Œë¬¸!)
</small>

$$
\begin{aligned}
M_Y(t) &= E[e^{tY}] \\
    &= \int^{\infty}_0 e^{tx} \cdot \frac{1}{\Gamma(\alpha)} \cdot x^{\alpha-1} e^{-x} \; dx\\
    &= \frac{1}{\Gamma(\alpha)} \cdot \int^{\infty}_0 x^{\alpha-1}\cdot e^{-(1-t)x} \; dx \\
\end{aligned}
$$

ì´ë•Œ, ìœ„ì˜ ì‹ì—ì„œ $\beta = \dfrac{1}{1-t}$ë¡œ ë‘ë©´, ìœ ë„ ê³¼ì • ì¤‘ì— ê°ë§ˆ ë¶„í¬ì— ëŒ€í•œ ì ë¶„ì´ ìˆê¸° ë•Œë¬¸ì— ì†ì‰½ê²Œ ê³¼ì •ì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.

$$
\begin{aligned}
\frac{1}{\Gamma(\alpha)} \cdot \int^{\infty}_0 x^{\alpha-1}\cdot e^{-(1-t)x} \; dx
&= \frac{1}{\Gamma(\alpha)} \cdot \int^{\infty}_0 x^{\alpha-1}\cdot e^{-\frac{x}{1/(1-t)}} \; dx \\
&= \frac{1}{(1-t)^{\alpha}} \cdot \cancelto{1}{\int^{\infty}_0 \frac{1}{\Gamma(\alpha) \cdot \frac{1}{(1-t)^{\alpha}}} \cdot x^{\alpha-1} \cdot e^{-\frac{x}{1/(1-t)}} \; dx} \\
&= \frac{1}{(1-t)^{\alpha}} \qquad \text{for} \quad t < 1
\end{aligned}
$$

ì´ì œ, $X = \beta Y$ì˜ ê´€ê³„ì‹ì„ ì´ìš©í•´ $X$ì˜ MGFë¥¼ êµ¬í•˜ë©´

$$
\begin{aligned}
M_X(t) = E[e^{tX}] = E[e^{t\beta Y}] = \frac{1}{(1-\beta t)^{\alpha}} \qquad \text{for} \quad \beta t < 1 \quad (=t < \lambda)
\end{aligned}
$$

ì´ì œ ìœ„ì˜ ì‹ì„ ì´ìš©í•´ Exponential Distributionì˜ MGFë„ êµ¬í•  ìˆ˜ ìˆëŠ”ë°,

$$
M_X(t) = \frac{1}{1-\beta t} \qquad \text{for} \quad \beta t < 1 \quad (=t < \lambda)
$$

<span class="statement-title">Example.</span><br>

Let $Z \sim N(0, 1)$, then find its MGF.

$$
\begin{aligned}
M_Z (t) &= E[e^{tZ}] \\
    &= \int^{\infty}_{-\infty} e^{tx} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \; dx \\
    &= \int^{\infty}_{-\infty} \frac{1}{\sqrt{2\pi}} \cdot e^{tx} \cdot e^{-\frac{(x-t)^2 + 2xt - t^2}{2}} \; dx \\
    &= \int^{\infty}_{-\infty} \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{(x-t)^2}{2}} \cdot e^{tx} \cdot e^{-\frac{2xt-t^2}{2}} \; dx \\
    &= e^{t^2 / 2} \cdot \cancelto{1}{\int^{\infty}_{-\infty} \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{(x-t)^2}{2}}  \; dx} \\
    &= e^{t^2 / 2}
\end{aligned}
$$

ì´ì œ $X \sim N(\mu, \sigma^2)$ìœ¼ë¡œ ì¼ë°˜í™”í•˜ë©´, $X = \sigma Z + \mu$ì´ë¯€ë¡œ

$$
\begin{aligned}
M_X(t) &= E[e^{tX}] = E[e^{\sigma t z + \mu t}] \\
&= e^{\mu t} \cdot E[e^{\sigma t z}] \\
&= e^{\mu t} \cdot e^{\sigma^2t^2 / 2} \\
&= \exp \left( \mu t + \frac{\sigma^2 t^2}{2}\right)
\end{aligned}
$$

<hr/>

<span class="statement-title">Remark.</span><br>

If $X$ has the mgf $M_X(t)$, then $Y = aX + b$ has the mgf

$$
M_Y (t) = e^{bt} \cdot M_X(at)
$$

<hr/>

### Uniqueness Theorem for MGF

mgfëŠ” ë¯¸ë¶„ë§Œ í•˜ë©´ momentumì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ë„ ìˆì§€ë§Œ, \<Uniqueness Theroem\>ì´ë¼ëŠ” ì•„ë˜ì˜ ì •ë¦¬ì— ì˜í•´ ë‘ RVì´ ë™ì¼í•œ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ë³´ì¥í•˜ëŠ” ì¡°ê±´ì´ ë˜ê¸°ë„ í•œë‹¤.

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span> Uniqueness Theorem<br>

If $M_X(t) = M_Y(t)$ for all $t \in (-\delta, \delta)$ for some $\delta > 0$,

then $X$ and $Y$ have the same distribution.

</div>

ë”°ë¼ì„œ, ë‘ RVì´ ë™ì¼í•œ ë¶„í¬ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´, ë‘ RVì˜ mgfë¥¼ í™•ì¸í•´ë³´ë©´ ëœë‹¤!

<br/>

<span class="statement-title">Example.</span><br>

Q. Let $X$ be a random variable with $M_X(t) = \dfrac{1}{1-2t}$ for $t < \frac{1}{2}$. What is the distribution of $X$?

A. $X \sim \text{Exp}(\lambda = 2)$

<br/>

<span class="statement-title">Example.</span><br>

Q. How about $M_X(t) = \dfrac{1}{2} e^t + \dfrac{1}{2} e^{-t}$ for $t \in \mathbb{R}$?

A.

$$
\begin{aligned}
M_X(t) &= E[e^{tX}] \\
    &= \sum e^{tX} \cdot f(x) \\
    &= \sum_x e^{tX} \cdot P(X = x) \\
    &= e^{1\cdot x} \cdot P(X = 1) + e^{-1\cdot x} \cdot P(X = -1) \\
    &= e^{x} \cdot \frac{1}{2} + e^{-x} \cdot \frac{1}{2}
\end{aligned}
$$

ì´ë•Œ, ì•„ë˜ì™€ ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” RV $Y$ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì.

$$
f(y) := \begin{cases}
    1/2 & \text{if} \quad x \pm 1 \\
    0 & \text{else}
\end{cases}
$$

ì´ë•Œ, $Y$ì˜ mgfëŠ” $M_Y(t) = \dfrac{1}{2}e^t + \dfrac{1}{2}e^{-t}$ì´ë‹¤.

ìœ„ì—ì„œ ì–¸ê¸‰í•œ \<Uniqueness Theorem for MGF\>ì— ì˜í•´ $X$ì™€ $Y$ëŠ” ë™ì¼í•œ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤. $\blacksquare$

<br/>

ìŠ¤í¬ë¥¼ ì¡°ê¸ˆ í•˜ìë©´, \<Uniqueness Theorem of MGF\>ëŠ” ë‚˜ì¤‘ì— \<CLT; Central Limit Theorem\>ì„ ì¦ëª…í•  ë•Œ, ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©ëœë‹¤.

ğŸ‘‰ [*Proof* of CLT]({{"/2021/04/26/sampling-distribution-of-mean-and-clt#proof-of-clt" | relative_url}})

<hr/>

### MGF with Independence

If $X \perp Y$, then

$$
M_{X+Y} (t) = M_X(t) \cdot M_Y(t)
$$

In general, if $X_1, X_2, \dots, X_n$ are independent,, then

$$
M_{X_1 + \cdots + X_n}(t) = M_{X_1} (t) + \cdots + M_{X_n} (t) = \sum^n_{i=1} M_{X_i} (t)
$$

<br/>

<span class="statement-title">Example.</span> Tow Independent BIN<br>

Let $X \sim \text{BIN}(n, p)$ and $Y \sim \text{BIN}(m, p)$, and $X \perp Y$.

Then,

$$
X+Y \sim \text{BIN}(n+m, p)
$$

<div class="math-statement" markdown="1">

$$
\begin{aligned}
M_{X+Y}(t) &= M_X(t) \cdot M_Y(t) \quad (X \perp Y) \\
    &= (pe^t + q)^n \cdot (pe^t + q)^m \\
    &= (pe^t + q)^{n+m}
\end{aligned}
$$

ìœ„ì˜ mgfëŠ” ê³§ $\text{BIN}(n+m, p)$ì˜ mgfì™€ ë™ì¼í•˜ë‹¤. $\blacksquare$

</div>

<br/>

<span class="statement-title">Example.</span> Two Independent Poi<br>

Let $X \sim \text{Poi}(\lambda)$ and $Y \sim \text{Poi}(\mu)$, and $X \perp Y$.

Then,

$$
X+Y \sim \text{Poi}(\lambda + \mu)
$$

<div class="math-statement" markdown="1">

$$
\begin{aligned}
M_{X+Y}(t) &= M_X(t) \cdot M_Y(t) \quad (X \perp Y) \\
    &= \exp \left( \lambda (e^t - 1) \right) \cdot \exp \left( \mu (e^t - 1) \right) \\
    &= \exp \left( (\lambda + \mu) (e^t - 1) \right)
\end{aligned}
$$

ìœ„ì˜ mgfëŠ” ê³§ $\text{Poi}(\lambda + \mu)$ì˜ mgfì™€ ë™ì¼í•˜ë‹¤. $\blacksquare$

</div>

<br/>

<span class="statement-title">Example.</span> Two Independent NegBIN<br>

Let $X \sim \text{NegBIN}(r_1, p)$ and $Y \sim \text{NegBIN}(r_2, p)$, and $X \perp Y$.

Then,

$$
X+Y \sim \text{NegBIN}(r_1 + r_2, p)
$$


<br/>

<span class="statement-title">Example.</span> Two Independent Normal<br>

Let $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$, and $X \perp Y$.

Then,

$$
X+Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

<div class="math-statement" markdown="1">

$$
\begin{aligned}
M_{X+Y}(t) &= M_X(t) \cdot M_Y(t) \quad (X \perp Y) \\
    &= \exp \left( \mu_1 t + \frac{\sigma_1^2 t^2}{2} \right) \cdot \exp \left( \mu_2 t + \frac{\sigma_2^2 t^2}{2} \right) \\
    &= \exp \left( (\mu_1 + \mu_2) t + \frac{(\sigma^2 + \sigma_2^2) t^2}{2} \right)
\end{aligned}
$$

ìœ„ì˜ mgfëŠ” ê³§ $N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$ì˜ mgfì™€ ë™ì¼í•˜ë‹¤. $\blacksquare$

</div>

<br/>

<span class="statement-title">Example.</span> Two Independent Gamma<br>

Let $X \sim \text{Gamma}(\alpha_1, \beta)$ and $Y \sim \text{Gamma}(\alpha_2, \beta)$, and $X \perp Y$.

Then,

$$
X+Y \sim \text{Gamma}(\alpha_1 + \alpha_2, \beta)
$$

ìœ„ì˜ ì‚¬ì‹¤ì„ ì´ìš©í•˜ë©´, $\text{Exp}$ì™€ $\chi^2$ì— ëŒ€í•´ì„œë„ two independent ê²½ìš°ë¥¼ ë…¼í•  ìˆ˜ ìˆë‹¤!

1\. If $X \sim \text{Exp}(\beta)$, $Y \sim \text{Exp}(\beta)$ and $X \perp Y$. Then,

$$
X + Y \sim \text{Gamma}(2, \beta)
$$

â€» $\text{Exp}(\beta) = \text{Gamma}(1, \beta)$

<br/>

2\. If $X \sim \chi^2(n)$, $Y \sim \chi^2(m)$ and $X \perp Y$. Then,

$$
X + Y \sim \chi^2(n+m)
$$

â€» $\chi^2(n) = \text{Gamma}(n/2, 2)$

<br/>

<span class="statement-title">Example.</span><br>

Let $X \sim \text{Exp}(\lambda)$, $Y \sim \text{Exp}(\mu)$ and $X \perp Y$.

Then,

$$
Z = \min(X, Y) \sim \text{Exp}(\lambda + \mu)
$$

<hr/>

ì—¬ê¸°ê¹Œì§€ê°€ ì •ê·œìˆ˜ì—…ì˜ ì¤‘ê°„ê³ ì‚¬ ì‹œí—˜ ë²”ìœ„ì´ë‹¤. ê°œì¸ì ìœ¼ë¡œ ë…¼ë¦¬ë¥¼ ì „ê°œí•˜ëŠ” ë¶€ë¶„ì€ ì‹ì„ ì˜ ì •ë¦¬í•˜ê³ , ë¬¸ì œë¥¼ ì˜ ëª¨ë¸ë§í•˜ëŠ” ë¶€ë¶„ì„ ì¶©ë¶„íˆ ì—°ìŠµí•˜ë©´ ë  ê²ƒ ê°™ë‹¤. ë‹¤ë§Œ, ê° ë¶„í¬ì˜ ì •ì˜ì™€ í˜•íƒœê°€ ì¡°ê¸ˆì”© í—·ê°ˆë ¤ì„œ ì‹œí—˜ ì „ì— ëª¨ë“  ë¶„í¬ë¥¼ ë¹ ì§ì—†ì´ ë‹¤ ê¸°ìˆ í•  ìˆ˜ ìˆëŠ”ì§€ ë°±ì§€(ç™½ç´™)ì— ì²´í¬í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.
