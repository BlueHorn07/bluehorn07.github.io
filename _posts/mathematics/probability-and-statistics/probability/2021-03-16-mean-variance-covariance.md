---
title: "Mean, Variance, and Covariance"
toc: true
author: bluehorn_math
toc_sticky: true
categories: ["Probability"]
---

â€œí™•ë¥ ê³¼ í†µê³„(MATH230)â€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Probability and Statistics](https://bluehorn07.github.io/categories/probability-and-statistics)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ²
{: .notice--info}

## Mean

<br><span class="statement-title">Definition.</span><br>

The \<**expectation**\> or \<**mean**\> of a RV $X$ is defined as

$$
\mu := E[x] := \begin{cases}
    \displaystyle \sum_x x f(x) && X \; \text{is a discrete with pmf} f(x) \; \\
    \displaystyle \int^{\infty}_{\infty} x f(x) dx  && X \; \text{is a continuous with pdf} \; f(x)
\end{cases}
$$

ë§Œì•½ RV $X$ì— í•¨ìˆ˜ $g(x)$ë¥¼ ì·¨í•œë‹¤ë©´, \<Expectation\>ì€ ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.

<span class="statement-title">Theorem.</span><br>

Let $X$ be a random variable with probability distribution $f(x)$. The expected value of the random variable $g(X)$ is

$$
\mu_{g(X)} = E\left[g(X)\right] = \sum_x g(x) f(x) \quad \text{if } X \text{ is discrete RV}
$$

and

$$
\mu_{g(X)} = E\left[g(X)\right] = \int^{\infty}_{\infty} g(x) f(x) \quad \text{if } X \text{ is continuous RV}
$$

<small>($g(x)$ë¥¼ ì·¨í•˜ë„ ì—¬ì „íˆ $x$ì˜ ì •ì˜ì—­ì€ ìœ ì§€ë˜ë¯€ë¡œ, ìœ„ì™€ ê°™ì´ $g(x) f(x)$ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ íƒ€ë‹¹í•˜ë‹¤.)</small>

ps) ìˆ˜ì—… ì‹œê°„ì— êµìˆ˜ë‹˜ê»˜ì„œ ì´ì‚° RVì— ëŒ€í•œ ì¦ëª…ì€ ì‰½ê²Œ í•  ìˆ˜ ìˆì§€ë§Œ, ì—°ì† RVì— ëŒ€í•œ ì¦ëª…ì€ ì¢€ ê¹Œë‹¤ë¡­ë‹¤ê³  í•˜ì…¨ë‹¤.

<br/>

ì´ë²ˆì—ëŠ” joint distributionsì— ëŒ€í•œ \<Expectation\>ì„ ì‚´í´ë³´ì.

<span class="statement-title">Definition.</span><br>

Let $X$ and $Y$ be RVs with joint probability distribution $f(x, y)$. The expected value of the RV $g(X, Y)$ is

$$
\mu_{g(X, Y)} = E\left[g(X, Y)\right] = \sum_x \sum_y g(x, y) f(x, y) \quad \text{if } X \text{ and } Y \text{ is discrete RV}
$$

$$
\mu_{g(X, Y)} = E\left[g(X, Y)\right] = \int^{\infty}_{-\infty} \int^{\infty}_{-\infty} g(x, y) f(x, y) \; dx dy \quad \text{if } X \text{ and } Y \text{ is continuous RV}
$$

<br/>

Conditional Distributionì— ëŒ€í•´ì„œë„ \<Expectation\>ì„ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.

<span class="statement-title">Definition.</span><br>

$$
E\left[ X \mid Y = y \right] = \begin{cases}
    \displaystyle \sum_x x f(x \mid y) && X \; \text{is a discrete with joint pmf} f(x, y) \; \\
    \displaystyle \int^{\infty}_{\infty} x f(x \mid y) \; dx  && X \; \text{is a continuous with joint pdf} \; f(x, y)
\end{cases}
$$

### Linearity of Expectation

\<Expectation\>ì€ \<**Linearity**\>ë¼ëŠ” ì•„ì£¼ ì¢‹ì€ ì„±ì§ˆì„ ê°€ì§„ë‹¤.

<span class="statement-title">Theorem.</span><br>

Let $a, b \in \mathbb{R}$, then $E\left[aX + b\right] = aE[X] + b$.

ìœ„ì˜ ì •ë¦¬ê°€ ë§í•´ì£¼ëŠ” ê²ƒì€ \<Expectation\>ì´ **Linear Operator**ì„ì„ ë§í•´ì¤€ë‹¤!! ğŸ¤©

ì¢€ë” í™•ì¥í•´ì„œ ê¸°ìˆ í•´ë³´ë©´,

<span class="statement-title">Theorem.</span><br>

$$
E\left[g(X) + h(X)\right] = E\left[g(X)\right] + E\left[h(X)\right]
$$

<br><span class="statement-title">Theorem.</span><br>

$$
E\left[g(X, Y) + h(X, Y)\right] = E\left[g(X, Y)\right] + E\left[h(X, Y)\right]
$$

### Expectation with Independence

ë§Œì•½ ë‘ RV $X$, $Y$ê°€ ì„œë¡œ \<ë…ë¦½\>ì´ë¼ë©´, ë‘ RVì˜ ê³±ì— ëŒ€í•œ \<Expectation\>ì„ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.

<span class="statement-title">Theorem.</span><br>

If $X$ and $Y$ are independent, then

$$
E[XY] = E[X]E[Y]
$$

<hr/>

## Variance and Covariance

ë‘ RV $X$, $Y$ê°€ ë™ì¼í•œ í‰ê· ì„ ê°€ì§€ë”ë¼ë„; $E[X] = \mu = E[Y]$ RVì˜ ê°œë³„ ê°’ë“¤ì´ í‰ê·  $\mu$ë¡œë¶€í„° ë–¨ì–´ì ¸ ìˆëŠ” ì •ë„ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. \<ë¶„ì‚° Variance\>ëŠ” ì´ëŸ° í‰ê· ìœ¼ë¡œë¶€í„°ì˜ í¼ì§„ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•œë‹¤.

<br><span class="statement-title">Definition.</span><br>

The \<**variance**\> of a RV $X$ is defined as

$$
\text{Var}(X) = E[(X-\mu)^2]
$$

and $\sigma = \sqrt{\text{Var}(X)}$ is called the \<**standard deviation**\> of $X$.

ì•„ë˜ì˜ ê³µì‹ì„ ì‚¬ìš©í•˜ë©´, $\text{Var}(X)$ë¥¼ ì¢€ë” ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.

<br><span class="statement-title">Theorem.</span><br>

$$
\begin{aligned}
\text{Var}(X) &= E[(X-\mu)^2] = E\left[ X^2 - 2 \mu X + \mu^2 \right] \\
            &= E[X^2] - 2 \mu E[X]  + \mu^2 \\
            &= E[X^2] - 2 \mu \cdot \mu + \mu^2 \\
            &= E[X^2] - \mu^2 = E[X^2] - \left(E[X]\right)^2
\end{aligned}
$$

"ë¶„ì‚° = ì œí‰ - í‰ì œ", ê³ ë“±í•™êµ ë•Œ ë°°ìš´ ê³µì‹ì´ë‹¤!

<br/>

\<Expectation\>ì€ Linearityë¼ëŠ” ì¢‹ì€ ì„±ì§ˆì„ ê°€ì§€ê³  ìˆì—ˆë‹¤. \<ë¶„ì‚° Variance\>ì—ì„œëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì‚´í´ë³´ì.

<span class="statement-title">Theorem.</span><br>

For any $a, b \in \mathbb{R}$,

$$
\text{Var}(aX + b) = a^2 \text{Var}(X)
$$

<hr/>

## Covariance

\<ê³µë¶„ì‚° Covariance\>ëŠ” ë‘ RV ì‚¬ì´ì— ì–´ë–¤ \<ê´€ê³„ relation\>ì´ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•˜ëŠ” ì§€í‘œë‹¤. \<ê³µë¶„ì‚°\>ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.

<span class="statement-title">Definition.</span><br>

The \<**covariane**\> of $X$ and $Y$ is defined as

$$
\begin{aligned}
\sigma_{XY} := \text{Cov}(X, Y) &= E \left[ (X - \mu_X) (Y - \mu_Y) \right]  \\
                                &= E(XY) - E(X)E(Y)
\end{aligned}
$$

- $\text{Cov}(X, X) = \text{Var}(X)$
- $\text{Cov}(aX + b, Y) = a \cdot \text{Cov}(X, Y)$
- $\text{Cov}(X, c) = 0$

ì•ì—ì„œ ì‚´í´ë´¤ì„ ë•Œ, ë‘ RV $X$, $Y$ê°€ **ë…ë¦½**ì´ë¼ë©´, $E(XY) = E(X)E(Y)$ê°€ ë˜ì—ˆë‹¤. ë”°ë¼ì„œ ë‘ RVê°€ ë…ë¦½ì¼ ë•ŒëŠ” $\text{Cov}(X, Y) = 0$ì´ ëœë‹¤! ê·¸ëŸ¬ë‚˜ ì£¼ì˜í•  ì ì€ ëª…ì œì˜ ì—­(æ˜“)ì¸ <span class="half_HL">$\text{Cov}(X, Y) = 0$ì¼ ë•Œ, ë‘ RVê°€ í•­ìƒ ë…ë¦½ì„ì„ ë³´ì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤!</span>

\<Covariance\>ì€ ë‘ RVì˜ Linear Combinationì— ëŒ€í•œ ë¶„ì‚°ì„ êµ¬í•  ë•Œë„ ì‚¬ìš©í•œë‹¤.

Let $a, b, c \in \mathbb{R}$, then

$$
\text{Var}(aX + bY + c) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2 \text{Cov}(X, Y)
$$

ì¦ëª…ì€ $\text{Var}(aX + bY + c)$ì˜ ì˜ë¯¸ë¥¼ ê·¸ëŒ€ë¡œ ì „ê°œí•˜ë©´ ì‰½ê²Œ ìœ ë„í•  ìˆ˜ ìˆë‹¤.

$$
\text{Var}(aX + bY + c) = E\left[ \left( (X+Y) - (\mu_X + \mu_Y) \right)^2 \right]
$$

<hr/>

## Correlation

\<ê³µë¶„ì‚°\>ì„ ì¢€ë” ë³´ê¸° ì‰½ê²Œ Normalize í•œ ê²ƒì´ \<**Correlation**\>ì´ë‹¤.

<span class="statement-title">Definition.</span><br>

The \<**correlation**\> of $X$ and $Y$ is defined as

$$
\rho_{XY} := \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \sqrt{\text{Var}(Y)}}
$$

- if $\rho_{XY} > 0$, $X$ and $Y$ are positively correlated.
- if $\rho_{XY} < 0$, $X$ and $Y$ are negatively correlated.
- if $\rho_{XY} = 0$, $X$ and $Y$ are uncorrelated.

ë§Œì•½ ë‘ RVê°€ ì™„ë²½í•œ ì„ í˜•ì„±ì„ ë³´ì¸ë‹¤ë©´, $\rho_{XY}$ê°€ ì•„ë˜ì™€ ê°™ë‹¤.

- if $Y = aX + b$ for $a > 0$, then $\text{Corr}(X, Y) = 1$
- if $Y = aX + b$ for $a < 0$, then $\text{Corr}(X, Y) = -1$

ìœ„ì˜ ëª…ì œëŠ” ê·¸ ì—­ë„ ì„±ë¦½í•œë‹¤. ì¦ëª…ì€ ì•„ë˜ì˜ Exerciseì—ì„œ ì§„í–‰í•˜ê² ë‹¤.

\<Correlation\>ì€ $[-1, 1]$ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. ì´ëŠ” \<ì½”ì‹œ-ìŠˆë°”ë¥´íŠ¸ ë¶€ë“±ì‹\>ì„ í†µí•´ ìœ ë„í•  ìˆ˜ ìˆë‹¤!

<div class="math-statement" markdown="1">

**Cauchy-Schwarrtz inequality** :

$$
\left( \sum a_i b_i \right)^2 \le \sum a_i^2 \sum b_i^2
$$

Correlation ì‹ì„ ì˜ë¯¸ì— ë§¡ê²Œ í’€ì–´ì“°ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\begin{aligned}
\text{Corr}(X, Y) &= \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \sqrt{\text{Var}(Y)}} = \frac{E[(X-\mu_X)(Y - \mu_Y)]}{\sqrt{E[(X-\mu_X)^2]} \sqrt{E[(Y-\mu_Y)^2]}} \\
&= \frac{\sum (X-\mu_X)(Y - \mu_Y)}{\sqrt{\sum (X-\mu_X)^2} \sqrt{\sum (Y-\mu_Y)^2}}
\end{aligned}
$$

ì´ì œ ìœ„ì˜ ì‹ì„ ì œê³±í•´ì„œ ì‚´í´ë³´ë©´

$$
(\rho_{XY})^2 = \left( \frac{\sum (X-\mu_X)(Y - \mu_Y)}{\sqrt{\sum (X-\mu_X)^2} \sqrt{\sum (Y-\mu_Y)^2}} \right)^2 = \frac{\left( \sum (X-\mu_X)(Y - \mu_Y) \right)^2 }{\sum (X-\mu_X)^2 \sum (Y-\mu_Y)^2}
$$

\<ì½”ì‹œ-ìŠˆë°”ë¥´ì¸  ë¶€ë“±ì‹\>ì—ì„œ ìš°ë³€ì„ ì¢Œë³€ìœ¼ë¡œ ì´ë™í•˜ë©´, ì•„ë˜ì™€ ê°™ì€ ë¶€ë“±ì‹ì´ ì„±ë¦½í•œë‹¤.

$$
\frac{\left( \sum a_i b_i \right)^2}{\sum a_i^2 \sum b_i^2} \le 1
$$

ì´ë¥¼ \<Correlation\>ì˜ ì œê³±ì‹ì— ì ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
(\rho_{XY})^2 = \frac{\left( \sum (X-\mu_X)(Y - \mu_Y) \right)^2 }{\sum (X-\mu_X)^2 \sum (Y-\mu_Y)^2} \le 1
$$

ë”°ë¼ì„œ $(\rho_{XY})^2 \le 1$ì´ë¯€ë¡œ

$$
-1 \le \rho_{XY} \le 1
$$

$\blacksquare$

</div>

ì¶”ê°€ë¡œ \<Correlation\>ì€ "í‘œì¤€í™”"í•œ RVì˜ ê³µë¶„ì‚°ìœ¼ë¡œë„ í•´ì„í•  ìˆ˜ ìˆë‹¤.

$Z = \dfrac{X-\mu_X}{\sigma_X}$, $W = \dfrac{Y-\mu_Y}{\sigma_Y}$ë¼ê³  í‘œì¤€í™”í•œë‹¤ë©´, ì´ ë‘˜ì˜ ê³µë¶„ì‚°ì€ $X$, $Y$ì— ëŒ€í•œ Correlationê³¼ ê°™ë‹¤.

$$
\text{Var}(Z, W) = \text{Corr}(X, Y)
$$

ë”± ë³´ë©´ ì¦ëª… í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ì„œ ë”°ë¡œ ìœ ë„ëŠ” í•˜ì§€ ì•Šê² ë‹¤.

<hr/>

Q1. $\text{Var}(X) = 0$ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?

A1.

<br/>

Q2. $\text{Cov}(X, Y) = 0$ì´ì§€ë§Œ, ë‘ RVê°€ ë…ë¦½ì´ ì•„ë‹Œ ì˜ˆë¥¼ ì œì‹œí•˜ë¼.

<br/>

Q3. Prove that $-1 \le \text{Corr}(X, Y) \le 1$.

<br/>

Q4. Prove that if $\text{Corr}(X, Y) = 1$, then there exist $a>0$ and $b\in\mathbb{R}$ s.t. $Y = aX + b$.

<details class="math-statement" markdown="1">

<summary>í¼ì³ë³´ê¸°</summary>

A1. $p(x)$ê°€ delta-functionì„ì„ ì˜ë¯¸í•œë‹¤.

<hr/>

A2. $Y=X^2$ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì‰½ê²Œ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë…ë¦½ì„ì„ ë³´ì´ê¸° ìœ„í•´ $p(x, y)$ë¥¼ êµ¬í•´ì•¼ í•  ìˆ˜ë„ ìˆëŠ”ë°, ì´ê²ƒ ì—­ì‹œ ì ì ˆíˆ ì˜ ì„¤ì •í•´ì£¼ë©´ ì‰½ê²Œ reasonableí•˜ê²Œ ë””ìì¸ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

<hr/>

A3. & A4. Q3ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì¦ëª…ì„ í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œë„ ì¦ëª…í•  ìˆ˜ ìˆë‹¤! ğŸ‘‰ [ì´ê³³](http://people.math.gatech.edu/~ecroot/3225/rho_notes.pdf)ì˜ [2, 3]pë¥¼ ì°¸ê³ í•˜ë¼.

</details>

<hr/>

ì´ì–´ì§€ëŠ” ë‚´ìš©ì—ì„œëŠ” \<í‰ê· \>ê³¼ \<ë¶„ì‚°\>ì— ëŒ€í•œ ì•½ê°„ì˜ ì¶”ê°€ì ì¸ ë‚´ìš©ì„ ì‚´í´ë³¸ë‹¤.

ğŸ‘‰ [Chebyshev's Inequality]({{"/2021/03/17/chebyshev's-inequality" | relative_url}})

ê·¸ë¦¬ê³  Discrete RVì—ì„œì˜ ê¸°ë³¸ì ì¸ Probability Distributionì„ ì‚´í´ë³¸ë‹¤.

- Bernoulli Distribution
- Binomial Distributions
- Multinomial Distribution
- Hypergeometric Distributions
- etc...

ğŸ‘‰ [Discrete Probability Distributions - 1]({{"/2021/03/17/discrete-probability-distributions-1" | relative_url}})

ğŸ‘‰ [Discrete Probability Distributions - 2]({{"/2021/03/24/discrete-probability-distributions-2" | relative_url}})