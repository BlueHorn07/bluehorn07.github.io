---
title: "Point Estimation"
toc: true
author: bluehorn_math
toc_sticky: true
categories: ["Statistics"]
---

â€œí™•ë¥ ê³¼ í†µê³„(MATH230)â€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Probability and Statistics](/categories/probability-and-statistics)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ²
{: .notice--info}

## Introduction to Estimation

<div class="notice" markdown="1">

"\<**Statistics**\> is the area of science which can make inferences from data set."

</div>

<div class="notice" markdown="1">

"\<**Statistical Inference; í†µê³„ì  ì¶”ë¡ **\> means making generalization about the <u>population properties</u> <u>based on a random sample</u>."

</div>

Supp. someone gave you some data set $\\{ x_1, \dots, x_n \\}$ and it is known that this data set is taken from a normal random sample $X_i \sim N(\mu, 1)$.

Q. You are asked to estimate $\mu$. What can be a good estimate of $\mu$ from the sample?

A. $\bar{x}$, sample mean<br/>
why? by LLN, $\bar{x} \rightarrow \mu$ as $n \rightarrow \infty$.

<br/>

ìœ„ì˜ sample mean $\bar{x}$ ê°™ì´ ëª¨ì§‘ë‹¨(population)ì˜ ì„±ì§ˆì„ ì¶”ë¡ í•˜ëŠ” ê²ƒì„ \<**ì¶”ì •(Estimation)**\>ì´ë¼ê³  í•œë‹¤.

ì¶”ì •ì—ëŠ” \<Point Estimation\>ê³¼ \<Interval Estimation\>, 2ê°€ì§€ ë°©ì‹ì´ ì¡´ì¬í•œë‹¤.

population mean $\mu$ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ sample mean $\bar{x}$ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ \<Point Estimation\> ë°©ì‹ì´ë‹¤.

ë§Œì•½ "population mean $\mu$ëŠ” ì–´ë–¤ ë²”ìœ„(interval) $(a, b)$ì—ì„œ ë†’ì€ í™•ë¥ ë¡œ ì¡´ì¬í•  ê²ƒì´ë‹¤"ë¼ê³  interval $(a, b)$ì„ ì œì‹œí•˜ëŠ” ë°©ì‹ì„ \<Interval Estimation\>ì´ë¼ê³  í•œë‹¤.

ex) $P\left( \mu \in (a, b) \right) \approx 0.99 \quad \text{or} \quad 0.95$.

ğŸ’¥ Note: For true distribution $N(\mu, \sigma^2)$, $\mu$, $\sigma$ are <span class="half_HL">unknown, and not random</span>!!

<hr/>

## Point Estimation

Let $X_1, \dots, X_n$ be a random sample and $X_i \sim f(x; \theta)$ for some pdf(or pmf), and let $x_1, \dots, x_n$ be sample points.

A \<Point Estimation\> of some population parameter $\theta$ is <span class="half_HL">a single value $\hat{\theta}$ of statistic[^1] $\hat{\Theta}$</span>.

ì´ë•Œ, statistic $\hat{\Theta}$ë¥¼ estimatorë¼ê³  í•˜ë©°, <span class="half_HL">estimator $\hat{\Theta}$ëŠ” Random Variableì´ë‹¤</span>.

(hat $\hat{x}$ì´ ë¶™ìœ¼ë©´ random sampleë¡œë¶€í„° ìœ ë„ë˜ëŠ” ëŒ€ìƒì´ë‹¤.)

<br>

<span class="statement-title">Example.</span><br>

Let $X_1, X_2, \dots, X_n$ be a random sample taken from $N(\mu, \sigma^2)$.

Q1\. What can be a point estimator of $\mu$?

A1. sample mean, $\bar{X} = \dfrac{X_1 + \cdots + X_n}{n}$.

<br/>

Q2. How about a point estimator of $\sigma^2$?

A2. sample variance, $\displaystyle S^2 = \dfrac{1}{n-1} \sum^n_i (X_i - \bar{X})^2$ where $E[S^2] = \sigma^2$

or $\displaystyle \hat{S}^2 = \dfrac{1}{n} \sum^n_i (X_i - \bar{X})^2$ where $E[\hat{S}^2] = \dfrac{n-1}{n} \sigma^2$.

<div class="theorem"></div>

Q3. ë‘ estimator ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ê°€?

A3. ë‘ estimatorì˜ \<**bias**\>ë¥¼ ë¹„êµí•œë‹¤!

### Unbiased Estimator

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> unbiased estimator ğŸ”¥<br>

A statistic $\hat{\Theta}$ is called an \<**unbiased estimator**\> if

$$
E[\hat{\Theta}] = \theta \quad \text{for all} \quad \theta
$$

ì¦‰, <span class="half_HL">\<Estimator\>ì— í‰ê· ì„ ì·¨í–ˆì„ ë•Œ, population parameter $\theta$ê°€ ìœ ë„ë˜ëŠ” estimator</span>ë¥¼ ë§í•œë‹¤!

<div class="theorem"></div>

$E[\hat{\Theta} - \theta]$ is the "**bias**" of $\hat{\Theta}$ related to $\theta$.

ğŸ’¥ $E[\hat{\Theta} - \theta] = 0$, unbiased!

</div>


<span class="statement-title">Example.</span><br>

Let $X_1, X_2, \dots, X_n$ be a random sample taken from $N(\mu, \sigma^2)$.

Then, $\bar{X}$ is an unbiased estimator of $\mu$, and $S^2$ is an unbiased estimator of $\mu$.

Note that $E \left[ \frac{2X_1 + 0.5 X_2 + 0.5 X_3 + \cdots + X_n}{n}\right] = \mu$, so that one is also an unbiased estimator!

(Generalization) Let's consider a weigted average $\displaystyle\bar{X}_w = \sum^n_i w_i X_i$. This estimator is also an unbiased estimator.

$$
E\left[ \bar{X}_w \right] = \sum^n_i w_i E[X_i] = \cancelto{1}{\left( \sum^n_i w_i \right)} \mu = \mu
$$

Q. Why we use $\bar{X}$ instead of $\bar{X}_w$ for an estimator of $\mu$?

A. Because the **"variance"** of $\bar{X}$ is less than $\bar{X}_w$!

$$
\text{Var}(\bar{X}) = E \left[ (\bar{X} - \mu)^2 \right] = \frac{\sigma^2}{n} \le \text{Var}(\bar{X}_w)
$$

### Variance of Estimator

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> variance of estimator ğŸ”¥<br>

For an estimator $\hat{\Theta}$, the variance of estimator is

$$
\text{Var}(\hat{\Theta}) = E \left[ (\hat{\Theta} - E[\hat{\Theta}])^2 \right]
$$

\* Varianceì˜ ì •ì˜ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥¸ë‹¤. ê·¸ëŸ¬ë‚˜ $\hat{\Theta}$ê°€ statistic, ì¦‰ function of random samples $\hat{\Theta} = f(X_1, ..., X_n)$ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œ ê³„ì‚°ì€ random sampleì˜ distribution $X_i \sim g(x; \mu, \sigma)$ë¥¼ í™œìš©í•˜ë©´ ëœë‹¤. $\text{Var}(\hat{\Theta}) = \text{Var}(g(X_1, ..., X_n))$

</div>

<br/>


<span class="statement-title">Claim.</span><br>

Among all weighted averages $\\{ \bar{X}_w : w = (w_1, \dots, w_n), \sum w_i = 1\\}$, $\bar{X}$ has the smallest variance.

<div class="proof" markdown="1">

We know that $\displaystyle\text{Var}(\bar{X}) = \frac{\sigma^2}{n}$.

$$
\begin{aligned}
\text{Var}(\bar{X}_w)
&= \text{Var}\left( \sum^n_i w_i X_i \right) \\
&= \sum^n_i w_i^2 \cdot \text{Var}(X_i) \\
&= \sigma^2 \cdot \sum^n_i w_i^2
\end{aligned}
$$

For $\sum w_i = 1$,

$$
0 \le \sum^n_i \left(w_i - \frac{1}{n}\right)^2 = \sum w_i^2 - \frac{2}{n} \sum w_i + n \cdot \frac{1}{n^2} = \sum w_i^2 - \frac{1}{n}
$$

ë”°ë¼ì„œ,

$$
\text{Var}(\bar{X}) = \frac{\sigma^2}{n} \le \sigma^2 \cdot \sum^n_i w_i^2 = \text{Var}(\bar{X}_w)
$$

$\blacksquare$

</div>

### The Most Efficient Estimator

"bias"ì™€ "variance"ë¥¼ ì¢…í•©í•´ ì–´ë–¤ estimatorê°€ ì¢‹ì€ estimatorì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> the most efficient estimator of $\theta$ ğŸ”¥<br>

Among all <u>unbiased estimators</u> of parameter $\theta$, the one <u>with the smallest variance</u> is called **\<the most efficient estimator of $\theta$\>**.

</div>

<br/>

<span class="statement-title">Remark.</span><br>

When $X_i$'s are iid $N(\mu, \sigma^2)$, it is known that $\bar{X}$ is the most efficient estimator of $\mu$.

<br/>

Q. ì™œ most efficient estimatorëŠ” unbiased estimator ì¤‘ì—ì„œ ê³ ë¥´ëŠ” ê±¸ê¹Œ? biased estimator ì¤‘ì—ì„œ varianceê°€ ê°€ì¥ ì‘ì€ê²Œ ìˆì„ ìˆ˜ë„ ìˆì§€ ì•Šì„ê¹Œ?

A. Yes, it is possible that <span class="half_HL">a biased estimator can have smaller variance</span> than an unbiased estimator.

<br/>

<span class="statement-title">Exercise.</span><br>

Let $X_1, \dots, X_n$ be iid $N(\mu, \sigma^2)$.

Let $\displaystyle S^2 := \frac{1}{n-1} \sum^n_i (X_i - \bar{X})^2$ and $\displaystyle \hat{S}^2 := \frac{1}{n} \sum^n_i (X_i - \bar{X})^2$

Show that $\text{Var}(S^2) > \text{Var}(\hat{S}^2)$.

(HomeworkğŸˆ)

### Mean Squared Error

\<MSE; Mean Squared Error\>ë¥¼ Point Estimatorì˜ í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤!

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> MSE; Mean Squared Error ğŸ”¥<br>

The \<MSE; Mean Squared Error\> of an estimator is defined as

$$
\text{MSE} := E \left[ \left( \hat{\Theta} - \theta \right)^2 \right]
$$

</div>

<div class="notice" markdown="1">

<span class="statement-title">Claim.</span><br/>

$$
\text{MSE}
:= E \left[ \left( \hat{\Theta} - \theta \right)^2 \right]
= \text{Var}(\hat{\Theta}) + \left[ \text{Bias} \right]^2
$$

where $\text{Bias} := E \left[ \hat{\Theta} - \theta \right]$.

</div>

<div class="proof" markdown="1">

<span class="statement-title">*Proof*.</span><br/>

(HomeworkğŸˆ) / [(Solution)]({{"/2021/06/06/statistics-ps1" | relative_url}})

</div>

ì¼ë‹¨ ìœ„ì˜ ëª…ì œëŠ” ì°¸ì´ë¼ê³  ë°›ì•„ë“¤ì´ê³ , ì´ ëª…ì œê°€ ì™œ ì¤‘ìš”í•œì§€ë¥¼ ì„¤ëª…í•´ë³´ê² ë‹¤.

Estimator $\hat{\Theta}$ê°€ statisticì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•˜ëŠ”ê°€? Estimator $\hat{\Theta}$ëŠ” random sample $X_i$ì˜ í•¨ìˆ˜ë¡œ í‘œí˜„ëœë‹¤.

$$
\hat{\Theta} = f(X_1, X_2, ..., X_n)
$$

ê·¸ë˜ì„œ ì´ $\hat{\Theta}$ì˜ mean, varianceëŠ” ëª¨ë‘ random sample $X_i$ì˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•´ ì•„ì£¼ ì‰½ê²Œ ìœ ë„í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [unbiased estimator](#unbiased-estimator) ë¬¸ë‹¨ì—ì„œ ë“¤ì—ˆë˜ sample mean $\bar{X}$ì˜ ì‚¬ë¡€ë¥¼ ë‹¤ì‹œ ë³´ë©´...

Let random sample $X_i$ is taken from $N(\mu, \sigma^2)$. Then, the $E(\bar{X})$ is

$$
E(\bar{X}) = E \left( \frac{\sum^n_i X_i}{n} \right) = \frac{1}{n} \sum^n_i E[X_i] = \frac{1}{n} \cdot n \mu = \mu
$$

ë§ˆì°¬ê°€ì§€ë¡œ Estimator $\hat{\Theta}$ì˜ ë¶„ì‚°ë„ random sampleì˜ ë¶„í¬ë¥¼ ì´ìš©í•´ ì‰½ê²Œ ìœ ë„í•  ìˆ˜ ìˆë‹¤.

ê·¸ëŸ°ë°, Estimatorì˜ MSEëŠ” ê·¸ë ‡ì§€ ì•Šë‹¤. meanê³¼ varianceì™€ëŠ” ë‹¬ë¦¬ random sampleì˜ ë¶„í¬ì—ì„œ ìœ ë„í•˜ëŠ” ë°©ë²•ì´ straight í•˜ê²Œ ë– ì˜¤ë¥´ì§€ ì•Šì„ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ìœ„ì˜ "MSEëŠ” Estimatorì˜ ë¶„ì‚°ê³¼ biasì˜ ì œê³±ì˜ í•©ì´ë‹¤"ë¼ëŠ” ëª…ì œë¥¼ í™œìš©í•´ Estimatorì˜ MSEë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ í›¨ì”¬í›¨ì”¬ ì‰½ë‹¤.

ë§Œì•½ ì´ëŸ° ë°°ê²½ì„ ëª¨ë¥´ê³ , MSEë¥¼ ë§ˆì£¼í•œë‹¤ë©´ ê½¤ í˜¼ë€ìŠ¤ëŸ½ë‹¤. ë³¸ì¸ì€ ë¨¸ì‹  ëŸ¬ë‹ì´ë‚˜ ë°ì´í„° ë¶„ì„ì„ í•˜ë©´ì„œ ëª¨ë¸ì˜ MSEë¥¼ ë¨¼ì € ì ‘í–ˆëŠ”ë°, Estimatorì˜ MSEë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ê½¤ ëœ¬ê¸ˆì—†ë‹¤ê³  ëŠê¼ˆë‹¤. ëª¨ë¸ì˜ MSEëŠ” `300.5`ì™€ ê°™ì´ ê°’ìœ¼ë¡œ ì–»ì–´ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ Estimatorì˜ MSEëŠ” ë°°ê²½ì„ ì´í•´í•˜ê³  ìœ„ì˜ ëª…ì œë¥¼ ë°›ì•„ ë“¤ì—¬ì•¼ í•œë‹¤.

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë˜ë‹¤ë¥¸ estimation ë°©ì‹ì¸ \<Interval Estimation\>ì— ëŒ€í•´ ì‚´í´ë³´ê² ë‹¤. ì´ë•Œ, ì£¼ì–´ì§„ Intervalì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ì•Œë ¤ì£¼ëŠ” ì§€í‘œê°€ ë°”ë¡œ \<confidence level\> $1 - \alpha$ë‹¤!

ğŸ‘‰ [Interval Estimation]({{"/2021/05/06/interval-estimation" | relative_url}})

í¬ìŠ¤íŠ¸ì— ì œì‹œ ë˜ì—ˆë˜ HW ë¬¸ì œë“¤ì€ ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ì— ë³„ë„ë¡œ ì •ë¦¬í•´ë‘ì—ˆë‹¤.

ğŸ‘‰ [Statistics - PS1]({{"/2021/06/06/statistics-ps1" | relative_url}})

<hr/>

[^1]: \<statistic; í†µê³„ëŸ‰\>ì€ random samples $X_1, ..., X_n$ì˜ í•¨ìˆ˜ $f(X_1, ..., X_n)$ì„ ë§í•œë‹¤. [Sampling Distribution](/2021/04/25/sampling-distribution) í¬ìŠ¤íŠ¸ ì°¸ê³ 