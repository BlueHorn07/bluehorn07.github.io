---
title: "Regression Analysis and Simple Linear Regression"
toc: true
toc_sticky: true
categories: ["Statistics"]
---

<!-- Least Square ë°©ë²•ì„ ë³„ë„ì˜ í¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ”ê²Œ ì¢‹ì„ ë“¯ -->

â€œí™•ë¥ ê³¼ í†µê³„(MATH230)â€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Probability and Statistics]({{"/category/probability-and-statistics" | relative_url}})ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ²

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„  \<Regression Analysis\>ì˜ ì»¨ì…‰ì„ ì‚´í´ë´…ë‹ˆë‹¤. \<Regression\>ì´ deterministic relationshipê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€, ëœë¤ì„±ì„ í¬í•¨í•˜ê¸° ìœ„í•´ ì–´ë–¤ ê°€ì •ì„ í•˜ëŠ”ì§€ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì‚´í´ë´…ì‹œë‹¤.

<br><span class="statement-title">TOC.</span><br>

- [Introduction to Linear Regression](#introduction-to-linear-regression)
- [Simple Linear Regression](#simple-linear-regression)
- [Least Square Method](#least-square-method)

<hr/>

## Introduction to Regression

ìš°ë¦¬ê°€ $n$ë²ˆì˜ ì‹¤í—˜ì„ í†µí•´ $n$ê°œì˜ ë°ì´í„° $\\{ (x_i, y_i) \\}_n$ë¥¼ ì–»ì—ˆë‹¤ê³  í•˜ì. ì´ ë°ì´í„°ë¥¼ ìœ ì‹¬íˆ ì‚´í´ë³´ë‹ˆ... $n$ê°œ ë°ì´í„°ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ê´€ê³„ë¥¼ ë°œê²¬í–ˆë‹¤.

$$
Y = \beta_0 + \beta_1 x
$$

ì™€ìš°! ì´ ê´€ê³„ê°€ ì‚¬ì‹¤ì´ë¼ë©´, ìš°ë¦¬ëŠ” $x$ ê°’ë§Œìœ¼ë¡œ ì •í™•í•œ $y$ ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤! ì´ëŸ° í˜•íƒœì˜ ê´€ê³„ì‹ì„ *deterministic* relationshipì´ë¼ê³  í•œë‹¤. ì´ëŸ° ê´€ê³„ëŠ” ëœë¤ì„±ì´ë‚˜ í™•ë¥ ì ì´ì§€ ì•Šì€ ìƒí™©ì—ì„œë§Œ ìœ íš¨í•  ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ëŸ° deterministic ì¼€ì´ìŠ¤ëŠ” í”ì¹˜ ì•Šë‹¤. ìš°ë¦¬ê°€ ëª¨ë“  ì‹¤í—˜ì„ í†µì œí•  ìˆ˜ ì—†ê³ , ëª¨ë“  dependent variable $x_i$ë¥¼ ë¶„ë³„í•  ìˆ˜ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì–»ì€ ë°ì´í„° $\\{ (x_i, y_i) \\}_n$ì—ëŠ” probabilisticí•œ ì„±ì§ˆì´ ì¡´ì¬í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë¦¬ê³  ê·¸ëŸ¬ëŠ” í¸ì´ generalization ê´€ì ì—ì„œ ë” ì•ˆì „í•˜ë‹¤!

ì•ìœ¼ë¡œ ê³µë¶€í•  ì»¨ì…‰ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="statement" align="center" markdown="1">

Model the relationship btw $x$ and $y$ <br/>
by finding a function $y = f(x)$ <br/>
that is a close fit to the given data $\\{ (x_i, y_i) \\}_n$

</div>

ìœ„ì™€ ê°™ì€ ëª¨ë¸ë§ì„ **\<Regression Analysis\>**ë¼ê³  í•œë‹¤.

### Multiple, Simple, Linear

ë§Œì•½ \<Regression Analysis\>ì—ì„œ ë‘˜ ì´ìƒì˜ dependent variableì„ ë‹¤ë£¨ëŠ” $y = f(x_1, x_2)$ë¼ë©´, \<multiple regression\>ì— ëŒ€í•œ ë¶„ì„ì´ë‹¤. ë°˜ëŒ€ë¡œ í•˜ë‚˜ì˜ dependent variable $y = f(x_1)$ë¼ë©´, \<simple regression\>ì— ëŒ€í•œ ë¶„ì„ì´ë‹¤.

ë˜, \<Regression Analysis\>ì—ì„œ ê´€ê³„ë¥¼ Linearë¡œ ê°€ì •í•œë‹¤ë©´: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ ë˜ëŠ” $y = \beta_0 + \beta_1 x_1$ë¼ë©´, \<linear regression\>ì— ëŒ€í•œ ë¶„ì„ì´ë‹¤.

ìš°ë¦¬ëŠ” í†µê³„ì˜ ì…ë¬¸ ìˆ˜ì—…ì„ ë“£ê³  ìˆê¸°ì— ê°€ì¥ ì‰¬ìš´ \<simple lienar regression; SLR\>ì— ëŒ€í•´ ê³µë¶€í•  ì˜ˆì •ì´ë‹¤.

<hr/>

## Simple Linear Regression

ì• ë¬¸ë‹¨ì—ì„œ \<Regression Analysis\>ê°€ ë‘ ë³€ìˆ˜ì˜ non-deterministic relationì„ ëª¨ë¸ë§í•˜ëŠ” ê³¼ì •ì´ë¼ê³  ì •ì˜í–ˆë‹¤. ì´ëŸ° non-deterministic ê²½ìš°ë¥¼ "\<random component\>ê°€ ìˆë‹¤"ë¼ê³  í‘œí˜„í•˜ê¸°ë„ í•œë‹¤.

ë™ì¼í•œ $x$ ê°’ìœ¼ë¡œ ì‹¤í—˜ì„ í•˜ë”ë¼ë„ ì—¬ëŸ¬ ìš”ì¸ì— ì˜í•´ $y$ì€ ë³€í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ response $y$ì— ëœë¤ì„±ì´ ìˆë‹¤ê³  ë³´ëŠ” ê²ƒì´ ì ì ˆí•˜ë‹¤. ë§Œì•½ $y$ë¥¼ $Y$ë¡œ í‘œí˜„í•œë‹¤ë©´, random variableë¡œì¨ í‘œí˜„í•œ ê²ƒì´ë‹¤. $y_i$ëŠ” ë°ì´í„°ì…‹ $\\{ (x_i, y_i) \\}_n$ì˜ í•œ ê°’ìœ¼ë¡œì¨ í‘œí˜„í•œ ê²ƒì´ë‹¤. ë‘˜ì„ êµ¬ë¶„í•´ì•¼ í•œë‹¤.

<br/>

ì, ì´ì œ \<Regression Analysis\>ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ Modelì„ ì •ì˜í•´ë³´ì. ìš°ë¦¬ëŠ” Simple Linear Regression Modelì„ ì •ì˜í•  ê²ƒì´ë‹¤.

$$
Y = \beta_0 + \beta_1 x + \epsilon
$$

$\beta_0$ì™€ $\beta_1$ëŠ” ìµìˆ™í•˜ë“¯ regression parameterì´ë‹¤. ê°ê° interceptì™€ slopeì˜ ì—­í• ì´ë‹¤.

$\epsilon$ì€ random variableì´ë‹¤. ì‹¤í—˜ê³¼ ë°ì´í„°ì…‹ì˜ ëœë¤ì„±, ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ëŠ” ì—­í• ì´ë‹¤. ì´ë•Œ, random variable $\epsilon$ì€ í‰ê· ê³¼ ë¶„ì‚°ì´ $E(\epsilon) = 0$, $\text{Var}(\epsilon) = \sigma^2$ìœ¼ë¡œ ì •ì˜ëœë‹¤.

ë‚´ìš©ì„ ë” ì§„í–‰í•˜ê¸° ì „ì— ëª‡ê°€ì§€ ì‚¬ì‹¤ë“¤ì„ ì •ë¦¬í•˜ê³  ê°€ì.

- $x$ëŠ” not randomì´ê³ , valueì¼ ë¿ì´ë‹¤.
- $Y$ëŠ” random variableì´ë‹¤. ì™œëƒí•˜ë©´, $\epsilon$ì´ random variableì´ê¸° ë•Œë¬¸ì´ë‹¤.

<!-- ì—¬ê¸°ì—ì„œ beta_0, beta_1ì€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ì–¸ê¸‰í•´ì•¼ í• ê¹Œ...? ì•„ë‹˜ ê·¸ ë‘˜ì— ëŒ€í•´ì„  ë’¤ì—ì„œ ë°í˜€ì§„ë‹¤ê³  í• ê¹Œ? -->

### Random Error



<hr/>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Simple Linear Regression Model<br>

For $n$ sample points $(x_1, y_1), \dots, (x_n, y_n)$,

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where $\epsilon_i$ are independent random variables with mean 0 and variance $\sigma^2$.

ìœ„ì™€ ê°™ì€ Regression Modelingì„ \<**Simple Linear Regression Model**\>ì´ë¼ê³  í•œë‹¤!!

</div>
$y_i$ê°€ $x_i$ì— dependent í•˜ë‹¤ê³  ê°€ì •í•œë‹¤. ì´ë•Œ, ë‘˜ì€ random factorì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì´ random factorëŠ” $\epsilon_i$ë¡œ í‘œí˜„ëœë‹¤.

<div class="statement" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. $x_i$ is called the \<predictor\> or \<regressor\>, and <span style="color: red">we assume $x_i$s are non-random.</span>

2\. $y_i$ is called the \<response\>, and it is a random variable with $E[y_i] = \beta_0 + \beta_1 x_i$ and $\text{Var}(y_i) = \sigma^2$.

3\. $\epsilon_i$ is called an \<error\>, and $\sigma^2$ is called the \<**error variance**\>. ğŸ”¥

</div>

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì´ëŸ° ì˜ë¬¸ì´ ë“ ë‹¤!

Q. ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ data pointsì— ë§ëŠ” line $y = \beta_0 + \beta_1 x$ë¥¼ ì°¾ê³ ì‹¶ë‹¤. ì´ë•Œ, $\beta_0$, $\beta_1$ë¡œ ê°€ëŠ¥í•œ ê°’ì´ ì•„ì£¼ ë§ì„ í…ë°, ì–´ë–¤ $\beta_0$, $\beta_1$ ê°’ì´ ì¢‹ë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ê¹Œ??

ìš°ë¦¬ëŠ” ì´ "Linear Regression"ì˜ ì¢‹ì€ ì •ë„ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ \<residual\>ê³¼ ê·¸ë“¤ì˜ í•©ì¸ \<residual sum\>ì„ ì •ì˜í•œë‹¤.

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> residual<br>

For a line $\hat{y} = b_0 + b_1 x$, the \<residual\> $e_i$  of a data point $(x_i, y_i)$ is defined to be

$$
e_i := y_i - \hat{y}_i
$$

</div>

ìš°ë¦¬ëŠ” ì´ residualì„ ìµœì†Œí™”í•˜ëŠ” $b_0$, $b_1$ì˜ ê°’ì„ ì°¾ê³  ì‹¶ë‹¤!! ì´ë•Œ, ì“°ëŠ” ë°©ë²•ì´ ë°”ë¡œ \<Least Square Method\>ë‹¤!

<hr/>

### Least Square Method

\<LS Method\>ëŠ” ìµœì„ ì˜ $\beta_0$, $\beta_1$ì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ì˜ \<SSE; Sum of Squares of the Errors\>ë¥¼ ìµœì†Œí™”í•˜ëŠ” $b_0$, $b_1$ë¥¼ ì°¾ê³ ì í•œë‹¤!

$$
\text{SSE} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2
$$

ìœ„ì˜ ì‹ì„ ìµœì í™”í•˜ëŠ” ê±´ ì •ë§ ê°„ë‹¨í•˜ë‹¤. ê·¸ëƒ¥ SSEë¥¼ $b_0$, $b_1$ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•´ì„œ 0ì´ ë˜ëŠ” $b_0$, $b_1$ì˜ ê°’ì„ ì°¾ìœ¼ë©´ ëœë‹¤.

Let $f(b_0, b_1) = \text{SSE}$, then

$$
\frac{\partial f}{\partial b_0} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\frac{\partial f}{\partial b_1} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

ë¨¼ì €, $b_0$ì— ëŒ€í•œ ì‹ë¶€í„° ì •ë¦¬í•´ë³´ì.

<div class="statement" markdown="1">

$$
\frac{\partial f}{\partial b_0} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\sum_{i=1}^n y_i = b_0 \sum_{i=1}^n 1 + b_1 \sum_{i=1}^n x_i
$$

ì–‘ë³€ì„ $n$ìœ¼ë¡œ ë‚˜ëˆ„ë©´,

$$
\bar{y} = b_0 + b_1 \bar{x}
$$

ë”°ë¼ì„œ,

$$
b_0 = \bar{y} - b_1 \bar{x}
$$

$\blacksquare$

</div>

ì´ë²ˆì—ëŠ” $b_1$ì— ëŒ€í•œ ì‹ì„ ì •ë¦¬í•´ë³´ì.

<div class="statement" markdown="1">

$$
\frac{\partial f}{\partial b_1} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

ìœ„ì˜ ì‹ì—ì„œ ì•„ê¹Œ êµ¬í•œ $b_0$ë¥¼ ëŒ€ì…í•´ì£¼ì!

$$
\sum_{i=1}^n (y_i - \bar{y} + b_1 \bar{x} - b_1 x_i) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - \bar{y} + b_1 (\bar{x} - x_i)) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - \bar{y})x_i + \sum_{i=1}^n b_1 (\bar{x} - x_i) x_i= 0
$$

$$
b_1 \cdot \sum_{i=1}^n (\bar{x} - x_i) x_i= - \sum_{i=1}^n (y_i - \bar{y})x_i
$$

$$
b_1 = - \frac{\sum (y_i - \bar{y})x_i}{\sum (\bar{x} - x_i) x_i}
$$

$$
b_1 = \frac{\sum (y_i - \bar{y})x_i}{\sum (x_i - \bar{x}) x_i}
$$

ë˜ëŠ” ìœ„ì˜ ì‹ì„ ì•½ê°„ ë³€í˜•í•´ ì•„ë˜ì™€ ê°™ì´ ì“°ê¸°ë„ í•œë‹¤.

$$
b_1 = \frac{\sum (y_i - \bar{y})(x_i - \bar{x})}{\sum (x_i - \bar{x}) (x_i - \bar{x})}
$$

ì´ê²Œ ê°€ëŠ¥í•œ ê²ƒì€ $b_1$ì— ëŒ€í•œ ì²«ë²ˆì§¸ ì‹ì—ì„œ $\sum (y_i - \bar{y}) \bar{x}$, $\sum (x_i - \bar{x}) \bar{x}$ë¥¼ ë¹¼ì¤„ ë•Œ, $\sum (y_i - \bar{y}) = \sum (x_i - \bar{x}) = 0$ì´ê¸° ë•Œë¬¸ì´ë‹¤!!

</div>

ë‹¤ì‹œ ì˜ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="theorem" markdown="1">

In \<LS method\>, the regression coefficients of $\beta_0$ and $\beta_1$ are estimated by

$$
b_1 = \frac{\sum (y_i - \bar{y})(x_i - \bar{x})}{\sum (x_i - \bar{x}) (x_i - \bar{x})} = \frac{S_{xy}}{S_{xx}}
$$

$$
b_0 = \bar{y} - b_1 \bar{x}
$$

</div>

ì—¬ê¸°ê¹Œì§€ ì§„í–‰í•˜ë©´, ì´ì œ ì•„ë˜ì™€ ê°™ì€ ì˜ë¬¸ì´ ë“ ë‹¤.

Q. Are $b_1$ and $b_0$ good estimators? ğŸ¤”

A. Yes!!

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

$b_1$ and $b_0$ are unbiased for $\beta_1$ and $\beta_0$ respectively.

$$
E[b_1] = \beta_1 \quad \text{and} \quad E[b_0] = \beta_0
$$

</div>


<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
E[b_1]
&= E \left[ \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{S_{xx}} \right] \\
&= E \left[ \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{S_{xx}} \right] \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})E[y_i]}{S_{xx}}
\end{aligned}
$$

ì‹ì—ì„œ ìœ„ì™€ ê°™ì´ $E[y_i]$ê°€ ê°€ëŠ¥í•œ ì´ìœ ëŠ” <span style="color: red">$x_i$ëŠ” Random Variableì´ ì•„ë‹ˆê¸° ë•Œë¬¸</span>ì´ë‹¤!!

$$
\begin{aligned}
&= \frac{\sum_{i=1}^n (x_i - \bar{x})E[y_i]}{S_{xx}} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})(\beta_0 + \beta_1 x_i )}{S_{xx}} \\
&= \frac{\cancel{\sum_{i=1}^n (x_i - \bar{x})\beta_0} + \sum_{i=1}^n (x_i - \bar{x}) \beta_1 x_i }{S_{xx}} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) \beta_1 x_i }{S_{xx}} \\
&= \beta_1 \cdot \frac{\sum_{i=1}^n (x_i - \bar{x}) x_i }{S_{xx}} \\
&= \beta_1 \cdot \cancelto{1}{\frac{\sum_{i=1}^n (x_i - \bar{x}) (x_i - \bar{x})}{S_{xx}}} \\
&= \beta_1
\end{aligned}
$$

$\blacksquare$

</div>

<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
E[\beta_0]
&= E[\bar{y} - b_1 \bar{x}] \\
&= E[\bar{y}] - E[b_1] \cdot \bar{x} \\
&= (\beta_0 + \beta_1 \bar{x}) - \beta_1 \bar{x} \\
&= \beta_0
\end{aligned}
$$

$\blacksquare$

</div>

<div class="statement" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. The derivation of LSEs does not depend on the distribution of $\epsilon_i$.

2\. If $\epsilon_i$s are iid $N(0, \sigma^2)$, then $b_0$ and $b_1$ are the MLEs for $\beta_0$ and $\beta_1$.

3\. $\sum e_i = 0$

4\. $\sum x_i e_i = 0$

(Homework ğŸˆ)

</div>

ìœ„ì˜ ëª…ì œ [3, 4]ë¥¼ í™œìš©í•´ ì•„ë˜ì˜ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

$$
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

ì´ë•Œ, ê° í…€ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\text{SST} = \text{SSR} + \text{SSE}
$$

- SST: Total Sum of Squares
- SSR: the Regression Sum of Squares
- SSE: the Residual Sum of Squares

ì¦ëª…ì€ ë§ˆì°¬ê°€ì§€ë¡œ (Homework ğŸˆ)

<hr/>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> R-square; ê²°ì • ê³„ìˆ˜<br>

$$
R^2 := 1 - \frac{\text{SSE}}{\text{SST}}
$$

be the "coefficient of determination; ê²°ì • ê³„ìˆ˜".

- $R^2 = 1$ is equivalent to
  - $\text{SSE} = 0$
  - $\hat{y_i} = y_i$ for all inputs
  - Regression model work very well!
- $R^2 = 0$ is equivalent to
  - $\text{SSE} = \text{SST}$
  - $\text{SSR} = 0$
  - $\hat{y}_i = \bar{y}$ for all inputs
  - Regression model outputs a constant.

</div>


<div class="definition" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. $0 \le R^2 \le 1$

2\. $R^2$ represents the proportionate reduction of total variation in $Y$ associated with the use of the variable $X$.

(a) If $R^2=1$, then $SSE = 0$, this means $\hat{y}_i = y_i$.

All observations fall on the line.

(b) If $R^2 = 0$, then $\text{SSE} = \text{SST}$ or $\text{SSR} = 0$.

The fitted regression line is the constant, $\bar{y}$.

</div>

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” \<Simple Linear Regression\>ì˜ ì„±ì§ˆì„ì„ ì´ì–´ì„œ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. \<Linear Regression\>ì—ì„œ ê³„ìˆ˜ $b_0$, $b_1$ì˜ ë¶„í¬ë¥¼ ì‚´í´ë³´ê³  ì´ë¥¼ í†µí•´ ê²€ì •(Test)ì„ ìˆ˜í–‰í•œë‹¤. ë˜, Regressionì„ í†µí•´ ì–»ì€ Prediction ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ \<Prediction Inference\>ë¥¼ ìˆ˜í–‰í•œë‹¤!

ğŸ‘‰ [Test on Regression]({{"/2021/06/09/test-on-regression.html" | relative_url}})