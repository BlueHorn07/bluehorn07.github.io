---
title: "Sampling Distribution of Mean, and CLT"
toc: true
toc_sticky: true
categories: ["Statistics"]
---
â€œí™•ë¥ ê³¼ í†µê³„(MATH230)â€ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì „ì²´ í¬ìŠ¤íŠ¸ëŠ” [Probability and Statistics](https://bluehorn07.github.io/categories/probability-and-statistics)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ²
{: .notice--info}

<div class="notice" markdown="1">

**ì‹œë¦¬ì¦ˆ: Sampling Distributions**

1. [Sampling Distribution]({{"/2021/04/25/sampling-distribution" | relative_url}})
2. [Sampling Distribution of Mean]({{"/2021/04/26/sampling-distribution-of-mean-and-clt" | relative_url}}) ğŸ‘€
3. [Sampling Distribution of Variance]({{"/2021/04/27/sampling-distribution-of-variance" | relative_url}})
4. [Student's t-distribution]({{"/2021/04/27/student-t-distribution" | relative_url}})
5. [F-distribution]({{"/2021/05/04/F-distribution" | relative_url}})
6. [EDF and Quantile]({{"/2021/05/04/EDF-and-Quantile" | relative_url}})

</div>

<br><span class="statement-title">TOC.</span><br>

- Sampling Distribution of Mean & CLT
- [Weak Law of Large Numbers](#weak-law-of-large-numbers)
- [CLT; Central Limit Theorem](#clt-central-limit-theorem)
  - [*proof*](#proof-of-clt)

<hr/>

# Sampling Distribution of Mean

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Then,

- $E[\overline{X}] = \mu$
- $\text{Var}(\overline{X}) = E\left[\left(\overline{X} - E[\overline{X}]\right)^2 \right] = \dfrac{\sigma^2}{n}$

\<LLN; Law of Large Numbers\>ì— ë”°ë¥´ë©´, $n$ì´ ë¬´í•œìœ¼ë¡œ ê°ˆë•Œ, ë¶„ì‚° $\text{Var}(\overline{X}) = \sigma^2/n$ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤. ë”°ë¼ì„œ $\overline{X} \rightarrow \mu$ê°€ ëœë‹¤!

<hr/>

# Weak Law of Large Numbers

<div class="notice" markdown="1">

<span class="statement-title">Theorem.</span> WLLN<br>

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Let $\overline{X}$ be a sample mean.

For any $\epsilon > 0$, we have

$$
\lim_{n\rightarrow\infty} P\left(\left| \overline{X} - \mu \right| > \epsilon\right) = 0
$$

</div>

<div class="math-statement" markdown="1">

<span class="statement-title">*Proof*.</span><br>

[\<Chebyshev's Inequality\>]({{"/2021/03/17/chebyshev's-inequality" | relative_url}})ë¥¼ ì‚¬ìš©í•˜ë©´ ì•„ì£¼ ì‰½ê²Œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤!

$$
\begin{aligned}
P\left(\left| \overline{X} - \mu \right| > \epsilon\right) &\le \frac{\text{Var}(\overline{X})}{\epsilon^2} \\
&= \frac{1}{\epsilon^2} \cdot \frac{\sigma^2}{n} \rightarrow 0 \quad \text{as} \quad n \rightarrow \infty
\end{aligned}
$$

$\blacksquare$

</div>

<big>"WLLN says that as the sample size $n$ gets larger, then the sample mean is close to the true mean in probability!"</big>

ì´ë•Œ, WLLNê³¼ ê°™ì€ í˜•íƒœì˜ ìˆ˜ë ´ì„ <span class="half_HL">"the convergence in probability"</span>ë¼ê³  í•œë‹¤.

cf) ì°¸ê³ ë¡œ \<Strong Law of Large Numbers\>ë„ ì¡´ì¬í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì •ë¦¬ë¥¼ ì¦ëª…í•˜ë ¤ë©´, ì¸¡ë„(measure)ì— ëŒ€í•œ ê°œë…ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì†Œê°œë§Œ í•˜ê³  ë„˜ì–´ê°€ê² ë‹¤.

$$
P\left(\lim_{n\rightarrow\infty} \overline{X} = \mu \right) = 1
$$

<hr/>

# CLT; Central Limit Theorem

<div class="example" markdown="1">

<span class="statement-title">Example.</span><br>

Q. What is the probability of $P\left( \overline{X} > 7\right)$?

Note that $X_1, \dots, X_n \sim \text{Ber}(p)$, and then $(X_1 + \cdots + X_n) \sim \text{BIN}(n, p)$.

When $n$ is larget, then $(X_1 + \cdots + X_n) \rightarrow N(\mu, \sigma^2)$.

Let's standardize it, then

$$
P\left( \frac{(X_1 + \cdots + X_n) - np}{\sqrt{npq}} \le z \right) \approx P(Z \le z)
$$

ì´ë•Œ, ì¢Œë³€ì˜ ë¶„ëª¨/ë¶„ìì— $n$ë¥¼ ë‚˜ì¤˜ì£¼ë©´

$$
\begin{aligned}
P\left( \frac{((X_1 + \cdots + X_n) - np)/n}{(\sqrt{npq})/n} \le z \right)
&= P\left( \frac{(X_1 + \cdots + X_n)/n \; - \; p}{\sqrt{pq/n}} \le z \right) \\
&= P\left( \frac{\overline{X} - E[\overline{X}]}{\sqrt{\text{Var}(\overline{X})}} \le z \right) \\
&\approx P(Z \le z)
\end{aligned}
$$

ê²°ë¡ ì€, ì›ë˜ ë¬¸ì œì˜€ë˜ $P\left(\overline{X} > 7\right)$ì„ ì˜ ì •ê·œí™”í•´ì„œ normal ë¶„í¬ë¡œ ê·¼ì‚¬í•˜ì—¬ í’€ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.

</div>

ê·¸ëŸ°ë°, ì§€ê¸ˆì˜ ê²½ìš°ëŠ” $\overline{X}$ê°€ BIN ë¶„í¬ì˜€ê¸° ë•Œë¬¸ì— \<Normal Approximation\>ì— ì˜í•´ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„ëœ ê²ƒì´ì—ˆë‹¤. ê³¼ì—° $\overline{X}$ ë˜ëŠ” $X_i$ê°€ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì ¸ë„ ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ í’€ ìˆ˜ ìˆì„ê¹Œ? ì´ ì˜ë¬¸ì— ëŒ€í•œ ë‹µì„ ì œì‹œí•˜ëŠ” ê²ƒì´ ë°”ë¡œ \<CLT; Central Limit Theorem\>ì´ë‹¤ ğŸ¤©

<br/>

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span> CLT; Central Limit Theorem<br>

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Let $\overline{X}_n := (X_1 + \cdots + X_n)/n$, sample mean.

Let $Z_n := \dfrac{\overline{X}_n - E[\overline{X}]}{\sqrt{\text{Var}(\overline{X})}} = \dfrac{\overline{X}_n - \mu}{\sigma/\sqrt{n}}$.

then, for any $z \in \mathbb{R}$, we have

$$
P(Z_n \le z) \rightarrow P(Z \le z) \quad \text{as} \quad n \rightarrow \infty
$$

where $Z \sim N(0, 1)$.

</div>

ì¦‰, <span class="half_HL">ëª¨ì§‘ë‹¨ì—ì„œ ì¶”ì¶œí•œ í‘œë³¸ $n$ì´ ì¶©ë¶„íˆ í¬ë‹¤ë©´, "í‘œë³¸í‰ê· " $\bar{X}$ì˜ ë¶„í¬ëŠ” ì •ê·œ ë¶„í¬ì— ê·¼ì‚¬í•œë‹¤!</span>

<span class="statement-title">Remark.</span><br>

1\. As long as iid, RVs have finite second moment[^1], then we have CLT.

ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ì•„ì£¼ ê°•ë ¥í•˜ë‹¤ğŸ’¥ $X_i$ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ìƒê´€ì—†ì´ CLTë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ê¸° ë•Œë¬¸ì´ë‹¤!! ì´ëŸ° ì  ë•Œë¬¸ì— CLTë¥¼ <span class="half_HL">"universal result"</span>ë¼ê³  í•œë‹¤!

<br/>

2\. We call $z: = \dfrac{\overline{x} - \mu}{\sigma / \sqrt{n}}$ as \<$z$-value\> or \<$z$-score; $z$-ì ìˆ˜, í‘œì¤€ ì ìˆ˜\>, we define $z_\alpha$ as the number $z$ s.t. $P(Z \ge z) = \alpha$ when $Z \sim N(0, 1)$.

<div class="img-wrapper">
<img src="https://upload.wikimedia.org/wikipedia/commons/2/25/The_Normal_Distribution.svg" height="400px">
</div>

## Proof of CLT

<div class="notice" markdown="1">

<span class="statement-title">*Proof*.</span> CLT<br>

\<CLT\>ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ì •ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤.

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span> Uniqueness of MGF<br>

If the mgf of $X_n$ converges to the mgf of $X$ for $t \in (-\delta, \delta)$ for some $\delta > 0$,

i.e.

$$
M_{X_n} (t) \rightarrow M_{X} (t) \quad \text{for} \quad t \in (-\delta, \delta)
$$

and $X$ is continuous, then CDF $F_{X_n}(x)$ converges to $F_{X}(x)$ for all $x \in \mathbf{R}$.

$$
F_{X_n}(x) \rightarrow F_{X}(x)
$$

</div>

âœ¨ Goal: Show that the MGF of $Z = \dfrac{\bar{X} - \mu}{\sigma / \sqrt{n}}$ converges to the MGF of $N(0, 1)$ as $n \rightarrow \infty$.

Let $W = \dfrac{\bar{X} - \mu}{\sigma / \sqrt{n}}$, and then multiply $n$ both to the numerator and denominator.

$$
W = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} = \frac{\displaystyle \sum_{i=1}^n X_i - n \mu}{\sqrt{n} \cdot \sigma}
$$

The mgf of $W$ is

$$
\begin{aligned}
M_W (t)
&= E [e^{tW}]
= E\left[\exp \left( \frac{t}{\sqrt{n} \sigma}\sum^n_{i=1} X_i - n\mu \right) \right] \\
&= E \left[ \exp \left( \frac{t}{\sqrt{n}} \cdot \frac{X_1 - \mu}{\sigma}\right) \cdot \exp \left( \frac{t}{\sqrt{n}} \cdot \frac{X_2 - \mu}{\sigma}\right) \cdots \exp \left( \frac{t}{\sqrt{n}} \cdot \frac{X_n - \mu}{\sigma}\right) \right] \\
&= E \left[ \exp \left( \frac{t}{\sqrt{n}} \cdot \frac{X_1 - \mu}{\sigma}\right) \right] \cdots E \left[ \exp \left( \frac{t}{\sqrt{n}} \cdot \frac{X_n - \mu}{\sigma}\right) \right] \quad \text{iid} \\
&= M_{Z_1} (t / \sqrt{n}) \cdots M_{Z_n} (t / \sqrt{n}) \\
&= \left[ M_{Z} (t / \sqrt{n}) \right]^n
\end{aligned}
$$

ì´ì œ $M_W(t)$ì— $\log$ë¥¼ ì·¨í•˜ê³ , ê·¹í•œ $n\rightarrow \infty$ë¥¼ ì·¨í•˜ë©´,

$$
\begin{aligned}
\lim_{n\rightarrow \infty} \log M_W(t) \\
= \lim_{n\rightarrow \infty} \log \left[ M_{Z} (t / \sqrt{n}) \right]^n \\
&= \lim_{n\rightarrow \infty} n \cdot \log M_Z (t / \sqrt{n})
\end{aligned}
$$

ì—¬ê¸°ì„œ $y = 1 / \sqrt{n}$ë¡œ ì¹˜í™˜í•´ì£¼ë©´, ìœ„ì˜ ê·¹í•œì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\lim_{n\rightarrow \infty} n \cdot \log M_Z (t / \sqrt{n})
= \lim_{y \rightarrow 0} \frac{\log M_Z (yt)}{y^2}
$$

ì´ë•Œ, $\displaystyle \lim_{y\rightarrow 0} M_Z(yt) = M_Z(0) = 1$ì´ë¯€ë¡œ ê·¹í•œì‹ì´ $\dfrac{0}{0}$ ê¼´ì˜ ë¶€ì •í˜•ì´ ëœë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ "ë¡œí”¼íƒˆ ì •ë¦¬"ë¥¼ ì‚¬ìš©í•œë‹¤.

$$
\begin{aligned}
\lim_{y \rightarrow 0} \frac{\log M_Z (yt)}{y^2}
&= \lim_{y \rightarrow 0} \frac{t \cdot \dfrac{M_z(yt)'}{M_Z(yt)}}{2y} \\
&= \frac{t}{2} \cdot \lim_{y \rightarrow 0} \frac{M_z(yt)'}{y \cdot M_Z (yt)} \\
&= \frac{t}{2} \cdot \lim_{y \rightarrow 0} \frac{M_z(yt)'}{y} \quad \left(\because \; \lim_{y\rightarrow 0} M_z(yt) = 1\right)
\end{aligned}
$$

ì´ë•Œ, ìœ„ì˜ ì‹ì—ì„œë„ $\displaystyle \lim_{y\rightarrow 0} M_Z(yt)' = M_Z(0)' = 0 = \mu$ì´ë¯€ë¡œ, ë‹¤ì‹œ "ë¡œí”¼íƒˆ ì •ë¦¬"ë¥¼ ì ìš©í•˜ë©´,

$$
\begin{aligned}
\frac{t}{2} \cdot \lim_{y \rightarrow 0} \frac{M_z(yt)'}{y}
&= \frac{t}{2} \cdot \lim_{y \rightarrow 0} \frac{t M_z (yt) ''}{1} \\
&= \frac{t^2}{2} \cdot \lim_{y \rightarrow 0} M_z (yt) '' \\
&= \frac{t^2}{2} \quad \left(\because \; \lim_{y \rightarrow 0} M_z (yt) '' = 1 = \sigma^2\right)
\end{aligned}
$$

ë”°ë¼ì„œ,

$$
\lim_{n\rightarrow \infty} \log M_W(t) = \frac{t^2}{2}
$$

ìœ„ì˜ ì‹ì—ì„œ $\log$ë¥¼ ì‹œì¼œì„œ $\bar{X}$ì˜ ì •ê·œí™”ì¸ $W$ì˜ mgf $M_W(t)$ë¥¼ ì–»ìœ¼ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\lim_{n\rightarrow \infty} M_W(t) = e^{t^2/2}
$$

ì´ë•Œ, í‘œì¤€ì •ê·œë¶„í¬ $N(0, 1)$ì˜ mgfê°€ $e^{t^2/2}$ì´ê³ , ë‘ ë¶„í¬ì˜ mgfê°€ ê°™ìœ¼ë¯€ë¡œ, "Uniqueness of mgf"ì— ì˜í•´ ì•„ë˜ì˜ ëª…ì œê°€ ì„±ë¦½í•œë‹¤.

"$n$ì´ ì¶©ë¶„íˆ ì»¤ì§€ë©´, $\bar{X}$ì˜ ì •ê·œí™”ì¸ $\dfrac{\bar{X} - \mu}{\sigma/\sqrt{n}}$ëŠ” í‘œì¤€ì •ê·œë¶„í¬ $N(0, 1)$ì„ ë”°ë¥¸ë‹¤!"

$\blacksquare$

</div>

<hr/>

### Sampling Distribution of the difference btw two mean

ì´ë²ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¡œ populationì—ì„œ ë½‘ì€ ë‘ independent sampleì„ ìƒê°í•´ë³´ì!

Let $X_1, \dots, X_{n_1}$, and $Y_1, \dots, Y_{n_2}$ be two independent random samples with $E[X_1] = \mu_1$, $\text{Var}(X_1) = \sigma_1^2$, and  $E[X_2] = \mu_2$, $\text{Var}(Y_2) = \sigma_2^2$.

ìš°ë¦¬ëŠ” "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨" $\mu_1 - \mu_2$ì— ëŒ€í•œ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³ ì í•œë‹¤. ì´ë•Œ, $\overline{X} - \overline{Y}$ë¥¼ ì‚¬ìš©í•˜ë©´ "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨"ì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤!!

By CLT,

$$
\begin{aligned}
    \frac{\overline{X} - \mu_1}{\sigma_1/\sqrt{n_1}} \sim N(0, 1) \quad & \iff \quad \overline{X} \sim N\left(\mu_1, \sigma_1^2/n_1\right) \\
    \frac{\overline{Y} - \mu_2}{\sigma_2/\sqrt{n_2}} \sim N(0, 2) \quad & \iff \quad \overline{Y} \sim N\left(\mu_2, \sigma_2^2/n_2\right)
\end{aligned}
$$

ë”°ë¼ì„œ, $\overline{X} - \overline{Y}$ì— ëŒ€í•œ ë¶„í¬ëŠ” independentí•œ ë‘ normal distributionì— ëŒ€í•œ ë§ì…ˆìœ¼ë¡œ ì‰½ê²Œ ìœ ë„í•  ìˆ˜ ìˆë‹¤!

$$
\overline{X} - \overline{Y} \sim N\left( \mu_1 - \mu_2, \; \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2} \right)
$$

ìœ„ì˜ ì‚¬ì‹¤ì„ ì´ìš©í•´ì„œ, "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨"ì— ëŒ€í•œ ì¶”ë¡ ë„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ ğŸ˜‰

<hr/>

# ë§ºìŒë§

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” í‘œë³¸í‰ê·  $\bar{X}$ì— ëŒ€í•œ ë¶„í¬ì¸ "Sampling Distribuion of Mean"ì„ ë³´ì•˜ë‹¤. ë˜, í‘œë³¸í‰ê·  $\bar{X}$ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ê³ , í™œìš©í•˜ëŠ”ë° í•„ìš”í•œ \<WLLN\>ê³¼ \<CLT\>ë¥¼ ì‚´í´ë³´ì•˜ë‹¤.

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” "í‰ê· "ê³¼ í•¨ê»˜, í™•ë¥  ë¶„í¬ì˜ íŠ¹ì„±ì„ ê²°ì •í•˜ëŠ” parameterì¸ **"ë¶„ì‚°(Variance)"**ì´ Random Sampleì—ì„œ ì–´ë–»ê²Œ ìœ ë„ë˜ëŠ”ì§€ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤.

ğŸ‘‰ [Sampling Distribution of Variance]({{"/2021/04/27/sampling-distribution-of-variance" | relative_url}})

<hr/>

# references

- ['ì•Œí† 'ë‹˜ì˜ í¬ìŠ¤íŠ¸](http://blog.naver.com/psggoma/220899911971)