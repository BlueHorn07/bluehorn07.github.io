---
title: "Test on Regression"
layout: post
use_math: true
tags: ["statistics"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í™•ë¥ ê³¼ í†µê³„' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

[Introduction to Linear Regression]({{"/2021/06/06/introduction-to-linear-regression.html" | relative_url}}) í¬ìŠ¤íŠ¸ì—ì„œ ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 

<br><span class="statement-title">TOC.</span><br>

- [Review]
- [Distribution of Regression Coefficients $B_0$ and $B_1$](#distribution-of-regression-coefficients)
- [Estimator of Error Variance $\sigma^2$](#estimator-of-error-variance)

<hr/>

### Review



<hr/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì•„ë˜ì˜ ë‘ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ìš”í•˜ê²Œ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤.

Q1. What are the distributions of $B_1$ and $B_0$?

Q2. What can be an estimator for $\sigma^2$?

<hr/>

### Distribution of Regression Coefficients

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

Assume $\epsilon_i$s are iid normal random variables; $\epsilon_i \sim N(0, \sigma^2)$.

Then,

$$
B_1 \sim N(\beta_1, \frac{\sigma^2}{S_{xx}})
$$

$$
B_0 \sim N(\beta_0, \frac{\sum x_i^2}{n \; S_{xx}} \cdot \sigma^2)
$$

</div>

<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
B_1 
&:= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{S_{xx}} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{S_{xx}}
\end{aligned}
$$

is a linear combination of normal random variables $y_i$s, thus $B_1$ is also a normal RV.

Hense, we only need to find the mean and the variance of $B_1$.

<br/>

1\. Mean

$B_1$ is an unbiased estimator, so

$$
E[B_1] = \beta_1
$$

2\. Variance

$$
\begin{aligned}
\text{Var}(B_1) 
&= \text{Var}\left(\frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{S_{xx}}\right)  \\
&= \frac{1}{S_{xx}^2} \cdot \left( \cancelto{S_{xx}}{\sum_{i=1}^n (x_i - \bar{x})} \right)^2 \cdot \cancelto{\sigma^2}{\text{Var}(y_i)} \\
&= \frac{\sigma^2}{S_{xx}}
\end{aligned}
$$

</div>

<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
B_0 = \bar{y} - B_1 \bar{x}
$$

is also a linear combination of normal random variables $y_i$s.

1\. Mean

$B_0$ is also an unbiased estimator, so

$$
E[B_0] = \beta_0
$$

2\. Variance

(Homework ğŸˆ)

</div>

<hr/>

### Estimator of Error Variance

Recall that $\sigma^2 = \text{Var}(\epsilon_i)$, and the $\epsilon_i$ was the difference btw response $y_i$ and true regression $\beta_0 + \beta_1 x_i$; $\epsilon_i = y_i - (\beta_0 + \beta_1 x_i)$.

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

The unbiased estimator of $\sigma^2$ is 

$$
s^2 := \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n-2} = \frac{\text{SSE}}{n-2}
$$

</div>

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

$s^2$ is independent of $B_1$ and $B_0$, and 

$$
\frac{(n-2)S^2}{\sigma^2} \sim \chi^2(n-2)
$$

</div>

<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

ìœ„ì˜ ë‘ ì •ë¦¬ì— ëŒ€í•œ ì¦ëª…ì€ HWë¡œ ë‚¨ê²¨ë‘”ë‹¤.

(Homework ğŸˆ)

</div>

<hr/>

### Inferences for Regression Coefficients

Supp. we have sample points $(x_1, y_1), \dots, (x_n, y_n)$ from $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ where $\epsilon_i$s are iid $N(0, \sigma^2)$. Here, $\beta_0$ and $\beta_1$ are unknown parameters.

ìš°ë¦¬ëŠ” ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ $\beta_0$, $\beta_1$ì— ëŒ€í•œ \<confidence interval\>ì„ ì°¾ê³  ë˜ ê·¸ê²ƒì„ ì´ìš©í•´ **ê²€ì •**ì„ ìˆ˜í–‰í•´ ë³¼ ê²ƒì´ë‹¤!

<div class="statement" markdown="1">

ìš°ë¦¬ëŠ” $\beta_1$ì— ëŒ€í•œ point estimatorë¡œ $B_1 = S_{xy} / S_{xx}$ë¥¼ ì‚¬ìš©í–ˆê³ , ì´ë•Œ $B_1$ì˜ ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ì•˜ë‹¤.

$$
B_1 \sim N \left( \beta_1, \; \sigma^2/S_{xx} \right)
$$

ì´ë•Œ, $B_1$ì„ ì ë‹¹íˆ ì •ê·œí™”ì‹œí‚¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\frac{B_1 - \beta_1}{\sigma / \sqrt{S_{xx}}} \sim N(0, 1)
$$

ì´ë•Œ, ìš°ë¦¬ëŠ” error variance $\sigma^2$ì— ëŒ€í•œ ê°’ì„ ëª¨ë¥¸ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ sample error varianceì¸ $s^2 = \text{SSE}/(n-2)$ë¡œ ëŒ€ì²´í•´ì¤€ë‹¤! ê·¸ ê²°ê³¼ ë¶„í¬ëŠ” \<t-distribution\>ì„ ë”°ë¥¸ë‹¤.

$$
\frac{B_1 - \beta_1}{s / \sqrt{S_{xx}}} \sim t(n-2)
$$

ì´ì œ ìœ„ì˜ ë¶„í¬ì—ì„œ $\beta_1$ì— ëŒ€í•œ $100(1-\alpha)\%$ confidence intervalì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\left( b_1 - t_{\alpha/2} (n-2) \cdot \frac{s}{\sqrt{S_{xx}}}, \; b_1 + t_{\alpha/2} (n-2) \cdot \frac{s}{\sqrt{S_{xx}}}  \right)
$$

ë‹¤ìŒì€ $B_1$ì— ëŒ€í•œ ë¶„í¬ì‹ì„ í™œìš©í•´ ê²€ì •ì„ ì§„í–‰í•˜ë©´ ëœë‹¤!! ğŸ˜†

</div>

<div class="statement" markdown="1">

ë§ˆì°¬ê°€ì§€ë¡œ $B_0$ì— ëŒ€í•´ì„œë„ ê²€ì •ì„ ìˆ˜í–‰í•´ë³´ì. $B_0$ì˜ ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ì•˜ë‹¤.

$$
B_0 \sim N\left( \beta_0, \; \frac{\sigma^2 \cdot \sum_{i=1}^n x_i^2}{n S_{xx}}\right)
$$

$B_0$ë¥¼ ì •ê·œí™”í•˜ê³ , ë˜ $\sigma^2$ë¥¼ $s^2$ë¡œ ëŒ€ì²´í•´ì£¼ë©´ ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
\frac{B_0 - \beta_0}{s \sqrt{\frac{\sum_{i=1}^n x_i^2}{n S_{xx}}}} \sim t(n-2)
$$

ë§ˆì°¬ê°€ì§€ë¡œ $\beta_0$ì— ëŒ€í•œ $100(1-\alpha)\%$ confidence intervalì„ êµ¬í•˜ê³ , ì ë‹¹íˆ ê²€ì •ì„ ì˜ ìˆ˜í–‰í•˜ë©´ ëœë‹¤! ğŸ˜†

</div>

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„  Linear Regression ëª¨ë¸ì—ì„œ ìˆ˜í–‰í•˜ëŠ” Predictionì—ì„œ ìˆ˜í–‰í•˜ëŠ” ì¶”ì •ì— ëŒ€í•´ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ì‚´í´ë´¤ë˜ $B_1$, $B_0$ì˜ ë¶„í¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì‚¬ìš©í•  ì˜ˆì •ì´ë©°, ì´ ê³¼ì •ì„ í†µí•´ Regressionìœ¼ë¡œ ì–»ì€ ê²°ê³¼(response)ì˜ ì‹ ë¢°ë„ì™€ ê·¸ ì˜¤ì°¨ì— ëŒ€í•´ ë” ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.

ğŸ‘‰ [Prediction on Regression]({{"/2021/06/10/prediction-on-regression.html" | relative_url}})

ì´ë²ˆ í¬ìŠ¤íŠ¸ì— ì œì‹œ í–ˆë˜ HW ë¬¸ì œì˜ í’€ì´ëŠ” ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ì— ë³„ë„ë¡œ ì •ë¦¬í•´ë‘ì—ˆë‹¤.

ğŸ‘‰ [Statistics - PS3]({{"/2021/06/10/statistics-ps3.html"}})